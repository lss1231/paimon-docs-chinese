<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maintenance on Paimon Docs</title>
    <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/</link>
    <description>Recent content in Maintenance on Paimon Docs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://lss1231.github.io/paimon-docs-chinese/en/maintenance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Filesystems</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/filesystems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/filesystems/</guid>
      <description>Filesystems&#xD;#&#xD;Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.</description>
    </item>
    <item>
      <title>Write Performance</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/write-performance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/write-performance/</guid>
      <description>Write Performance&#xD;#&#xD;Paimon&amp;rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:&#xA;Flink Configuration (&#39;flink-conf.yaml&#39;/&#39;config.yaml&#39; or SET in SQL): Increase the checkpoint interval (&#39;execution.checkpointing.interval&#39;), increase max concurrent checkpoints to 3 (&#39;execution.checkpointing.max-concurrent-checkpoints&#39;), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode. Option &#39;changelog-producer&#39; = &#39;lookup&#39; or &#39;full-compaction&#39;, and option &#39;full-compaction.delta-commits&#39; have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.</description>
    </item>
    <item>
      <title>Dedicated Compaction</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/dedicated-compaction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/dedicated-compaction/</guid>
      <description>Dedicated Compaction&#xD;#&#xD;Paimon&amp;rsquo;s snapshot management supports writing with multiple writers.&#xA;For S3-like object store, its &#39;RENAME&#39; does not have atomic semantic. We need to configure Hive metastore and enable &#39;lock.enabled&#39; option for the catalog.&#xD;By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon&amp;rsquo;s latest partition, Simultaneously batch job (overwrite) writes records to the historical partition.&#xA;So far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated.</description>
    </item>
    <item>
      <title>Manage Snapshots</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-snapshots/</guid>
      <description>Manage Snapshots&#xD;#&#xD;This section will describe the management and behavior related to snapshots.&#xA;Expire Snapshots&#xD;#&#xD;Paimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.</description>
    </item>
    <item>
      <title>Rescale Bucket</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/rescale-bucket/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/rescale-bucket/</guid>
      <description>Rescale Bucket&#xD;#&#xD;Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.&#xA;Rescale Overwrite&#xD;#&#xD;-- rescale number of total buckets ALTER TABLE table_identifier SET (&amp;#39;bucket&amp;#39; = &amp;#39;.</description>
    </item>
    <item>
      <title>Manage Tags</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-tags/</guid>
      <description>Manage Tags&#xD;#&#xD;Paimon&amp;rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.&#xA;To solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot.</description>
    </item>
    <item>
      <title>Metrics</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/metrics/</guid>
      <description>Paimon Metrics&#xD;#&#xD;Paimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.&#xA;In Paimon&amp;rsquo;s metrics system, metrics are updated and reported at table granularity.&#xA;There are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.</description>
    </item>
    <item>
      <title>Manage Privileges</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-privileges/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-privileges/</guid>
      <description>Manage Privileges&#xD;#&#xD;Paimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.&#xA;Currently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.&#xA;This privilege system only prevents unwanted users from accessing tables through catalogs. It does not block access through temporary table (by specifying table path on filesystem), nor does it prevent user from directly modifying data files on filesystem.</description>
    </item>
    <item>
      <title>Manage Branches</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-branches/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-branches/</guid>
      <description>Manage Branches&#xD;#&#xD;In streaming data processing, it&amp;rsquo;s difficult to correct data for it may affect the existing data, and users will see the streaming provisional results, which is not expected.&#xA;We suppose the branch that the existing workflow is processing on is &amp;lsquo;main&amp;rsquo; branch, by creating custom data branch, it can help to do experimental tests and data validating for the new job on the existing table, which doesn&amp;rsquo;t need to stop the existing reading / writing workflows and no need to copy data from the main branch.</description>
    </item>
    <item>
      <title>Manage Partitions</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-partitions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/manage-partitions/</guid>
      <description>Manage Partitions&#xD;#&#xD;Paimon provides multiple ways to manage partitions, including expire historical partitions by different strategies or mark a partition done to notify the downstream application that the partition has finished writing.&#xA;Expiring Partitions&#xD;#&#xD;You can set partition.expiration-time when creating a partitioned table. Paimon streaming sink will periodically check the status of partitions and delete expired partitions according to time.&#xA;How to determine whether a partition has expired: you can set partition.</description>
    </item>
    <item>
      <title>Configurations</title>
      <link>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/configurations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lss1231.github.io/paimon-docs-chinese/en/maintenance/configurations/</guid>
      <description>Configuration&#xD;#&#xD;CoreOptions&#xD;#&#xD;Core options for paimon.&#xA;Key&#xD;Default&#xD;Type&#xD;Description&#xD;aggregation.remove-record-on-delete&#xD;false&#xD;Boolean&#xD;Whether to remove the whole row in aggregation engine when -D records are received.&#xD;alter-column-null-to-not-null.disabled&#xD;true&#xD;Boolean&#xD;If true, it disables altering column type from null to not null. Default is true. Users can disable this option to explicitly convert null column type to not null.&#xD;async-file-write&#xD;true&#xD;Boolean&#xD;Whether to enable asynchronous IO writing when writing files.</description>
    </item>
  </channel>
</rss>

"use strict";(function(){const t={encode:!1,tokenize:function(e){return e.replace(/[\x00-\x7F]/g,"").split("")}};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/maintenance/filesystems/",title:"Filesystems",section:"Maintenance",content:`Filesystems#Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.
Supported FileSystems#FileSystem URI Scheme Pluggable Description Local File System file:// N Built-in Support HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment Aliyun OSS oss:// Y S3 s3:// Y Tencent Cloud Object Storage cosn:// Y Microsoft Azure Storage abfs:// Y Huawei OBS obs:// Y Google Cloud Storage gs:// Y Dependency#We recommend you to download the jar directly: Download Link.
You can also manually build bundled jar from the source code.
To build from source code, clone the git repository.
Build shaded jar with the following command.
mvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-\${fs}/target/paimon-\${fs}-1.2.0.jar.
HDFS#You don&rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.
HDFS Configuration#For HDFS, the most important thing is to be able to read your HDFS configuration.
FlinkYou may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:
Set environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog. The first approach is recommended.
If you do not want to include the value of the environment variable, you can configure hadoop-conf-loader to option.
Hive/SparkHDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details.Hadoop-compatible file systems (HCFS)#All Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.
This way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).
HDFS Alluxio (see configuration specifics below) XtreemFS … The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.
For Alluxio support add the following entry into the core-site.xml file:
&lt;property&gt; &lt;name&gt;fs.alluxio.impl&lt;/name&gt; &lt;value&gt;alluxio.hadoop.FileSystem&lt;/value&gt; &lt;/property&gt; Kerberos#FlinkIt is recommended to use Flink Kerberos Keytab.SparkIt is recommended to use Spark Kerberos Keytab.HiveAn intuitive approach is to configure Hive&rsquo;s kerberos authentication.Trino/JavaAPIConfigure the following three options in your catalog configuration:
security.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache. For JavaAPI:
SecurityContext.install(catalogOptions); HDFS HA#Ensure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.
HDFS ViewFS#Ensure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.
OSS#Download paimon-oss-1.2.0.jar.FlinkIf you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.Put paimon-oss-1.2.0.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;oss://&lt;bucket&gt;/&lt;path&gt;&#39;, &#39;fs.oss.endpoint&#39; = &#39;oss-cn-hangzhou.aliyuncs.com&#39;, &#39;fs.oss.accessKeyId&#39; = &#39;xxx&#39;, &#39;fs.oss.accessKeySecret&#39; = &#39;yyy&#39; ); SparkIf you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.Place paimon-oss-1.2.0.jar together with paimon-spark-1.2.0.jar under Spark&rsquo;s jars directory, and start like
spark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=oss://&lt;bucket&gt;/&lt;path&gt; \\ --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\ --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\ --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy HiveIf you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.NOTE: You need to ensure that Hive metastore can access oss.
Place paimon-oss-1.2.0.jar together with paimon-hive-connector-1.2.0.jar under Hive&rsquo;s auxlib directory, and start like
SET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore
SELECT * FROM test_table; SELECT COUNT(1) FROM test_table; TrinoFrom version 0.8, paimon-trino uses trino filesystem as basic file read and write system. We strongly recommend you to use jindo-sdk in trino.
You can find How to config jindo sdk on trino here. Please note that:
Use paimon to replace hive-hadoop2 when you decompress the plugin jar and find location to put in. You can specify the core-site.xml in paimon.properties on configuration hive.config.resources. Presto and Jindo use the same configuration method. If you environment has jindo sdk dependencies, you can use Jindo Fs to connect OSS. Jindo has better read and write efficiency.
Download paimon-jindo-1.2.0.jar.S3#Download paimon-s3-1.2.0.jar.FlinkIf you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.Put paimon-s3-1.2.0.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;s3://&lt;bucket&gt;/&lt;path&gt;&#39;, &#39;s3.endpoint&#39; = &#39;your-endpoint-hostname&#39;, &#39;s3.access-key&#39; = &#39;xxx&#39;, &#39;s3.secret-key&#39; = &#39;yyy&#39; ); SparkIf you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.Place paimon-s3-1.2.0.jar together with paimon-spark-1.2.0.jar under Spark&rsquo;s jars directory, and start like
spark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=s3://&lt;bucket&gt;/&lt;path&gt; \\ --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\ --conf spark.sql.catalog.paimon.s3.access-key=xxx \\ --conf spark.sql.catalog.paimon.s3.secret-key=yyy HiveIf you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.NOTE: You need to ensure that Hive metastore can access s3.
Place paimon-s3-1.2.0.jar together with paimon-hive-connector-1.2.0.jar under Hive&rsquo;s auxlib directory, and start like
SET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore
SELECT * FROM test_table; SELECT COUNT(1) FROM test_table; TrinoPaimon use shared trino filesystem as basic read and write system.
Please refer to Trino S3 to config s3 filesystem in trino.
S3 Compliant Object Stores#The S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent&rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.
s3.endpoint: your-endpoint-hostname Configure Path Style Access#Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.
s3.path.style.access: true S3A Performance#Tune Performance for S3AFileSystem.
If you encounter the following exception:
Caused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.
Google Cloud Storage#Download paimon-gs-1.2.0.jar.FlinkIf you have already configured gcs access through Flink (Via Flink FileSystem), here you can skip the following configuration.Put paimon-gs-1.2.0.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;gs://&lt;bucket&gt;/&lt;path&gt;&#39;, &#39;fs.gs.auth.type&#39; = &#39;SERVICE_ACCOUNT_JSON_KEYFILE&#39;, &#39;fs.gs.auth.service.account.json.keyfile&#39; = &#39;/path/to/service-account-.json&#39; ); Microsoft Azure Storage#Download paimon-azure-1.2.0.jar.FlinkIf you have already configured azure access through Flink (Via Flink FileSystem), here you can skip the following configuration.Put paimon-azure-1.2.0.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;wasb://,&lt;container&gt;@&lt;account&gt;.blob.core.windows.net/&lt;path&gt;&#39;, &#39;fs.azure.account.key.Account.blob.core.windows.net&#39; = &#39;yyy&#39; ); SparkIf you have already configured azure access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.Place paimon-azure-1.2.0.jar together with paimon-spark-1.2.0.jar under Spark&rsquo;s jars directory, and start like
spark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=wasb://,&lt;container&gt;@&lt;account&gt;.blob.core.windows.net/&lt;path&gt; \\ --conf fs.azure.account.key.Account.blob.core.windows.net=yyy \\ OBS#Download paimon-obs-1.2.0.jar.FlinkIf you have already configured obs access through Flink (Via Flink FileSystem), here you can skip the following configuration.Put paimon-obs-1.2.0.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;obs://&lt;bucket&gt;/&lt;path&gt;&#39;, &#39;fs.obs.endpoint&#39; = &#39;obs-endpoint-hostname&#39;, &#39;fs.obs.access.key&#39; = &#39;xxx&#39;, &#39;fs.obs.secret.key&#39; = &#39;yyy&#39; ); SparkIf you have already configured obs access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.Place paimon-obs-1.2.0.jar together with paimon-spark-1.2.0.jar under Spark&rsquo;s jars directory, and start like
spark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=obs://&lt;bucket&gt;/&lt;path&gt; \\ --conf spark.sql.catalog.paimon.fs.obs.endpoint=obs-endpoint-hostname \\ --conf spark.sql.catalog.paimon.fs.obs.access.key=xxx \\ --conf spark.sql.catalog.paimon.fs.obs.secret.key=yyy HiveIf you have already configured obs access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.NOTE: You need to ensure that Hive metastore can access obs.
Place paimon-obs-1.2.0.jar together with paimon-hive-connector-1.2.0.jar under Hive&rsquo;s auxlib directory, and start like
SET paimon.fs.obs.endpoint=obs-endpoint-hostname; SET paimon.fs.obs.access.key=xxx; SET paimon.fs.obs.secret.key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore
SELECT * FROM test_table; SELECT COUNT(1) FROM test_table; `}),e.add({id:1,href:"/migration/migration-from-hive/",title:"Migration From Hive",section:"Migration",content:`Hive Table Migration#Apache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table.
Now, we can use paimon hive catalog with Migrate Table Procedure to totally migrate a table from hive to paimon. At the same time, you can use paimon hive catalog with Migrate Database Procedure to fully synchronize all tables in the database to paimon.
Migrate Table Procedure: Paimon table does not exist, use the procedure upgrade hive table to paimon table. Hive table will disappear after action done. Migrate Database Procedure: Paimon table does not exist, use the procedure upgrade all hive tables in database to paimon table. All hive tables will disappear after action done. These three actions now support file format of hive &ldquo;orc&rdquo; and &ldquo;parquet&rdquo; and &ldquo;avro&rdquo;.
We highly recommend to back up hive table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. Migrate Hive Table#Flink SQLCREATE CATALOG PAIMON WITH ( &#39;type&#39;=&#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, &#39;uri&#39; = &#39;thrift://localhost:9083&#39;, &#39;warehouse&#39;=&#39;/path/to/warehouse/&#39;); USE CATALOG PAIMON; CALL sys.migrate_table( connector =&gt; &#39;hive&#39;, source_table =&gt; &#39;default.hivetable&#39;, -- You can specify the target table, and if the target table already exists -- the file will be migrated directly to it -- target_table =&gt; &#39;default.paimontarget&#39;, -- You can specify delete_origin is false, this won&#39;t delete hivetable -- delete_origin =&gt; false, options =&gt; &#39;file.format=orc&#39;); Flink Action&lt;FLINK_HOME&gt;/flink run ./paimon-flink-action-1.2.0.jar \\ migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --table default.hive_or_paimon After invoke, &ldquo;hivetable&rdquo; will totally convert to paimon format. Writing and reading the table by old &ldquo;hive way&rdquo; will fail.
Migrate Hive Database#Flink SQLCREATE CATALOG PAIMON WITH ( &#39;type&#39;=&#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, &#39;uri&#39; = &#39;thrift://localhost:9083&#39;, &#39;warehouse&#39;=&#39;/path/to/warehouse/&#39;); USE CATALOG PAIMON; CALL sys.migrate_database( connector =&gt; &#39;hive&#39;, source_database =&gt; &#39;default&#39;, options =&gt; &#39;file.format=orc&#39;); Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ migrate_databse \\ --warehouse &lt;warehouse-path&gt; \\ --source_type hive \\ --database &lt;database&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--options &lt;paimon-table-conf [,paimon-table-conf ...]&gt; ] Example:
&lt;FLINK_HOME&gt;/flink run ./paimon-flink-action-1.2.0.jar migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --database default After invoke, all tables in &ldquo;default&rdquo; database will totally convert to paimon format. Writing and reading the table by old &ldquo;hive way&rdquo; will fail.
`}),e.add({id:2,href:"/migration/migration-from-iceberg/",title:"Migration From Iceberg",section:"Migration",content:`Iceberg Migration#Apache Iceberg data with parquet file format could be migrated to Apache Paimon. When migrating an iceberg table to a paimon table, the origin iceberg table will permanently disappear. So please back up your data if you still need the original table. The migrated paimon table will be an append table.
We highly recommend to back up iceberg table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. Migrate Iceberg Table#Currently, we can use paimon catalog with MigrateIcebergTableProcedure or MigrateIcebergTableAction to migrate the data used by latest iceberg snapshot in an iceberg table to a paimon table.
Iceberg tables managed by hadoop-catalog or hive-catalog are supported to be migrated to paimon. As for the type of paimon catalog, it needs to have access to the file system where the iceberg metadata and data files are located. This means we could migrate an iceberg table managed by hadoop-catalog to a paimon table in hive catalog if their warehouses are in the same file system.
When migrating, the iceberg data files which were marked by DELETED will be ignored. Only the data files referenced by manifest entries with &lsquo;EXISTING&rsquo; and &lsquo;ADDED&rsquo; content will be migrated to paimon. Notably, now we don&rsquo;t support migrating iceberg tables with delete files(deletion vectors, position delete files, equality delete files etc.)
Now only parquet format is supported in iceberg migration.
MigrateIcebergTableProcedure#You can run the following command to migrate an iceberg table to a paimon table.
-- Use named argument CALL sys.migrate_iceberg_table(source_table =&gt; &#39;database_name.table_name&#39;, iceberg_options =&gt; &#39;iceberg_options&#39;, options =&gt; &#39;paimon_options&#39;, parallelism =&gt; parallelism); -- Use indexed argument CALL sys.migrate_iceberg_table(&#39;source_table&#39;,&#39;iceberg_options&#39;, &#39;options&#39;, &#39;parallelism&#39;); source_table, string type, is used to specify the source iceberg table to migrate, it&rsquo;s required. iceberg_options, string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it&rsquo;s required. options, string type, is used to specify the additional options for the target paimon table, it&rsquo;s optional. parallelism, integer type, is used to specify the parallelism of the migration job, it&rsquo;s optional. hadoop-catalog#To migrate iceberg table managed by hadoop-catalog, you need set metadata.iceberg.storage=hadoop-catalog and iceberg_warehouse. Example:
CREATE CATALOG paimon_catalog WITH (&#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;/path/to/paimon/warehouse&#39;); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =&gt; &#39;iceberg_db.iceberg_tbl&#39;, iceberg_options =&gt; &#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse&#39; ); If you want the metadata of the migrated paimon table to be managed by hive, you can also create a hive catalog of paimon for migration. Example:
CREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, &#39;uri&#39; = &#39;thrift://&lt;host&gt;:&lt;port&gt;&#39;, &#39;warehouse&#39; = &#39;/path/to/paimon/warehouse&#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =&gt; &#39;iceberg_db.iceberg_tbl&#39;, iceberg_options =&gt; &#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse&#39; ); hive-catalog#To migrate iceberg table managed by hive-catalog, you need set metadata.iceberg.storage=hive-catalog and provide information about Hive Metastore used by the iceberg table in iceberg_options.
OptionDefaultTypeDescriptionmetadata.iceberg.urinoneStringHive metastore uri for Iceberg Hive catalog.metadata.iceberg.hive-conf-dirnoneStringhive-conf-dir for Iceberg Hive catalog.metadata.iceberg.hadoop-conf-dirnoneStringhadoop-conf-dir for Iceberg Hive catalog.metadata.iceberg.hive-client-classorg.apache.hadoop.hive.metastore.HiveMetaStoreClientStringHive client class name for Iceberg Hive Catalog.Example:
CREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, &#39;uri&#39; = &#39;thrift://&lt;host&gt;:&lt;port&gt;&#39;, &#39;warehouse&#39; = &#39;/path/to/paimon/warehouse&#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =&gt; &#39;iceberg_db.iceberg_tbl&#39;, iceberg_options =&gt; &#39;metadata.iceberg.storage=hive-catalog,metadata.iceberg.uri=thrift://&lt;host&gt;:&lt;port&gt;&#39; ); MigrateIcebergTableAction#You can also use flink action for migration:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ migrate_iceberg_table \\ --table &lt;icebergDatabase.icebergTable&gt; \\ --iceberg_options &lt;iceberg-conf [,iceberg-conf ...]&gt; \\ [--parallelism &lt;parallelism&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--options &lt;paimon-table-conf [,paimon-table-conf ...]&gt; ] Example:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ migrate_iceberg_table \\ --table iceberg_db.iceberg_tbl \\ --iceberg_options metadata.iceberg.storage=hive-catalog, metadata.iceberg.uri=thrift://localhost:9083 \\ --parallelism 6 \\ --catalog_conf warehouse=/path/to/paimon/warehouse \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://localhost:9083 `}),e.add({id:3,href:"/append-table/overview/",title:"Overview",section:"Table w/o PK",content:`Overview#If a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through upsert. It can only receive incoming data from append data.
FlinkCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( -- &#39;target-file-size&#39; = &#39;256 MB&#39;, -- &#39;file.format&#39; = &#39;parquet&#39;, -- &#39;file.compression&#39; = &#39;zstd&#39;, -- &#39;file.compression.zstd-level&#39; = &#39;3&#39; ); Batch write and batch read in typical application scenarios, similar to a regular Hive partition table, but compared to the Hive table, it can bring:
Object storage (S3, OSS) friendly Time Travel and Rollback DELETE / UPDATE with low cost Automatic small file merging in streaming sink Streaming read &amp; write like a queue High performance query with order and index `}),e.add({id:4,href:"/cdc-ingestion/overview/",title:"Overview",section:"CDC Ingestion",content:`Overview#Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.
We currently support the following sync ways:
MySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database. Program API Sync: synchronize your custom DataStream input into one Paimon table. Kafka Synchronizing Table: synchronize one Kafka topic&rsquo;s table into one Paimon table. Kafka Synchronizing Database: synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. MongoDB Synchronizing Collection: synchronize one Collection from MongoDB into one Paimon table. MongoDB Synchronizing Database: synchronize the whole MongoDB database into one Paimon database. Pulsar Synchronizing Table: synchronize one Pulsar topic&rsquo;s table into one Paimon table. Pulsar Synchronizing Database: synchronize one Pulsar topic containing multiple tables or multiple topics containing one table each into one Paimon database. What is Schema Evolution#Suppose we have a MySQL table named tableA, it has three fields: field_1, field_2, field_3. When we want to load this MySQL table to Paimon, we can do this in Flink SQL, or use MySqlSyncTableAction.
Flink SQL:
In Flink SQL, if we change the table schema of the MySQL table after the ingestion, the table schema change will not be synchronized to Paimon.
MySqlSyncTableAction:
In MySqlSyncTableAction, if we change the table schema of the MySQL table after the ingestion, the table schema change will be synchronized to Paimon, and the data of field_4 which is newly added will be synchronized to Paimon too.
Schema Change Evolution#Cdc Ingestion supports a limited number of schema changes. Currently, the framework can not rename table, drop columns, so the behaviors of RENAME TABLE and DROP COLUMN will be ignored, RENAME COLUMN will add a new column. Currently supported schema changes includes:
Adding columns.
Altering column types. More specifically,
altering from a string type (char, varchar, text) to another string type with longer length, altering from a non-string type to string type (char, varchar, text), altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range, are supported.
Computed Functions#--computed_column are the definitions of computed columns. The argument field is from source table field name.
Temporal Functions#Temporal functions can convert date and epoch time to another form. A common use case is to generate partition values.
FunctionDescriptionyear(temporal-column [, precision])Extract year from the input. Output is an INT value represent the year.month(temporal-column [, precision])Extract month of year from the input. Output is an INT value represent the month of year.day(temporal-column [, precision])Extract day of month from the input. Output is an INT value represent the day of month.hour(temporal-column [, precision])Extract hour from the input. Output is an INT value represent the hour.minute(temporal-column [, precision])Extract minute from the input. Output is an INT value represent the minute.second(temporal-column [, precision])Extract second from the input. Output is an INT value represent the second.date_format(temporal-column, format-string [, precision])Convert the input to desired formatted string. Output type is STRING.now()Get the timestamp when ingesting the record. Output type is TIMESTAMP_LTZ(3).The data type of the temporal-column can be one of the following cases:
DATE, DATETIME or TIMESTAMP. Any integer numeric type (such as INT and BIGINT). In this case, the data will be considered as epoch time of 1970-01-01 00:00:00. You should set precision of the value (default is 0). STRING. In this case, if you didn&rsquo;t set the time unit, the data will be considered as formatted string of DATE, DATETIME or TIMESTAMP value. Otherwise, the data will be considered as string value of epoch time. So you must set time unit in the latter case. The precision represents the unit of the epoch time. Currently, There are four valid precisions: 0 (for epoch seconds), 3 (for epoch milliseconds), 6(for epoch microseconds) and 9 (for epoch nanoseconds). Take the time point 1970-01-01 00:00:00.123456789 as an example, the epoch seconds are 0, the epoch milliseconds are 123, the epoch microseconds are 123456, and the epoch nanoseconds are 123456789. The precision should match the input values. You can set precision in this way: date_format(epoch_col, yyyy-MM-dd, 0).
date_format is a flexible function which is able to convert the temporal value to various formats with different format strings. A most common format string is yyyy-MM-dd HH:mm:ss.SSS. Another example is yyyy-ww which can extract the year and the week-of-the-year from the input. Note that the output is affected by the locale. For example, in some regions the first day of a week is Monday while in others is Sunday, so if you use date_format(date_col, yyyy-ww) and the input of date_col is 2024-01-07 (Sunday), the output maybe 2024-01 (if the first day of a week is Monday) or 2024-02 (if the first day of a week is Sunday).
Other Functions#FunctionDescriptionsubstring(column,beginInclusive)Get column.substring(beginInclusive). Output is a STRING.substring(column,beginInclusive,endExclusive)Get column.substring(beginInclusive,endExclusive). Output is a STRING.truncate(column,width)Truncate column by width. Output type is the same with column. If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely \`value.substring(0, width)\`.If the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm \`v - (((v % W) + W) % W)\`. The \`redundant\` compute part is to keep the result always positive.If the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let \`scaled_W = decimal(W, scale(v))\`, then return \`v - (v % scaled_W)\`.cast(value,dataType)Get a constant value. The output is an atomic type, such as STRING, INT, BOOLEAN, etc.upper(value)Convert string column to upper case. The input should be a STRING and the output is a STRING.lower(value)Convert string column to lower case. The input should be a STRING and the output is a STRING.trim(value)Trim string column. The input should be a STRING and the output is a STRING.Special Data Type Mapping#It is possible that some data types of upstream systems cannot be directly mapped to Paimon data types. We have some special data types mapping rules:
MySQL TINYINT(1) type will be mapped to Boolean. MySQL BIT(1) type will be mapped to Boolean. MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL will be mapped to DECIMAL(20, 0). MySQL BINARY will be mapped to Paimon VARBINARY. This is because the binary value is passed as bytes in binlog, so it should be mapped to byte type (BYTES or VARBINARY). We choose VARBINARY because it can retain the length information. Some upstream systems may not pass decimal precision and scale information. In this case, we will use DECIMAL(38, 18). When using Hive catalog, MySQL TIME type will be mapped to STRING. We provide some options to customize the mapping rules. Please use --type_mapping option1,option2,... to specify them:
tinyint1-not-bool: Map MySQL TINYINT(1) to Paimon TINYINT instead of Boolean. to-nullable: Ignore all NOT NULL constraints (except primary keys). to-string: Map all MySQL data type to STRING. char-to-string: Map MySQL CHAR(length)/VARCHAR(length) types to STRING. longtext-to-bytes: Map MySQL LONGTEXT types to BYTES. decimal_no_change: Avoid that Paimon CDC framework automatically use DECIMAL(38, 18). bigint-unsigned-to-bigint: Map MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to Paimon BIGINT, but there is potential data overflow because BIGINT UNSIGNED can store up to 20 digits integer value but Paimon BIGINT can only store up to 19 digits integer value. So you should ensure the overflow won&rsquo;t occur when using this option. allow_non_string_to_string: Schema change doesn&rsquo;t support non-string type to string by default. Use this option to allow this change. Custom Job Settings#Checkpointing#Use -Dexecution.checkpointing.interval=&lt;interval&gt; to enable checkpointing and set interval. For 0.7 and later versions, if you haven&rsquo;t enabled checkpointing, Paimon will enable checkpointing by default and set checkpoint interval to 180 seconds.
Job Name#Use -Dpipeline.name=&lt;job-name&gt; to set custom synchronization job name.
table configuration#You can use --table_conf to set table properties and some flink job properties (like sink.parallelism). If the table is created by the cdc job, the table&rsquo;s properties will be equal to the given properties. Otherwise, the job will use the given properties to alter table&rsquo;s properties. But note that immutable options (like merge-engine) and bucket number won&rsquo;t be altered.
`}),e.add({id:5,href:"/concepts/rest/overview/",title:"Overview",section:"RESTCatalog",content:`RESTCatalog#Overview#Paimon REST Catalog provides a lightweight implementation to access the catalog service. Paimon could access the catalog service through a catalog server which implements REST API. You can see all APIs in REST API.
Key Features#User Defined Technology-Specific Logic Implementation All technology-specific logic within the catalog server. This ensures that the user can define logic that could be owned by the user. Decoupled Architecture The REST Catalog interacts with the catalog server through a well-defined REST API. This decoupling allows for independent evolution and scaling of the catalog server and clients. Language Agnostic Developers can implement the catalog server in any programming language, provided that it adheres to the specified REST API. This flexibility enables teams to utilize their existing tech stacks and expertise. Support for Any Catalog Backend REST Catalog is designed to work with any catalog backend. As long as they implement the relevant APIs, they can seamlessly integrate with REST Catalog. Conclusion#REST Catalog offers adaptable solution for accessing the catalog service. According to REST API is decoupled from the catalog service.
Technology-specific Logic is encapsulated on the catalog server. At the same time, the catalog server supports any backend and languages.
Token Provider#RESTCatalog supports multiple access authentication methods, including the following:
Bear Token. DLF Token. REST Open API#See REST API.
REST Java API#See REST Java API.
`}),e.add({id:6,href:"/ecosystem/overview/",title:"Overview",section:"Ecosystem",content:`Overview#Compatibility Matrix#Engine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite DELETE &amp; UPDATE MERGE INTO Time Travel Flink 1.15 - 1.20 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅ ✅(1.17+) ❌ ✅ Spark 3.2 - 3.5 ✅ ✅ ✅ ✅ ✅(3.3+) ✅(3.3+) ✅ ✅ ✅ ✅(3.3+) Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Trino 420 - 440 ✅ ✅(427+) ✅(427+) ✅(427+) ❌ ❌ ❌ ❌ ❌ ✅ Presto 0.236 - 0.280 ✅ ❌ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ StarRocks 3.1+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Doris 2.0.6+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Streaming Engines#Flink Streaming#Flink is the most comprehensive streaming computing engine that is widely used for data CDC ingestion and the construction of streaming pipelines.
Recommended version is Flink 1.17.2.
Spark Streaming#You can also use Spark Streaming to build a streaming pipeline. Spark&rsquo;s schema evolution capability will be better implemented, but you must accept the mechanism of mini-batch.
Batch Engines#Spark Batch#Spark Batch is the most widely used batch computing engine.
Recommended version is Spark 3.4.3.
Flink Batch#Flink Batch is also available, which can make your pipeline more integrated with streaming and batch unified.
OLAP Engines#StarRocks#StarRocks is the most recommended OLAP engine with the most advanced integration.
Recommended version is StarRocks 3.2.6.
Other OLAP#You can also use Doris and Trino and Presto, or, you can just use Spark, Flink and Hive to query Paimon tables.
Download#Download Link
`}),e.add({id:7,href:"/iceberg/overview/",title:"Overview",section:"Iceberg Metadata",content:`Overview#Paimon supports generating Iceberg compatible metadata, so that Paimon tables can be consumed directly by Iceberg readers.
Set the following table options, so that Paimon tables can generate Iceberg compatible metadata.
OptionDefaultTypeDescriptionmetadata.iceberg.storagedisabledEnumWhen set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.disabled: Disable Iceberg compatibility support.table-location: Store Iceberg metadata in each table's directory.hadoop-catalog: Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog.hive-catalog: Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.metadata.iceberg.storage-location(none)EnumSpecifies where to store Iceberg metadata files. If not set, the storage location will default based on the selected metadata.iceberg.storage type.table-location: Store Iceberg metadata in each table's directory. Useful for standalone Iceberg tables or Iceberg Java API access. Can also be used with Hive Catalog.catalog-location: Store Iceberg metadata in a separate directory. This is the default behavior when using Hive Catalog or Hadoop Catalog.For most SQL users, we recommend setting 'metadata.iceberg.storage' = 'hadoop-catalog' or 'metadata.iceberg.storage' = 'hive-catalog', so that all tables can be visited as an Iceberg warehouse. For Iceberg Java API users, you might consider setting 'metadata.iceberg.storage' = 'table-location', so you can visit each table with its table path. When using metadata.iceberg.storage = hadoop-catalog or hive-catalog, you can optionally configure metadata.iceberg.storage-location to control where the metadata is stored. If not set, the default behavior depends on the storage type.
Supported Types#Paimon Iceberg compatibility currently supports the following data types.
Paimon Data Type Iceberg Data Type BOOLEAN boolean INT int BIGINT long FLOAT float DOUBLE double DECIMAL decimal CHAR string VARCHAR string BINARY binary VARBINARY binary DATE date TIMESTAMP* timestamp TIMESTAMP_LTZ* timestamptz ARRAY list MAP map ROW struct *: TIMESTAMP and TIMESTAMP_LTZ type only support precision from 4 to 6
`}),e.add({id:8,href:"/primary-key-table/merge-engine/overview/",title:"Overview",section:"Merge Engine",content:`Overview#When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.
Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.Deduplicate#The deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.
Specifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted. You can config ignore-delete to ignore it.
`}),e.add({id:9,href:"/primary-key-table/overview/",title:"Overview",section:"Table with PK",content:`Overview#If you define a table with primary key, you can insert, update or delete records in the table.
Primary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key. See CREATE TABLE.
Bucket#Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.
Each bucket directory contains an LSM tree and its changelog files.
The range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.
A bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 200MB - 1GB.
Also, see rescale bucket if you want to adjust the number of buckets after a table is created.
LSM Trees#Paimon adopts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.
Sorted Runs#LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.
Records within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.
As you can see, different sorted runs may have overlapped primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.
New records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.
`}),e.add({id:10,href:"/flink/quick-start/",title:"Quick Start",section:"Engine Flink",content:`Quick Start#This documentation is a guide for using Paimon in Flink.
Jars#Paimon currently supports Flink 2.0, 1.20, 1.19, 1.18, 1.17, 1.16, 1.15. We recommend the latest Flink version for a better experience.
Download the jar file with corresponding version.
Currently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction, Version Type Jar Flink 2.0 Bundled Jar paimon-flink-2.0-1.2.0.jar Flink 1.20 Bundled Jar paimon-flink-1.20-1.2.0.jar Flink 1.19 Bundled Jar paimon-flink-1.19-1.2.0.jar Flink 1.18 Bundled Jar paimon-flink-1.18-1.2.0.jar Flink 1.17 Bundled Jar paimon-flink-1.17-1.2.0.jar Flink 1.16 Bundled Jar paimon-flink-1.16-1.2.0.jar Flink 1.15 Bundled Jar paimon-flink-1.15-1.2.0.jar Flink Action Action Jar paimon-flink-action-1.2.0.jar You can also manually build bundled jar from the source code.
To build from source code, clone the git repository.
Build bundled jar with the following command.
mvn clean install -DskipTests You can find the bundled jar in ./paimon-flink/paimon-flink-&lt;flink-version&gt;/target/paimon-flink-&lt;flink-version&gt;-1.2.0.jar, and the action jar in ./paimon-flink/paimon-flink-action/target/paimon-flink-action-1.2.0.jar.
Start#Step 1: Download Flink
If you haven&rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.
tar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar
Copy paimon bundled jar to the lib directory of your Flink home.
cp paimon-flink-*.jar &lt;FLINK_HOME&gt;/lib/ Step 3: Copy Hadoop Bundled Jar
If the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.Download Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.
cp flink-shaded-hadoop-2-uber-*.jar &lt;FLINK_HOME&gt;/lib/ Step 4: Start a Flink Local Cluster
In order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in &lt;FLINK_HOME&gt;/conf/flink-conf.yaml(Flink version &lt; 1.19) or &lt;FLINK_HOME&gt;/conf/config.yaml(Flink version &gt;= 1.19).
taskmanager.numberOfTaskSlots: 2 To start a local cluster, run the bash script that comes with Flink:
&lt;FLINK_HOME&gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.
You can now start Flink SQL client to execute SQL scripts.
&lt;FLINK_HOME&gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table
Catalog-- if you&#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( &#39;type&#39;=&#39;paimon&#39;, &#39;warehouse&#39;=&#39;file:/tmp/paimon&#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Generic-CatalogUsing FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!
In this mode, you should use &lsquo;connector&rsquo; option for creating tables.
Paimon will use hive.metastore.warehouse.dir in your hive-site.xml, please use path with scheme. For example, hdfs://.... Otherwise, Paimon will use the local path.CREATE CATALOG my_catalog WITH ( &#39;type&#39;=&#39;paimon-generic&#39;, &#39;hive-conf-dir&#39;=&#39;...&#39;, &#39;hadoop-conf-dir&#39;=&#39;...&#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ) WITH ( &#39;connector&#39;=&#39;paimon&#39; ); Step 6: Write Data
-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( &#39;connector&#39; = &#39;datagen&#39;, &#39;fields.word.length&#39; = &#39;1&#39; ); -- paimon requires checkpoint interval in streaming mode SET &#39;execution.checkpointing.interval&#39; = &#39;10 s&#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query
-- use tableau result mode SET &#39;sql-client.execution.result-mode&#39; = &#39;tableau&#39;; -- switch to batch mode RESET &#39;execution.checkpointing.interval&#39;; SET &#39;execution.runtime-mode&#39; = &#39;batch&#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.
Step 8: Streaming Query
-- switch to streaming mode SET &#39;execution.runtime-mode&#39; = &#39;streaming&#39;; -- track the changes of table and calculate the count interval statistics SELECT \`interval\`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS \`interval\` FROM word_count) GROUP BY \`interval\`; Step 9: Exit
Cancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.
-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count; -- exit sql-client EXIT; Stop the Flink local cluster.
./bin/stop-cluster.sh Use Flink Managed Memory#Paimon tasks can create memory pools based on executor memory which will be managed by Flink executor, such as managed memory in Flink task manager. It will improve the stability and performance of sinks by managing writer buffers for multiple tasks through executor.
The following properties can be set if using Flink managed memory:
Option Default Description sink.use-managed-memory-allocator false If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator, which means each task allocates and manages its own memory pool (heap memory), if there are too many tasks in one Executor, it may cause performance issues and even OOM. sink.managed.writer-buffer-memory 256M Weight of writer buffer in managed memory, Flink will compute the memory size, for writer according to the weight, the actual memory used depends on the running environment. Now the memory size defined in this property are equals to the exact memory allocated to write buffer in runtime. Use In SQL Users can set memory weight in SQL for Flink Managed Memory, then Flink sink operator will get the memory pool size and create allocator for Paimon writer.
INSERT INTO paimon_table /*+ OPTIONS(&#39;sink.use-managed-memory-allocator&#39;=&#39;true&#39;, &#39;sink.managed.writer-buffer-memory&#39;=&#39;256M&#39;) */ SELECT * FROM ....; Setting dynamic options#When interacting with the Paimon table, table options can be tuned without changing the options in the catalog. Paimon will extract job-level dynamic options and take effect in the current session. The dynamic table option&rsquo;s key format is paimon.\${catalogName}.\${dbName}.\${tableName}.\${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. The dynamic global option&rsquo;s key format is \${config_key}. Global options will take effect for all the tables. Table options will override global options if there are conflicts.
For example:
-- set scan.timestamp-millis=1697018249001 for all tables SET &#39;scan.timestamp-millis&#39; = &#39;1697018249001&#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T SET &#39;paimon.mycatalog.default.T.scan.timestamp-millis&#39; = &#39;1697018249000&#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table default.T in any catalog SET &#39;paimon.*.default.T.scan.timestamp-millis&#39; = &#39;1697018249000&#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T1 -- set scan.timestamp-millis=1697018249001 for others tables SET &#39;paimon.mycatalog.default.T1.scan.timestamp-millis&#39; = &#39;1697018249000&#39;; SET &#39;scan.timestamp-millis&#39; = &#39;1697018249001&#39;; SELECT * FROM T1 JOIN T2 ON xxxx; `}),e.add({id:11,href:"/spark/quick-start/",title:"Quick Start",section:"Engine Spark",content:`Quick Start#Preparation#Paimon currently supports Spark 3.5, 3.4, 3.3, and 3.2. We recommend the latest Spark version for a better experience.
Download the jar file with corresponding version.
Version Jar Spark 3.5 paimon-spark-3.5-1.2.0.jar Spark 3.4 paimon-spark-3.4-1.2.0.jar Spark 3.3 paimon-spark-3.3-1.2.0.jar Spark 3.2 paimon-spark-3.2-1.2.0.jar You can also manually build bundled jar from the source code.
To build from source code, clone the git repository.
Build bundled jar with the following command.
mvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-1.2.0.jar.
Setup#If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.Step 1: Specify Paimon Jar File
Append path to paimon jar file to the --jars argument when starting spark-sql.
spark-sql ... --jars /path/to/paimon-spark-3.3-1.2.0.jar OR use the --packages option.
spark-sql ... --packages org.apache.paimon:paimon-spark-3.3:1.2.0 Alternatively, you can copy paimon-spark-3.3-1.2.0.jar under spark/jars in your Spark installation directory.
Step 2: Specify Paimon Catalog
CatalogWhen starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.
spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon \\ --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Catalogs are configured using properties under spark.sql.catalog.(catalog_name). In above case, &lsquo;paimon&rsquo; is the catalog name, you can change it to your own favorite catalog name.
After spark-sql command line has started, run the following SQL to create and switch to database default.
USE paimon; USE default; After switching to the catalog ('USE paimon'), Spark&rsquo;s existing tables will not be directly accessible, you can use the spark_catalog.\${database_name}.\${table_name} to access Spark tables.
Generic CatalogWhen starting spark-sql, use the following command to register Paimon’s Spark Generic catalog to replace Spark default catalog spark_catalog. (default warehouse is Spark spark.sql.warehouse.dir)
Currently, it is only recommended to use SparkGenericCatalog in the case of Hive metastore, Paimon will infer Hive conf from Spark session, you just need to configure Spark&rsquo;s Hive conf.
spark-sql ... \\ --conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog \\ --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Using SparkGenericCatalog, you can use Paimon tables in this Catalog or non-Paimon tables such as Spark&rsquo;s csv, parquet, Hive tables, etc.
Create Table#Catalogcreate table my_table ( k int, v string ) tblproperties ( &#39;primary-key&#39; = &#39;k&#39; ); Generic Catalogcreate table my_table ( k int, v string ) USING paimon tblproperties ( &#39;primary-key&#39; = &#39;k&#39; ); Insert Table#SQLINSERT INTO my_table VALUES (1, &#39;Hi&#39;), (2, &#39;Hello&#39;); DataFrame-- you can use Seq((1, &#34;Hi&#34;), (2, &#34;Hello&#34;)).toDF(&#34;k&#34;, &#34;v&#34;) .write.format(&#34;paimon&#34;).mode(&#34;append&#34;).saveAsTable(&#34;my_table&#34;) -- or Seq((1, &#34;Hi&#34;), (2, &#34;Hello&#34;)).toDF(&#34;k&#34;, &#34;v&#34;) .write.format(&#34;paimon&#34;).mode(&#34;append&#34;).save(&#34;file:/tmp/paimon/default.db/my_table&#34;) Query Table#SQLSELECT * FROM my_table; /* 1	Hi 2	Hello */ DataFrame-- you can use spark.read.format(&#34;paimon&#34;).table(&#34;my_table&#34;).show() -- or spark.read.format(&#34;paimon&#34;).load(&#34;file:/tmp/paimon/default.db/my_table&#34;).show() /* +---+------+ | k | v| +---+------+ | 1| Hi| | 2| Hello| +---+------+ */ Spark Type Conversion#This section lists all supported type conversion between Spark and Paimon. All Spark&rsquo;s data types are available in package org.apache.spark.sql.types.
Spark Data TypePaimon Data TypeAtomic TypeStructTypeRowTypefalseMapTypeMapTypefalseArrayTypeArrayTypefalseBooleanTypeBooleanTypetrueByteTypeTinyIntTypetrueShortTypeSmallIntTypetrueIntegerTypeIntTypetrueLongTypeBigIntTypetrueFloatTypeFloatTypetrueDoubleTypeDoubleTypetrueStringTypeVarCharType(Integer.MAX_VALUE)trueVarCharType(length)VarCharType(length)trueCharType(length)CharType(length)trueDateTypeDateTypetrueTimestampTypeLocalZonedTimestamptrueTimestampNTZType(Spark3.4+)TimestampTypetrueDecimalType(precision, scale)DecimalType(precision, scale)trueBinaryTypeVarBinaryType, BinaryTypetrueDue to the previous design, in Spark3.3 and below, Paimon will map both Paimon&rsquo;s TimestampType and LocalZonedTimestamp to Spark&rsquo;s TimestampType, and only correctly handle with TimestampType.
Therefore, when using Spark3.3 and below, reads Paimon table with LocalZonedTimestamp type written by other engines, such as Flink, the query result of LocalZonedTimestamp type will have time zone offset, which needs to be adjusted manually.
When using Spark3.4 and above, all timestamp types can be parsed correctly.
`}),e.add({id:12,href:"/program-api/rest-api/",title:"REST API",section:"Program API",content:`REST API#This is Java API for REST.
Dependency#Maven dependency:
&lt;dependency&gt; &lt;groupId&gt;org.apache.paimon&lt;/groupId&gt; &lt;artifactId&gt;paimon-api&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; Or download the jar file: Paimon API.RESTApi#import org.apache.paimon.options.Options; import org.apache.paimon.rest.RESTApi; import java.util.List; import static org.apache.paimon.options.CatalogOptions.WAREHOUSE; import static org.apache.paimon.rest.RESTCatalogOptions.DLF_ACCESS_KEY_ID; import static org.apache.paimon.rest.RESTCatalogOptions.DLF_ACCESS_KEY_SECRET; import static org.apache.paimon.rest.RESTCatalogOptions.TOKEN; import static org.apache.paimon.rest.RESTCatalogOptions.TOKEN_PROVIDER; import static org.apache.paimon.rest.RESTCatalogOptions.URI; public class RESTApiExample { public static void main(String[] args) { Options options = new Options(); options.set(URI, &#34;&lt;catalog server url&gt;&#34;); options.set(WAREHOUSE, &#34;my_instance_name&#34;); setBearToken(options); // or setDlfToken RESTApi api = new RESTApi(options); List&lt;String&gt; tables = api.listTables(&#34;my_database&#34;); System.out.println(tables); } private static void setBearToken(Options options) { options.set(TOKEN_PROVIDER, &#34;bear&#34;); options.set(TOKEN, &#34;&lt;token&gt;&#34;); } private static void setDlfToken(Options options) { options.set(TOKEN_PROVIDER, &#34;dlf&#34;); options.set(DLF_ACCESS_KEY_ID, &#34;&lt;access-key-id&gt;&#34;); options.set(DLF_ACCESS_KEY_SECRET, &#34;&lt;access-key-secret&gt;&#34;); } } See more methods in 'RESTApi'.
`}),e.add({id:13,href:"/project/roadmap/",title:"Roadmap",section:"Project",content:`Roadmap#Flink Lookup Join#Support Flink Custom Data Distribution Lookup Join to reach large-scale data lookup join.
Produce Iceberg snapshots#Introduce a mode to produce Iceberg snapshots.
Variant Type#Support Variant Type with Spark 4.0 and Flink 2.0. Unlocking support for semi-structured data.
File Index#Add more index:
Inverse Vector Compaction#Support Vector Compaction for super Wide Table.
Function support#Paimon Catalog supports functions.
Files Schema Evolution Ingestion#Introduce a files Ingestion with Schema Evolution.
`}),e.add({id:14,href:"/learn-paimon/understand-files/",title:"Understand Files",section:"Learn Paimon",content:`Understand Files#This article is specifically designed to clarify the impact that various file operations have on files.
This page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.
Prerequisite#Before delving further into this page, please ensure that you have read through the following sections:
Basic Concepts, Primary Key Table and Append Table How to use Paimon in Flink. Understand File Operations#Create Catalog#Start Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.
CREATE CATALOG paimon WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;file:///tmp/paimon&#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.
Create Table#Execute the following create table statement will create a Paimon table with 3 fields:
CREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT &#39;timestamp string in format yyyyMMdd&#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0
Insert Records Into Table#Run the following insert statement in Flink SQL:
INSERT INTO T VALUES (1, 10001, &#39;varchar00001&#39;, &#39;20230501&#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:
The content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:
{ &#34;version&#34; : 3, &#34;id&#34; : 1, &#34;schemaId&#34; : 0, &#34;baseManifestList&#34; : &#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0&#34;, &#34;deltaManifestList&#34; : &#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1&#34;, &#34;changelogManifestList&#34; : null, &#34;commitUser&#34; : &#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf&#34;, &#34;commitIdentifier&#34; : 9223372036854775807, &#34;commitKind&#34; : &#34;APPEND&#34;, &#34;timeMillis&#34; : 1684155393354, &#34;logOffsets&#34; : { }, &#34;totalRecordCount&#34; : 1, &#34;deltaRecordCount&#34; : 1, &#34;changelogRecordCount&#34; : 0, &#34;watermark&#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):
./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1	manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.
manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.
manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.
Now let&rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:
INSERT INTO T VALUES (2, 10002, &#39;varchar00002&#39;, &#39;20230502&#39;), (3, 10003, &#39;varchar00003&#39;, &#39;20230503&#39;), (4, 10004, &#39;varchar00004&#39;, &#39;20230504&#39;), (5, 10005, &#39;varchar00005&#39;, &#39;20230505&#39;), (6, 10006, &#39;varchar00006&#39;, &#39;20230506&#39;), (7, 10007, &#39;varchar00007&#39;, &#39;20230507&#39;), (8, 10008, &#39;varchar00008&#39;, &#39;20230508&#39;), (9, 10009, &#39;varchar00009&#39;, &#39;20230509&#39;), (10, 10010, &#39;varchar00010&#39;, &#39;20230510&#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:
% ls -1tR . ./T: dt=20230501 dt=20230502	dt=20230503	dt=20230504	dt=20230505	dt=20230506	dt=20230507	dt=20230508	dt=20230509	dt=20230510	snapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1 # delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2	manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0	# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 # delta manifest list for snapshot-1 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table#Now let&rsquo;s delete records that meet the condition dt&gt;=20230503. In Flink SQL, execute the following statement:
BatchDELETE FROM T WHERE dt &gt;= &#39;20230503&#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:
./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:
+I[1, 10001, &#39;varchar00001&#39;, &#39;20230501&#39;]+I[2, 10002, &#39;varchar00002&#39;, &#39;20230502&#39;] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.
Compact Table#As you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.
Let&rsquo;s trigger the full-compaction now, and run a dedicated compaction job through flink run:
BatchFlink SQLCALL sys.compact( \`table\` =&gt; &#39;database_name.table_name&#39;, partitions =&gt; &#39;partition_name&#39;, order_strategy =&gt; &#39;order_strategy&#39;, order_by =&gt; &#39;order_by&#39;, options =&gt; &#39;paimon_table_dynamic_conf&#39; ); Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition &lt;partition-name&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-dynamic-conf&gt; [--table_conf &lt;paimon-table-dynamic-conf&gt;] ...] an example would be (suppose you&rsquo;re already in Flink home)
Flink SQLCALL sys.compact(&#39;T&#39;); Flink Action./bin/flink run \\ ./lib/paimon-flink-action-1.2.0.jar \\ compact \\ --path file:///tmp/paimon/default.db/T All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:
{ &#34;version&#34; : 3, &#34;id&#34; : 4, &#34;schemaId&#34; : 0, &#34;baseManifestList&#34; : &#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0&#34;, &#34;deltaManifestList&#34; : &#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1&#34;, &#34;changelogManifestList&#34; : null, &#34;commitUser&#34; : &#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48&#34;, &#34;commitIdentifier&#34; : 9223372036854775807, &#34;commitKind&#34; : &#34;COMPACT&#34;, &#34;timeMillis&#34; : 1684163217960, &#34;logOffsets&#34; : { }, &#34;totalRecordCount&#34; : 2, &#34;deltaRecordCount&#34; : -16, &#34;changelogRecordCount&#34; : 0, &#34;watermark&#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)
For partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file. This is because there has been an upgrade of the file from level 0 to the highest level. Please rest assured that this is only a change in metadata, and the file is still the same. Alter Table#Execute the following statement to configure full-compaction:
ALTER TABLE T SET (&#39;full-compaction.delta-commits&#39; = &#39;1&#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.
Expire Snapshots#Remind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.
During the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.
Let&rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:
It first deletes all marked data files, and records any changed buckets.
It then deletes any changelog files and associated manifests.
Finally, it deletes the snapshots themselves and writes the earliest hint file.
If any directories are left empty after the deletion process, they will be deleted as well.
Let&rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are
to be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:
As a result, partition 20230503 to 20230510 are physically deleted.
Flink Stream Write#Finally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.
To begin, let&rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.
MySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots. Next, we will go over end-to-end data flow.
MySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.
Paimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.
During checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot
contains information about all data files in the table.
At later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.
Understand Small Files#Many users are concerned about small files, which can lead to:
Stability issue: Too many small files in HDFS, NameNode will be overstressed. Cost issue: A small file in HDFS will temporarily use the size of a minimum of one Block, for example 128 MB. Query efficiency: The efficiency of querying too many small files will be affected. Understand Checkpoints#Assuming you are using Flink Writer, each checkpoint generates 1-2 snapshots, and the checkpoint forces the files to be generated on DFS, so the smaller the checkpoint interval the more small files will be generated.
So first thing is increase checkpoint interval. By default, not only checkpoint will cause the file to be generated, but writer&rsquo;s memory (write-buffer-size) exhaustion will also flush data to DFS and generate the corresponding file. You can enable write-buffer-spillable to generate spilled files in writer to generate bigger files in DFS.
So second thing is increase write-buffer-size or enable write-buffer-spillable. Understand Snapshots#Paimon maintains multiple versions of files, compaction and deletion of files are logical and do not actually delete files. Files are only really deleted when Snapshot is expired, so the first way to reduce files is to reduce the time it takes for snapshot to be expired. Flink writer will automatically expire snapshots.
See Expire Snapshots.
Understand Partitions and Buckets#Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.
For example, the following table:
CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( &#39;bucket&#39; = &#39;10&#39; ); The table data will be physically sliced into different partitions, and different buckets inside, so if the overall data volume is too small, there is at least one file in a single bucket, I suggest you configure a smaller number of buckets, otherwise there will be quite a few small files as well.
Understand LSM for Primary Table#LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.
By default, sorted runs number depends on num-sorted-run.compaction-trigger, see Compaction for Primary Key Table, this means that there are at least 5 files in a bucket. If you want to reduce this number, you can keep fewer files, but write performance may suffer.
Understand Files for Bucketed Append Table#By default, Append also does automatic compaction to reduce the number of small files.
However, for Bucketed Append table, it will only compact the files within the Bucket for sequential purposes, which may keep more small files. See Bucketed Append.
Understand Full-Compaction#Maybe you think the 5 files for the primary key table are actually okay, but the Append table (bucket) may have 50 small files in a single bucket, which is very difficult to accept. Worse still, partitions that are no longer active also keep so many small files.
Configure ‘full-compaction.delta-commits’ perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.
`}),e.add({id:15,href:"/migration/upsert-to-partitioned/",title:"Upsert To Partitioned",section:"Migration",content:`Upsert To Partitioned#Note: Only Hive Engine can be used to query these upsert-to-partitioned tables.The Tag Management will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.
When using primary key tables, a non-partitioned approach is often used to maintain updates, in order to mirror and synchronize tables from upstream database tables. This allows users to query the latest data. The tradition of Hive data warehouses is not like this. Offline data warehouses require an immutable view every day to ensure the idempotence of calculations. So we created a Tag mechanism to output these views.
However, the traditional use of Hive data warehouses is more accustomed to using partitions to specify the query&rsquo;s Tag, and is more accustomed to using Hive computing engines.
So, we introduce 'metastore.tag-to-partition' and 'metastore.tag-to-partition.preview' to mapping a non-partitioned primary key table to the partition table in Hive metastore, and mapping the partition field to the name of the Tag to be fully compatible with Hive.
Example for Tag to Partition#Step 1: Create table and tag in Flink SQL
FlinkCREATE CATALOG my_hive WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, -- &#39;uri&#39; = &#39;thrift://&lt;hive-metastore-host-name&gt;:&lt;port&gt;&#39;, default use &#39;hive.metastore.uris&#39; in HiveConf -- &#39;hive-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;hadoop-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;warehouse&#39; = &#39;hdfs:///path/to/table/store/warehouse&#39;, default use &#39;hive.metastore.warehouse.dir&#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( &#39;bucket&#39; = &#39;-1&#39;, &#39;metastore.tag-to-partition&#39; = &#39;dt&#39; ); INSERT INTO t VALUES (1, &#39;10&#39;, &#39;100&#39;), (2, &#39;20&#39;, &#39;200&#39;); -- create tag &#39;2023-10-16&#39; for snapshot 1 CALL sys.create_tag(&#39;mydb.t&#39;, &#39;2023-10-16&#39;, 1); Step 2: Query table in Hive with Partition Pruning
HiveSHOW PARTITIONS t; /* OK dt=2023-10-16 */ SELECT * FROM t WHERE dt=&#39;2023-10-16&#39;; /* OK 1 10 100 2023-10-16 2 20 200 2023-10-16 */ Example for Tag Preview#The above example can only query tags that have already been created, but Paimon is a real-time data lake, and you also need to query the latest data. Therefore, Paimon provides a preview feature:
Step 1: Create table and tag in Flink SQL
FlinkCREATE CATALOG my_hive WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, -- &#39;uri&#39; = &#39;thrift://&lt;hive-metastore-host-name&gt;:&lt;port&gt;&#39;, default use &#39;hive.metastore.uris&#39; in HiveConf -- &#39;hive-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;hadoop-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;warehouse&#39; = &#39;hdfs:///path/to/table/store/warehouse&#39;, default use &#39;hive.metastore.warehouse.dir&#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( &#39;bucket&#39; = &#39;-1&#39;, &#39;metastore.tag-to-partition&#39; = &#39;dt&#39;, -- preview tag creation mode process-time -- paimon will create partitions early based on process-time &#39;metastore.tag-to-partition.preview&#39; = &#39;process-time&#39; ); INSERT INTO t VALUES (1, &#39;10&#39;, &#39;100&#39;), (2, &#39;20&#39;, &#39;200&#39;); -- create tag &#39;2023-10-16&#39; for snapshot 1 CALL sys.create_tag(&#39;mydb.t&#39;, &#39;2023-10-16&#39;, 1); -- new data in &#39;2023-10-17&#39; INSERT INTO t VALUES (3, &#39;30&#39;, &#39;300&#39;), (4, &#39;40&#39;, &#39;400&#39;); -- haven&#39;t finished writing the data for &#39;2023-10-17&#39; yet, so there&#39;s no need to create a tag for now -- but the data is already visible for Hive Step 2: Query table in Hive with Partition Pruning
HiveSHOW PARTITIONS t; /* OK dt=2023-10-16 dt=2023-10-17 */ SELECT * FROM t WHERE dt=&#39;2023-10-17&#39;; -- preview tag &#39;2023-10-17&#39; /* OK 1 10 100 2023-10-17 2 20 200 2023-10-17 3 30 300 2023-10-17 4 40 400 2023-10-17 */ `}),e.add({id:16,href:"/concepts/",title:"概念 Concepts",section:"Apache Paimon",content:``}),e.add({id:17,href:"/concepts/overview/",title:"概述",section:"概念 Concepts",content:`概述#Apache Paimon 的架构：
如上图架构所示：
读写能力： Paimon 支持多样化的数据读写方式及 OLAP 查询。
读取方面，支持 从历史快照读取（批处理模式）， 从最新偏移量读取（流处理模式）， 或以混合方式读取增量快照。 写入方面，支持 从数据库变更日志（CDC）进行流式同步， 从离线数据批量插入或覆盖。 生态系统： 除了 Apache Flink，Paimon 还支持 Apache Spark、StarRocks、Apache Doris、Apache Hive 和 Trino 等计算引擎的读取。
内部实现：
Paimon 底层将列式文件存储在文件系统或对象存储中。 文件的元数据保存在 manifest 文件中，支持大规模存储与数据跳过。 对于主键表，采用 LSM 树结构支持大量数据更新和高性能查询。 统一存储#对于像 Apache Flink 这样的流式引擎，通常有三类连接器：
消息队列，如 Apache Kafka，既用作数据源，也用作流水线中的中间环节，保证延迟维持在秒级以内。 OLAP 系统，如 ClickHouse，以流式方式接收处理后的数据，支持用户的临时查询。 批处理存储，如 Apache Hive，支持传统批处理的各种操作，包括 INSERT OVERWRITE。 Paimon 提供了表抽象，其使用方式与传统数据库无异：
在 batch 执行模式下，表现如 Hive 表，支持多种批量 SQL 操作，可查询最新快照。 在 streaming 执行模式下，表现如消息队列，查询时就像查询一个永不过期的流式变更日志。 `}),e.add({id:18,href:"/concepts/spec/overview/",title:"概述 Overview",section:"规范说明 Specification",content:`Spec Overview#这是 Paimon 表格式的规范，本文档规范了 Paimon 的底层文件结构和设计。
术语#Schema：字段、主键定义、分区键定义及选项。 Snapshot：某一特定时间点提交的所有数据的入口。 Manifest list：包含多个 manifest 文件的列表。 Manifest：包含多个数据文件或变更日志文件。 Data File：包含增量记录。 Changelog File：包含由变更日志生成器产生的记录。 Global Index：桶或分区的索引。 Data File Index：数据文件的索引。 使用 Paimon 运行 Flink SQL：
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;/your/path&#39; ); USE CATALOG my_catalog; CREATE TABLE my_table ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, f1 STRING ); INSERT INTO my_table VALUES (1, 11, &#39;111&#39;); Take a look to the disk:
warehouse └── default.db └── my_table ├── bucket-0 │ └── data-59f60cb9-44af-48cc-b5ad-59e85c663c8f-0.orc ├── index │ └── index-5625e6d9-dd44-403b-a738-2b6ea92e20f1-0 ├── manifest │ ├── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 │ ├── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 │ ├── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-0 │ └── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 `}),e.add({id:19,href:"/iceberg/append-table/",title:"Append Table",section:"Iceberg Metadata",content:"\rAppend Tables\r#\rLet&rsquo;s walk through a simple example, where we query Paimon tables with Iceberg connectors in Flink and Spark. Before trying out this example, make sure that your compute engine already supports Iceberg. Please refer to Iceberg&rsquo;s document if you haven&rsquo;t set up Iceberg.\nFlink: Preparation when using Flink SQL Client Spark: Using Iceberg in Spark 3 Let&rsquo;s now create a Paimon append only table with Iceberg compatibility enabled and insert some data.\nFlink SQL\rCREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;&#39; ); CREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) WITH ( &#39;metadata.iceberg.storage&#39; = &#39;hadoop-catalog&#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (&#39;usa&#39;, &#39;new york&#39;), (&#39;germany&#39;, &#39;berlin&#39;), (&#39;usa&#39;, &#39;chicago&#39;), (&#39;germany&#39;, &#39;hamburg&#39;); Spark SQL\rStart spark-sql with the following command line.\nspark-sql --jars &lt;path-to-paimon-jar&gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=&lt;path-to-warehouse&gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-&lt;iceberg-version&gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=&lt;path-to-warehouse&gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table and insert data.\nCREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) TBLPROPERTIES ( &#39;metadata.iceberg.storage&#39; = &#39;hadoop-catalog&#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (&#39;usa&#39;, &#39;new york&#39;), (&#39;germany&#39;, &#39;berlin&#39;), (&#39;usa&#39;, &#39;chicago&#39;), (&#39;germany&#39;, &#39;hamburg&#39;); Now let&rsquo;s query this Paimon table with Iceberg connector.\nFlink SQL\rCREATE CATALOG iceberg_catalog WITH ( &#39;type&#39; = &#39;iceberg&#39;, &#39;catalog-type&#39; = &#39;hadoop&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;/iceberg&#39;, &#39;cache-enabled&#39; = &#39;false&#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = &#39;germany&#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL\rSELECT * FROM iceberg_catalog.`default`.cities WHERE country = &#39;germany&#39;; /* germany berlin germany hamburg */ Let&rsquo;s insert more data and query again.\nFlink SQL\rINSERT INTO paimon_catalog.`default`.cities VALUES (&#39;usa&#39;, &#39;houston&#39;), (&#39;germany&#39;, &#39;munich&#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = &#39;germany&#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | munich | | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL\rINSERT INTO paimon_catalog.`default`.cities VALUES (&#39;usa&#39;, &#39;houston&#39;), (&#39;germany&#39;, &#39;munich&#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = &#39;germany&#39;; /* germany munich germany berlin germany hamburg */ "}),e.add({id:20,href:"/concepts/rest/bear/",title:"Bear Token",section:"RESTCatalog",content:`Bear Token#A bearer token is an encrypted string, typically generated by the server based on a secret key. When the client sends a request to the server, it must include Authorization: Bearer &lt;token&gt; in the request header. After receiving the request, the server extracts the &lt;token&gt; and validates its legitimacy. If the validation passes, the authentication is successful.
CREATE CATALOG \`paimon-rest-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;uri&#39; = &#39;&lt;catalog server url&gt;&#39;, &#39;metastore&#39; = &#39;rest&#39;, &#39;warehouse&#39; = &#39;my_instance_name&#39;, &#39;token.provider&#39; = &#39;bear&#39; &#39;token&#39; = &#39;&lt;token&gt;&#39; ); `}),e.add({id:21,href:"/primary-key-table/data-distribution/",title:"Data Distribution",section:"Table with PK",content:`Data Distribution#A bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.
Fixed Bucket#Configure a bucket greater than 0, using Fixed Bucket mode, according to Math.abs(key_hashcode % numBuckets) to compute the bucket of record.
Rescaling buckets can only be done through offline processes, see Rescale Bucket. A too large number of buckets leads to too many small files, and a too small number of buckets leads to poor write performance.
Dynamic Bucket#Default mode for primary key table, or configure 'bucket' = '-1'.
The keys that arrive first will fall into the old buckets, and the new keys will fall into the new buckets, the distribution of buckets and keys depends on the order in which the data arrives. Paimon maintains an index to determine which key corresponds to which bucket.
Paimon will automatically expand the number of buckets.
Option1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.initial-buckets': controls the number of initialized bucket. Option3: 'dynamic-bucket.max-buckets': controls the number of max buckets. Dynamic Bucket only support single write job. Please do not start multiple jobs to write to the same partition (this can lead to duplicate data). Even if you enable 'write-only' and start a dedicated compaction job, it won&rsquo;t work.Normal Dynamic Bucket Mode#When your updates do not cross partitions (no partitions, or primary keys contain all partition fields), Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.
Performance:
Generally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance. Normal Dynamic Bucket Mode supports sort-compact to speed up queries. See Sort Compact.
Cross Partitions Upsert Dynamic Bucket Mode#When you need cross partition upsert (primary keys not contain all partition fields), Dynamic Bucket mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting stream write job. Different merge engines have different behaviors:
Deduplicate: Delete data from the old partition and insert new data into the new partition. PartialUpdate &amp; Aggregation: Insert new data into the old partition. FirstRow: Ignore new data if there is old value. Performance: For tables with a large amount of data, there will be a significant loss in performance. Moreover, initialization takes a long time.
If your upsert does not rely on too old data, you can consider configuring index TTL to reduce Index and initialization time:
'cross-partition-upsert.index-ttl': The TTL in rocksdb index and initialization, this can avoid maintaining too many indexes and lead to worse and worse performance. But please note that this may also cause data duplication.
Postpone Bucket#Postpone bucket mode is configured by 'bucket' = '-2'. This mode aims to solve the difficulty to determine a fixed number of buckets and support different buckets for different partitions.
When writing records into the table, all records will first be stored in the bucket-postpone directory of each partition and are not available to readers.
To move the records into the correct bucket and make them readable, you need to run a compaction job. See compact procedure. The bucket number for the partitions compacted for the first time is configured by the option postpone.default-bucket-num, whose default value is 4.
Finally, when you feel that the bucket number of some partition is too small, you can also run a rescale job. See rescale procedure.
Pick Partition Fields#The following three types of fields may be defined as partition fields in the warehouse:
Creation Time (Recommended): The creation time is generally immutable, so you can confidently treat it as a partition field and add it to the primary key. Event Time: Event time is a field in the original table. For CDC data, such as tables synchronized from MySQL CDC or Changelogs generated by Paimon, they are all complete CDC data, including UPDATE_BEFORE records, even if you declare the primary key containing partition field, you can achieve the unique effect (require 'changelog-producer'='input'). CDC op_ts: It cannot be defined as a partition field, unable to know previous record timestamp. So you need to use cross partition upsert, it will consume more resources. `}),e.add({id:22,href:"/project/download/",title:"Download",section:"Project",content:`Download#This documentation is a guide for downloading Paimon Jars.
Engine Jars#Version Jar Flink 2.0 paimon-flink-2.0-1.2.0.jar Flink 1.20 paimon-flink-1.20-1.2.0.jar Flink 1.19 paimon-flink-1.19-1.2.0.jar Flink 1.18 paimon-flink-1.18-1.2.0.jar Flink 1.17 paimon-flink-1.17-1.2.0.jar Flink 1.16 paimon-flink-1.16-1.2.0.jar Flink 1.15 paimon-flink-1.15-1.2.0.jar Flink Action paimon-flink-action-1.2.0.jar Spark 3.5 paimon-spark-3.5-1.2.0.jar Spark 3.4 paimon-spark-3.4-1.2.0.jar Spark 3.3 paimon-spark-3.3-1.2.0.jar Spark 3.2 paimon-spark-3.2-1.2.0.jar Hive 3.1 paimon-hive-connector-3.1-1.2.0.jar Hive 2.3 paimon-hive-connector-2.3-1.2.0.jar Hive 2.2 paimon-hive-connector-2.2-1.2.0.jar Hive 2.1 paimon-hive-connector-2.1-1.2.0.jar Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.2.0.jar Trino Download from master Filesystem Jars#Version Jar paimon-oss paimon-oss-1.2.0.jar paimon-jindo paimon-jindo-1.2.0.jar paimon-s3 paimon-s3-1.2.0.jar API Jars#Version Jar paimon-bundle paimon-bundle-1.2.0.jar `}),e.add({id:23,href:"/program-api/flink-api/",title:"Flink API",section:"Program API",content:`Flink API#If possible, recommend using Flink SQL or Spark SQL, or simply use SQL APIs in programs.Dependency#Maven dependency:
&lt;dependency&gt; &lt;groupId&gt;org.apache.paimon&lt;/groupId&gt; &lt;artifactId&gt;paimon-flink-1.20&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt; &lt;version&gt;1.20.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; Or download the jar file: Paimon Flink.Please choose your Flink version.
Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.
Not only DataStream API, you can also read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.
Write to Table#import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.sink.FlinkSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.types.DataType; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() throws Exception { // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval // env.enableCheckpointing(60_000); // create a changelog DataStream DataStream&lt;Row&gt; input = env.fromElements( Row.ofKind(RowKind.INSERT, &#34;Alice&#34;, 12), Row.ofKind(RowKind.INSERT, &#34;Bob&#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, &#34;Alice&#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, &#34;Alice&#34;, 100)) .returns( Types.ROW_NAMED( new String[] {&#34;name&#34;, &#34;age&#34;}, Types.STRING, Types.INT)); // get table from catalog Options catalogOptions = new Options(); catalogOptions.set(&#34;warehouse&#34;, &#34;/path/to/warehouse&#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(&#34;my_db&#34;, &#34;T&#34;)); DataType inputType = DataTypes.ROW( DataTypes.FIELD(&#34;name&#34;, DataTypes.STRING()), DataTypes.FIELD(&#34;age&#34;, DataTypes.INT())); FlinkSinkBuilder builder = new FlinkSinkBuilder(table).forRow(input, inputType); // set sink parallelism // builder.parallelism(_your_parallelism) // set overwrite mode // builder.overwrite(...) builder.build(); env.execute(); } } Read from Table#import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.source.FlinkSourceBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get table from catalog Options catalogOptions = new Options(); catalogOptions.set(&#34;warehouse&#34;, &#34;/path/to/warehouse&#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(&#34;my_db&#34;, &#34;T&#34;)); // table = table.copy(Collections.singletonMap(&#34;scan.file-creation-time-millis&#34;, &#34;...&#34;)); FlinkSourceBuilder builder = new FlinkSourceBuilder(table).env(env); // builder.sourceBounded(true); // builder.projection(...); // builder.predicate(...); // builder.limit(...); // builder.sourceParallelism(...); DataStream&lt;Row&gt; dataStream = builder.buildForRow(); // use this datastream dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] } } Cdc ingestion Table#Paimon supports ingest data into Paimon tables with schema evolution.
You can use Java API to write cdc records into Paimon Tables. You can write records to Paimon&rsquo;s partial-update table with adding columns dynamically. Here is an example to use RichCdcSinkBuilder API:
import org.apache.paimon.catalog.CatalogLoader; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.sink.cdc.RichCdcRecord; import org.apache.paimon.flink.sink.cdc.RichCdcSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataTypes; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import static org.apache.paimon.types.RowKind.INSERT; public class WriteCdcToTable { public static void writeTo() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval // env.enableCheckpointing(60_000); DataStream&lt;RichCdcRecord&gt; dataStream = env.fromElements( RichCdcRecord.builder(INSERT) .field(&#34;order_id&#34;, DataTypes.BIGINT(), &#34;123&#34;) .field(&#34;price&#34;, DataTypes.DOUBLE(), &#34;62.2&#34;) .build(), // dt field will be added with schema evolution RichCdcRecord.builder(INSERT) .field(&#34;order_id&#34;, DataTypes.BIGINT(), &#34;245&#34;) .field(&#34;price&#34;, DataTypes.DOUBLE(), &#34;82.1&#34;) .field(&#34;dt&#34;, DataTypes.TIMESTAMP(), &#34;2023-06-12 20:21:12&#34;) .build()); Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;T&#34;); Options catalogOptions = new Options(); catalogOptions.set(&#34;warehouse&#34;, &#34;/path/to/warehouse&#34;); CatalogLoader catalogLoader = () -&gt; FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalogLoader.load().getTable(identifier); new RichCdcSinkBuilder(table) .forRichCdcRecord(dataStream) .identifier(identifier) .catalogLoader(catalogLoader) .build(); env.execute(); } } `}),e.add({id:24,href:"/cdc-ingestion/mysql-cdc/",title:"Mysql CDC",section:"CDC Ingestion",content:`MySQL CDC#Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.
Prepare CDC Bundled Jar#Download CDC Bundled Jar and put them under &lt;FLINK_HOME&gt;/lib/.
Version Bundled Jar 3.1.x flink-sql-connector-mysql-cdc-3.1.x.jar mysql-connector-java-8.0.27.jar Only cdc 3.1+ is supported.
You can download the flink-connector-mysql-cdc jar package by clicking here.
Synchronizing Tables#By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_table \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--type_mapping &lt;option1,option2...&gt;] \\ [--computed_column &lt;&#39;column-name=expr-name(args[, ...])&#39;&gt; [--computed_column ...]] \\ [--metadata_column &lt;metadata-column&gt;] \\ [--mysql_conf &lt;mysql-cdc-source-conf&gt; [--mysql_conf &lt;mysql-cdc-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--tableThe Paimon table name.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.--computed_columnThe definitions of computed columns. The argument field is from MySQL table field name. See here for a complete list of configurations. --metadata_column--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.--mysql_confThe configuration for Flink CDC MySQL sources. Each configuration should be specified in the format "key=value". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.
Example 1: synchronize tables into one Paimon table#&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=&#39;source_db&#39; \\ --mysql_conf table-name=&#39;source_table1|source_table2&#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 As example shows, the mysql_conf&rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.
Example 2: synchronize shards into one Paimon table#You can also set &lsquo;database-name&rsquo; with a regular expression to capture multiple databases. A typical scenario is that a table &lsquo;source_table&rsquo; is split into database &lsquo;source_db1&rsquo;, &lsquo;source_db2&rsquo; &hellip;, then you can synchronize data of all the &lsquo;source_table&rsquo;s into one Paimon table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=&#39;source_db.+&#39; \\ --mysql_conf table-name=&#39;source_table&#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronizing Databases#By using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_database \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ [--ignore_incompatible &lt;true/false&gt;] \\ [--merge_shards &lt;true/false&gt;] \\ [--table_prefix &lt;paimon-table-prefix&gt;] \\ [--table_suffix &lt;paimon-table-suffix&gt;] \\ [--including_tables &lt;mysql-table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;mysql-table-name|name-regular-expr&gt;] \\ [--mode &lt;sync-mode&gt;] \\ [--metadata_column &lt;metadata-column&gt;] \\ [--type_mapping &lt;option1,option2...&gt;] \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--mysql_conf &lt;mysql-cdc-source-conf&gt; [--mysql_conf &lt;mysql-cdc-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--ignore_incompatibleIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.--merge_shardsIt is default true, in this case, if some tables in different databases have the same name, their schemas will be merged and their records will be synchronized into one Paimon table. Otherwise, each table's records will be synchronized to a corresponding Paimon table, and the Paimon table will be named to 'databaseName_tableName' to avoid potential name conflict.--table_prefixThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have "ods_" as prefix, you can specify "--table_prefix ods_".--table_suffixThe suffix of all Paimon tables to be synchronized. The usage is same as "--table_prefix".--including_tablesIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying "--including_tables test|paimon.*" means to synchronize table 'test' and all tables start with 'paimon'.--excluding_tablesIt is used to specify which source tables are not to be synchronized. The usage is same as "--including_tables". "--excluding_tables" has higher priority than "--including_tables" if you specified both.--modeIt is used to specify synchronization mode.
Possible values:"divided" (the default mode if you haven't specified one): start a sink for each table, the synchronization of the new table requires restarting the job."combined": start a single combined sink for all tables, the new table will be automatically synchronized.--metadata_column--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".If the keys are not in source table, the sink table won't set partition keys.--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.Otherwise, the sink table won't set primary keys.--mysql_confThe configuration for Flink CDC MySQL sources. Each configuration should be specified in the format "key=value". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.Only tables with primary keys will be synchronized.
For each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.
Example 1: synchronize entire database#&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: synchronize newly added tables under database#Let&rsquo;s say at first a Flink job is synchronizing tables [product, user, address] under database source_db. The command to submit the job looks like:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 \\ --including_tables &#39;product|user|address&#39; At a later point we would like the job to also synchronize tables [order, custom], which contains history data. We can achieve this by recovering from the previous snapshot of the job and thus reusing existing state of the job. The recovered job will first snapshot newly added tables, and then continue reading changelog from previous position automatically.
The command to recover from previous snapshot and add new tables to synchronize looks like:
&lt;FLINK_HOME&gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables &#39;product|user|address|order|custom&#39; You can set --mode combined to enable synchronizing newly added tables without restarting job.Example 3: synchronize and merge multiple shards#Let&rsquo;s say you have multiple database shards db1, db2, &hellip; and each database has tables tbl1, tbl2, &hellip;. You can synchronize all the db.+.tbl.+ into tables test_db.tbl1, test_db.tbl2 &hellip; by following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=&#39;db.+&#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 \\ --including_tables &#39;tbl.+&#39; By setting database-name to a regular expression, the synchronization job will capture all tables under matched databases and merge tables of the same name into one table.
You can set --merge_shards false to prevent merging shards. The synchronized tables will be named to &lsquo;databaseName_tableName&rsquo; to avoid potential name conflict.FAQ#Chinese characters in records ingested from MySQL are garbled. Try to set env.java.opts: -Dfile.encoding=UTF-8 in flink-conf.yaml(Flink version &lt; 1.19) or config.yaml(Flink version &gt;= 1.19) (the option is changed to env.java.opts.all since Flink-1.17). Synchronize MySQL table and column comment. Synchronize MySQL create table comment to the paimon table, you need to configure --mysql_conf jdbc.properties.useInformationSchema=true. Synchronize MySQL alter table or column comment to the paimon table, you need to configure --mysql_conf debezium.include.schema.comments=true. `}),e.add({id:25,href:"/primary-key-table/merge-engine/partial-update/",title:"Partial Update",section:"Merge Engine",content:`Partial Update#By specifying 'merge-engine' = 'partial-update', users have the ability to update columns of a record through multiple updates until the record is complete. This is achieved by updating the value fields one by one, using the latest data under the same primary key. However, null values are not overwritten in the process.
For example, suppose Paimon receives three records:
&lt;1, 23.0, 10, NULL&gt;- &lt;1, NULL, NULL, 'This is a book'&gt; &lt;1, 25.2, NULL, NULL&gt; Assuming that the first column is the primary key, the final result would be &lt;1, 25.2, 10, 'This is a book'&gt;.
For streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer. (&lsquo;input&rsquo; changelog producer is also supported, but only returns input records.)By default, Partial update can not accept delete records, you can choose one of the following solutions:
Configure &lsquo;ignore-delete&rsquo; to ignore delete records. Configure &lsquo;partial-update.remove-record-on-delete&rsquo; to remove the whole row when receiving delete records. Configure &lsquo;sequence-group&rsquo;s to retract partial columns. Also configure &lsquo;partial-update.remove-record-on-sequence-group&rsquo; to remove the whole row when receiving deleted records of specified sequence group. Sequence Group#A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update.
So we introduce sequence group mechanism for partial-update tables. It can solve:
Disorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update. See example:
CREATE TABLE t ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;partial-update&#39;, &#39;fields.g_1.sequence-group&#39; = &#39;a,b&#39;, &#39;fields.g_2.sequence-group&#39; = &#39;c,d&#39; ); INSERT INTO t VALUES (1, 1, 1, 1, 1, 1, 1); -- g_2 is null, c, d should not be updated INSERT INTO t VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT)); SELECT * FROM t; -- output 1, 2, 2, 2, 1, 1, 1 -- g_1 is smaller, a, b should not be updated INSERT INTO t VALUES (1, 3, 3, 1, 3, 3, 3); SELECT * FROM t; -- output 1, 2, 2, 2, 3, 3, 3 For fields.&lt;field-name&gt;.sequence-group, valid comparative data types include: DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ.
You can also configure multiple sorted fields in a sequence-group, like fields.&lt;field-name1&gt;,&lt;field-name2&gt;.sequence-group, multiple fields will be compared in order.
See example:
CREATE TABLE SG ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;partial-update&#39;, &#39;fields.g_1.sequence-group&#39; = &#39;a,b&#39;, &#39;fields.g_2,g_3.sequence-group&#39; = &#39;c,d&#39; ); INSERT INTO SG VALUES (1, 1, 1, 1, 1, 1, 1, 1); -- g_3 is null, g_2, g_3 are not bigger, c, d should not be updated INSERT INTO SG VALUES (1, 2, 2, 2, 2, 2, 1, CAST(NULL AS INT)); SELECT * FROM SG; -- output 1, 2, 2, 2, 1, 1, 1, 1 -- g_1 is smaller, a, b should not be updated INSERT INTO SG VALUES (1, 3, 3, 1, 3, 3, 3, 1); SELECT * FROM SG; -- output 1, 2, 2, 2, 3, 3, 3, 1 Aggregation For Partial Update#You can specify aggregation function for the input field, all the functions in the Aggregation are supported.
See example:
CREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;partial-update&#39;, &#39;fields.a.sequence-group&#39; = &#39;b&#39;, &#39;fields.b.aggregate-function&#39; = &#39;first_value&#39;, &#39;fields.c.sequence-group&#39; = &#39;d&#39;, &#39;fields.d.aggregate-function&#39; = &#39;sum&#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 1, 2, 3 You can also configure an aggregation function for a sequence-group within multiple sorted fields.
See example:
CREATE TABLE AGG ( k INT, a INT, b INT, g_1 INT, c VARCHAR, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;partial-update&#39;, &#39;fields.a.aggregate-function&#39; = &#39;sum&#39;, &#39;fields.g_1,g_3.sequence-group&#39; = &#39;a&#39;, &#39;fields.g_2.sequence-group&#39; = &#39;c&#39;); -- a in sequence-group g_1, g_3 with sum agg -- b not in sequence-group -- c in sequence-group g_2 without agg INSERT INTO AGG VALUES (1, 1, 1, 1, &#39;1&#39;, 1, 1); -- g_2 is null, c should not be updated INSERT INTO AGG VALUES (1, 2, 2, 2, &#39;2&#39;, CAST(NULL AS INT), 2); SELECT * FROM AGG; -- output 1, 3, 2, 2, &#34;1&#34;, 1, 2 -- g_1, g_3 are smaller, a should not beupdated INSERT INTO AGG VALUES (1, 3, 3, 2, &#39;3&#39;, 3, 1); SELECT * FROM AGG; -- output 1, 6, 3, 2, &#34;3&#34;, 3, 2 You can specify a default aggregation function for all the input fields with fields.default-aggregate-function, see example:
CREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;partial-update&#39;, &#39;fields.a.sequence-group&#39; = &#39;b&#39;, &#39;fields.c.sequence-group&#39; = &#39;d&#39;, &#39;fields.default-aggregate-function&#39; = &#39;last_non_null_value&#39;, &#39;fields.d.aggregate-function&#39; = &#39;sum&#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 2, 2, 3 `}),e.add({id:26,href:"/cdc-ingestion/postgres-cdc/",title:"Postgres CDC",section:"CDC Ingestion",content:`Postgres CDC#Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.
Prepare CDC Bundled Jar#flink-connector-postgres-cdc-*.jar Synchronizing Tables#By using PostgresSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from PostgreSQL into one Paimon table.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ postgres_sync_table \\ --warehouse &lt;warehouse_path&gt; \\ --database &lt;database_name&gt; \\ --table &lt;table_name&gt; \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary_keys&gt;] \\ [--type_mapping &lt;option1,option2...&gt;] \\ [--computed_column &lt;&#39;column-name=expr-name(args[, ...])&#39;&gt; [--computed_column ...]] \\ [--metadata_column &lt;metadata_column&gt;] \\ [--postgres_conf &lt;postgres_cdc_source_conf&gt; [--postgres_conf &lt;postgres_cdc_source_conf&gt; ...]] \\ [--catalog_conf &lt;paimon_catalog_conf&gt; [--catalog_conf &lt;paimon_catalog_conf&gt; ...]] \\ [--table_conf &lt;paimon_table_sink_conf&gt; [--table_conf &lt;paimon_table_sink_conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--tableThe Paimon table name.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".--type_mappingIt is used to specify how to map PostgreSQL data type to Paimon type.
Supported options:"to-string": maps all PostgreSQL types to STRING.--computed_columnThe definitions of computed columns. The argument field is from PostgreSQL table field name. See here for a complete list of configurations. --metadata_column--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,schema_name,op_ts. See its document for a complete list of available metadata.--postgres_confThe configuration for Flink CDC Postgres sources. Each configuration should be specified in the format "key=value". hostname, username, password, database-name, schema-name, table-name and slot.name are required configurations, others are optional. See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified PostgreSQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified PostgreSQL tables.
Example 1: synchronize tables into one Paimon table
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ postgres_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --postgres_conf hostname=127.0.0.1 \\ --postgres_conf username=root \\ --postgres_conf password=123456 \\ --postgres_conf database-name=&#39;source_db&#39; \\ --postgres_conf schema-name=&#39;public&#39; \\ --postgres_conf table-name=&#39;source_table1|source_table2&#39; \\ --postgres_conf slot.name=&#39;paimon_cdc&#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 As example shows, the postgres_conf&rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.
Example 2: synchronize shards into one Paimon table
You can also set &lsquo;schema-name&rsquo; with a regular expression to capture multiple schemas. A typical scenario is that a table &lsquo;source_table&rsquo; is split into schema &lsquo;source_schema1&rsquo;, &lsquo;source_schema2&rsquo; &hellip;, then you can synchronize data of all the &lsquo;source_table&rsquo;s into one Paimon table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ postgres_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --postgres_conf hostname=127.0.0.1 \\ --postgres_conf username=root \\ --postgres_conf password=123456 \\ --postgres_conf database-name=&#39;source_db&#39; \\ --postgres_conf schema-name=&#39;source_schema.+&#39; \\ --postgres_conf table-name=&#39;source_table&#39; \\ --postgres_conf slot.name=&#39;paimon_cdc&#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 `}),e.add({id:27,href:"/flink/sql-ddl/",title:"SQL DDL",section:"Engine Flink",content:`SQL DDL#Create Catalog#Paimon catalogs currently support three types of metastores:
filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. See CatalogOptions for detailed options when creating a catalog.
Create Filesystem Catalog#The following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.
Creating Hive Catalog#By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.
To use Hive catalog, Database name, Table name and Field names should be lower case.
Paimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Hive connector bundled jar and add it to classpath.
Metastore version Bundle Name SQL Client JAR 2.3.0 - 3.1.3 Flink Bundle Download 1.2.0 - x.x.x Presto Bundle Download The following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.
If your Hive requires security authentication such as Kerberos, LDAP, Ranger or you want the paimon table to be managed by Apache Atlas(Setting &lsquo;hive.metastore.event.listeners&rsquo; in hive-site.xml). You can specify the hive-conf-dir and hadoop-conf-dir parameter to the hive-site.xml file path.
CREATE CATALOG my_hive WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, -- &#39;uri&#39; = &#39;thrift://&lt;hive-metastore-host-name&gt;:&lt;port&gt;&#39;, default use &#39;hive.metastore.uris&#39; in HiveConf -- &#39;hive-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;hadoop-conf-dir&#39; = &#39;...&#39;, this is recommended in the kerberos environment -- &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39;, default use &#39;hive.metastore.warehouse.dir&#39; in HiveConf ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.
Also, you can create FlinkGenericCatalog.
When using hive catalog to change incompatible column types through alter table, you need to configure hive.metastore.disallow.incompatible.col.type.changes=false. see HIVE-17832.
If you are using Hive3, please disable Hive ACID:
hive.strict.managed.tables=false hive.create.as.insert.only=false metastore.create.as.acid=false Synchronizing Partitions into Hive Metastore#By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.
If you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.
Adding Parameters to a Hive Table#Using the table option facilitates the convenient definition of Hive table parameters. Parameters prefixed with hive. will be automatically defined in the TBLPROPERTIES of the Hive table. For instance, using the option hive.table.owner=Jon will automatically add the parameter table.owner=Jon to the table properties during the creation process.
Setting Location in Properties#If you are using an object storage , and you don&rsquo;t want that the location of paimon table/database is accessed by the filesystem of hive, which may lead to the error such as &ldquo;No FileSystem for scheme: s3a&rdquo;. You can set location in the properties of table/database by the config of location-in-properties. See setting the location of table/database in properties Creating JDBC Catalog#By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.
Currently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.
Paimon JDBC Catalog in Flink needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres
database type Bundle Name SQL Client JAR mysql mysql-connector-java Download postgres postgresql Download CREATE CATALOG my_jdbc WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;jdbc&#39;, &#39;uri&#39; = &#39;jdbc:mysql://&lt;host&gt;:&lt;port&gt;/&lt;databaseName&gt;&#39;, &#39;jdbc.user&#39; = &#39;...&#39;, &#39;jdbc.password&#39; = &#39;...&#39;, &#39;catalog-key&#39;=&#39;jdbc&#39;, &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39; ); USE CATALOG my_jdbc; You can configure any connection parameters that have been declared by JDBC through &ldquo;jdbc.&rdquo;, the connection parameters may be different between different databases, please configure according to the actual situation.
You can also perform logical isolation for databases under multiple catalogs by specifying &ldquo;catalog-key&rdquo;.
Additionally, when creating a JdbcCatalog, you can specify the maximum length for the lock key by configuring &ldquo;lock-key-max-length,&rdquo; which defaults to 255. Since this value is a combination of {catalog-key}.{database-name}.{table-name}, please adjust accordingly.
You can define any default table options with the prefix table-default. for tables created in the catalog.
Create Table#After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.
The following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog&rsquo;s default database, where dt, hh and user_id are the primary keys.
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); You can create partitioned table:
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); If you need cross partition upsert (primary keys not contain all partition fields), see Cross partition Upsert mode.By configuring partition.expiration-time, expired partitions can be automatically deleted.Specify Statistics Mode#Paimon will automatically collect the statistics of the data file for speeding up the query process. There are four modes supported:
full: collect the full metrics: null_count, min, max . truncate(length): length can be any positive number, the default mode is truncate(16), which means collect the null count, min/max value with truncated length of 16. This is mainly to avoid too big column which will enlarge the manifest file. counts: only collect the null count. none: disable the metadata stats collection. The statistics collector mode can be configured by 'metadata.stats-mode', by default is 'truncate(16)'. You can configure the field level by setting 'fields.{field_name}.stats-mode'.
For the stats mode of none, by default metadata.stats-dense-store is true, which will significantly reduce the storage size of the manifest. But the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.
Field Default Value#Paimon table currently supports setting default values for fields in table properties by 'fields.item_id.default-value', note that partition fields and primary key fields can not be specified.
Create Table As Select#Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;
We can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.
/* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table */ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as WITH (&#39;partition&#39; = &#39;dt&#39;) AS SELECT * FROM my_table_partition; /* change options */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) WITH (&#39;file.format&#39; = &#39;orc&#39;); CREATE TABLE my_table_options_as WITH (&#39;file.format&#39; = &#39;parquet&#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_pk_as WITH (&#39;primary-key&#39; = &#39;dt,hh&#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_all_as WITH (&#39;primary-key&#39; = &#39;dt,hh&#39;, &#39;partition&#39; = &#39;dt&#39;) AS SELECT * FROM my_table_all; Create Table Like#To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_like LIKE my_table (EXCLUDING OPTIONS); Work with Flink Temporary Tables#Flink Temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.
If you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.
CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( &#39;connector&#39; = &#39;filesystem&#39;, &#39;path&#39; = &#39;hdfs:///path/to/temp_table.csv&#39;, &#39;format&#39; = &#39;csv&#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k; `}),e.add({id:28,href:"/spark/sql-ddl/",title:"SQL DDL",section:"Engine Spark",content:`SQL DDL#Catalog#Create Catalog#Paimon catalogs currently support three types of metastores:
filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. See CatalogOptions for detailed options when creating a catalog.
Create Filesystem Catalog#The following Spark SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.
The following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse.
spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.
After spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.
USE paimon.default; Creating Hive Catalog#By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.
To use Hive catalog, Database name, Table name and Field names should be lower case.
Your Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.
The following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.
spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\ --conf spark.sql.catalog.paimon.metastore=hive \\ --conf spark.sql.catalog.paimon.uri=thrift://&lt;hive-metastore-host-name&gt;:&lt;port&gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.
After spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.
USE paimon.default; Also, you can create SparkGenericCatalog.
Synchronizing Partitions into Hive Metastore
By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.
If you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.
Creating JDBC Catalog#By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.
Currently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.
Paimon JDBC Catalog in Spark needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres
database type Bundle Name SQL Client JAR mysql mysql-connector-java Download postgres postgresql Download spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\ --conf spark.sql.catalog.paimon.metastore=jdbc \\ --conf spark.sql.catalog.paimon.uri=jdbc:mysql://&lt;host&gt;:&lt;port&gt;/&lt;databaseName&gt; \\ --conf spark.sql.catalog.paimon.jdbc.user=... \\ --conf spark.sql.catalog.paimon.jdbc.password=... USE paimon.default; Creating REST Catalog#By using the Paimon REST catalog, changes to the catalog will be directly stored in remote server.
bear token#spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=&lt;catalog server url&gt; \\ --conf spark.sql.catalog.paimon.token.provider=bear \\ --conf spark.sql.catalog.paimon.token=&lt;token&gt; dlf ak#spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=&lt;catalog server url&gt; \\ --conf spark.sql.catalog.paimon.token.provider=dlf \\ --conf spark.sql.catalog.paimon.dlf.access-key-id=&lt;access-key-id&gt; \\ --conf spark.sql.catalog.paimon.dlf.access-key-secret=&lt;security-token&gt; dlf sts token#spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=&lt;catalog server url&gt; \\ --conf spark.sql.catalog.paimon.token.provider=dlf \\ --conf spark.sql.catalog.paimon.dlf.access-key-id=&lt;access-key-id&gt; \\ --conf spark.sql.catalog.paimon.dlf.access-key-secret=&lt;access-key-secret&gt; \\ --conf spark.sql.catalog.paimon.dlf.security-token=&lt;security-token&gt; USE paimon.default; Table#Create Table#After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.
The following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog&rsquo;s default database, where dt, hh and user_id are the primary keys.
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;dt,hh,user_id&#39; ); You can create partitioned table:
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;dt,hh,user_id&#39; ); Create External Table#When the catalog&rsquo;s metastore type is hive, if the location is specified when creating a table, that table will be considered an external table; otherwise, it will be a managed table.
When you drop an external table, only the metadata in Hive will be removed, and the actual data files will not be deleted; whereas dropping a managed table will also delete the data.
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;dt,hh,user_id&#39; ) LOCATION &#39;/path/to/table&#39;; Furthermore, if there is already data stored in the specified location, you can create the table without explicitly specifying the fields, partitions and props or other information. In this case, the new table will inherit them all from the existing table’s metadata.
However, if you manually specify them, you need to ensure that they are consistent with those of the existing table (props can be a subset). Therefore, it is strongly recommended not to specify them.
CREATE TABLE my_table LOCATION &#39;/path/to/table&#39;; Create Table As Select#Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;
We can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.
CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table*/ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as PARTITIONED BY (dt) AS SELECT * FROM my_table_partition; /* change TBLPROPERTIES */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (&#39;file.format&#39; = &#39;orc&#39;); CREATE TABLE my_table_options_as TBLPROPERTIES (&#39;file.format&#39; = &#39;parquet&#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;dt,hh,user_id&#39; ); CREATE TABLE my_table_pk_as TBLPROPERTIES (&#39;primary-key&#39; = &#39;dt&#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;dt,hh,user_id&#39; ); CREATE TABLE my_table_all_as PARTITIONED BY (dt) TBLPROPERTIES (&#39;primary-key&#39; = &#39;dt,hh&#39;) AS SELECT * FROM my_table_all; View#Views are based on the result-set of an SQL query, when using org.apache.paimon.spark.SparkCatalog, views are managed by paimon itself. And in this case, views are supported when the metastore type is hive, and temporary views are not supported yet.
Create Or Replace View#CREATE VIEW constructs a virtual table that has no physical data.
-- create a view. CREATE VIEW v1 AS SELECT * FROM t1; -- create a view, if a view of same name already exists, it will be replaced. CREATE OR REPLACE VIEW v1 AS SELECT * FROM t1; Drop View#DROP VIEW removes the metadata associated with a specified view from the catalog.
-- drop a view DROP VIEW v1; Tag#Create Or Replace Tag#Create or replace a tag syntax with the following options.
Create a tag with or without the snapshot id and time retention. Create an existed tag is not failed if using IF NOT EXISTS syntax. Update a tag using REPLACE TAG or CREATE OR REPLACE TAG syntax. -- create a tag based on the latest snapshot and no retention. ALTER TABLE T CREATE TAG \`TAG-1\`; -- create a tag based on the latest snapshot and no retention if it doesn&#39;t exist. ALTER TABLE T CREATE TAG IF NOT EXISTS \`TAG-1\`; -- create a tag based on the latest snapshot and retain it for 7 day. ALTER TABLE T CREATE TAG \`TAG-2\` RETAIN 7 DAYS; -- create a tag based on snapshot-1 and no retention. ALTER TABLE T CREATE TAG \`TAG-3\` AS OF VERSION 1; -- create a tag based on snapshot-2 and retain it for 12 hour. ALTER TABLE T CREATE TAG \`TAG-4\` AS OF VERSION 2 RETAIN 12 HOURS; -- replace a existed tag with new snapshot id and new retention ALTER TABLE T REPLACE TAG \`TAG-4\` AS OF VERSION 2 RETAIN 24 HOURS; -- create or replace a tag, create tag if it not exist, replace tag if it exists. ALTER TABLE T CREATE OR REPLACE TAG \`TAG-5\` AS OF VERSION 2 RETAIN 24 HOURS; Delete Tag#Delete a tag or multiple tags of a table.
-- delete a tag. ALTER TABLE T DELETE TAG \`TAG-1\`; -- delete a tag if it exists. ALTER TABLE T DELETE TAG IF EXISTS \`TAG-1\` -- delete multiple tags, delimiter is &#39;,&#39;. ALTER TABLE T DELETE TAG \`TAG-1,TAG-2\`; Rename Tag#Rename an existing tag with a new tag name.
ALTER TABLE T RENAME TAG \`TAG-1\` TO \`TAG-2\`; Show Tags#List all tags of a table.
SHOW TAGS T; `}),e.add({id:29,href:"/spark/sql-functions/",title:"SQL Functions",section:"Engine Spark",content:`SQL Functions#This section introduce all available Paimon Spark functions.
max_pt#max_pt($table_name)
It accepts a string type literal to specify the table name and return a max-valid-toplevel partition value.
valid: the partition which contains data files toplevel: only return the first partition value if the table has multi-partition columns It would throw exception when:
the table is not a partitioned table the partitioned table does not have partition all of the partitions do not contains data files Example
&gt; SELECT max_pt(&#39;t&#39;); 20250101 &gt; SELECT * FROM t where pt = max_pt(&#39;t&#39;); a, 20250101 Since: 1.1.0
`}),e.add({id:30,href:"/flink/sql-write/",title:"SQL Write",section:"Engine Flink",content:`SQL Write#Syntax#INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:
Flink INSERT Statement
INSERT INTO#Use INSERT INTO to apply records and changes to tables.
INSERT INTO my_table SELECT ... INSERT INTO supports both batch and streaming mode. In Streaming mode, by default, it will also perform compaction, snapshot expiration, and even partition expiration in Flink Sink (if it is configured).
For multiple jobs to write the same table, you can refer to dedicated compaction job for more info.
Clustering#In Paimon, clustering is a feature that allows you to cluster data in your Append Table based on the values of certain columns during the write process. This organization of data can significantly enhance the efficiency of downstream tasks when reading the data, as it enables faster and more targeted data retrieval. This feature is only supported for Append Table(bucket = -1) and batch execution mode.
To utilize clustering, you can specify the columns you want to cluster when creating or writing to a table. Here&rsquo;s a simple example of how to enable clustering:
CREATE TABLE my_table ( a STRING, b STRING, c STRING, ) WITH ( &#39;sink.clustering.by-columns&#39; = &#39;a,b&#39;, ); You can also use SQL hints to dynamically set clustering options:
INSERT INTO my_table /*+ OPTIONS(&#39;sink.clustering.by-columns&#39; = &#39;a,b&#39;) */ SELECT * FROM source; The data is clustered using an automatically chosen strategy (such as ORDER, ZORDER, or HILBERT), but you can manually specify the clustering strategy by setting the sink.clustering.strategy. Clustering relies on sampling and sorting. If the clustering process takes too much time, you can decrease the total sample number by setting the sink.clustering.sample-factor or disable the sorting step by setting the sink.clustering.sort-in-cluster to false.
You can refer to FlinkConnectorOptions for more info about the configurations above.
Overwriting the Whole Table#For unpartitioned tables, Paimon supports overwriting the whole table. (or for partitioned table which disables dynamic-partition-overwrite option).
Use INSERT OVERWRITE to overwrite the whole unpartitioned table.
INSERT OVERWRITE my_table SELECT ... Overwriting a Partition#For partitioned tables, Paimon supports overwriting a partition.
Use INSERT OVERWRITE to overwrite a partition.
INSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite#Flink&rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.
-- MyTable is a Partitioned Table -- Dynamic overwrite INSERT OVERWRITE my_table SELECT ... -- Static overwrite (Overwrite whole table) INSERT OVERWRITE my_table /*+ OPTIONS(&#39;dynamic-partition-overwrite&#39; = &#39;false&#39;) */ SELECT ... Truncate tables#Flink 1.17-You can use INSERT OVERWRITE to purge tables by inserting empty value.
INSERT OVERWRITE my_table /*+ OPTIONS(&#39;dynamic-partition-overwrite&#39;=&#39;false&#39;) */ SELECT * FROM my_table WHERE false; Flink 1.18&#43;TRUNCATE TABLE my_table; Purging Partitions#Currently, Paimon supports two ways to purge partitions.
Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.
Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop_partition job through flink run.
-- Syntax INSERT OVERWRITE my_table /*+ OPTIONS(&#39;dynamic-partition-overwrite&#39;=&#39;false&#39;) */ PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM my_table WHERE false; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE my_table /*+ OPTIONS(&#39;dynamic-partition-overwrite&#39;=&#39;false&#39;) */ PARTITION (k0 = 0) SELECT k1, v FROM my_table WHERE false; -- or INSERT OVERWRITE my_table /*+ OPTIONS(&#39;dynamic-partition-overwrite&#39;=&#39;false&#39;) */ PARTITION (k0 = 0, k1 = 0) SELECT v FROM my_table WHERE false; Updating tables#Important table properties setting:
Only primary key table supports this feature. MergeEngine needs to be deduplicate or partial-update to support this feature. Do not support updating primary keys. Currently, Paimon supports updating records by using UPDATE in Flink 1.17 and later versions. You can perform UPDATE in Flink&rsquo;s batch mode.
-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( a STRING, b INT, c INT, PRIMARY KEY (a) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;deduplicate&#39; ); -- you can use UPDATE my_table SET b = 1, c = 2 WHERE a = &#39;myTable&#39;; Deleting from table#Flink 1.17&#43;Important table properties setting:
Only primary key tables support this feature. If the table has primary keys, the following MergeEngine support this feature: deduplicate. partial-update with option &lsquo;partial-update.remove-record-on-delete&rsquo; enabled. Do not support deleting from table in streaming mode. -- Syntax DELETE FROM table_identifier WHERE conditions; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( id BIGINT NOT NULL, currency STRING, rate BIGINT, dt String, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( &#39;merge-engine&#39; = &#39;deduplicate&#39; ); -- you can use DELETE FROM my_table WHERE currency = &#39;UNKNOWN&#39;; Partition Mark Done#For partitioned tables, each partition may need to be scheduled to trigger downstream batch computation. Therefore, it is necessary to choose this timing to indicate that it is ready for scheduling and to minimize the amount of data drift during scheduling. We call this process: &ldquo;Partition Mark Done&rdquo;.
Example to mark done:
CREATE TABLE my_partitioned_table ( f0 INT, f1 INT, f2 INT, ... dt STRING ) PARTITIONED BY (dt) WITH ( &#39;partition.timestamp-formatter&#39;=&#39;yyyyMMdd&#39;, &#39;partition.timestamp-pattern&#39;=&#39;$dt&#39;, &#39;partition.time-interval&#39;=&#39;1 d&#39;, &#39;partition.idle-time-to-done&#39;=&#39;15 m&#39;, &#39;partition.mark-done-action&#39;=&#39;done-partition&#39; ); You can also customize a PartitionMarkDoneAction to mark the partition completed.
partition.mark-done-action: custom partition.mark-done-action.custom.class: The partition mark done class for implement PartitionMarkDoneAction interface (e.g. org.apache.paimon.CustomPartitionMarkDoneAction). Define a class CustomPartitionMarkDoneAction to implement the PartitionMarkDoneAction interface.
package org.apache.paimon; public class CustomPartitionMarkDoneAction implements PartitionMarkDoneAction { @Override public void markDone(String partition) { // do something. } @Override public void close() {} } Paimon also support http-report partition mark done action, this action will report the partition to the remote http server.
partition.mark-done-action: http-report partition.mark-done-action.http.url : Action will report the partition to the remote http server. partition.mark-done-action.http.params : Http client request params in the request body json. Http Post request body :
{ &#34;table&#34;: &#34;table fullName&#34;, &#34;path&#34;: &#34;table location path&#34;, &#34;partition&#34;: &#34;mark done partition&#34;, &#34;params&#34; : &#34;custom params&#34; } Http Response body :
{ &#34;result&#34;: &#34;success&#34; } Firstly, you need to define the time parser of the partition and the time interval between partitions in order to determine when the partition can be properly marked done. Secondly, you need to define idle-time, which determines how long it takes for the partition to have no new data, and then it will be marked as done. Thirdly, by default, partition mark done will create _SUCCESS file, the content of _SUCCESS file is a json, contains creationTime and modificationTime, they can help you understand if there is any delayed data. You can also configure other actions, like 'done-partition', for example, partition 'dt=20240501' with produce 'dt=20240501.done' done partition. `}),e.add({id:31,href:"/spark/sql-write/",title:"SQL Write",section:"Engine Spark",content:`SQL Write#Insert Table#The INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.
Syntax
INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; Parameters
table_identifier: Specifies a table name, which may be optionally qualified with a database name.
part_spec: An optional parameter that specifies a comma-separated list of key and value pairs for partitions.
column_list: An optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table. Spark will reorder the columns of the input query to match the table schema according to the specified column list.
Note: Since Spark 3.4, INSERT INTO commands with explicit column lists comprising fewer columns than the target table will automatically add the corresponding default values for the remaining columns (or NULL for any column lacking an explicitly-assigned default value). In Spark 3.3 or earlier, column_list&rsquo;s size must be equal to the target table&rsquo;s column size, otherwise these commands would have failed.
value_expr ( { value | NULL } [ , … ] ) [ , ( … ) ]: Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.
For more information, please check the syntax document: Spark INSERT Statement
Insert Into#Use INSERT INTO to apply records and changes to tables.
INSERT INTO my_table SELECT ... Insert Overwrite#Use INSERT OVERWRITE to overwrite the whole table.
INSERT OVERWRITE my_table SELECT ... Insert Overwrite Partition#Use INSERT OVERWRITE to overwrite a partition.
INSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite Partition#Spark&rsquo;s default overwrite mode is static partition overwrite. To enable dynamic overwritten you need to set the Spark session configuration spark.sql.sources.partitionOverwriteMode to dynamic
For example:
CREATE TABLE my_table (id INT, pt STRING) PARTITIONED BY (pt); INSERT INTO my_table VALUES (1, &#39;p1&#39;), (2, &#39;p2&#39;); -- Static overwrite (Overwrite the whole table) INSERT OVERWRITE my_table VALUES (3, &#39;p1&#39;); -- or INSERT OVERWRITE my_table PARTITION (pt) VALUES (3, &#39;p1&#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 3| p1| +---+---+ */ -- Static overwrite with specified partitions (Only overwrite pt=&#39;p1&#39;) INSERT OVERWRITE my_table PARTITION (pt=&#39;p1&#39;) VALUES (3); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ -- Dynamic overwrite (Only overwrite pt=&#39;p1&#39;) SET spark.sql.sources.partitionOverwriteMode=dynamic; INSERT OVERWRITE my_table VALUES (3, &#39;p1&#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ Truncate Table#The TRUNCATE TABLE statement removes all the rows from a table or partition(s).
TRUNCATE TABLE my_table; Update Table#Updates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.
Note:
Update primary key columns is not supported when the target table is a primary key table.Spark supports update PrimitiveType and StructType, for example:
-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; CREATE TABLE t ( id INT, s STRUCT&lt;c1: INT, c2: STRING&gt;, name STRING) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;id&#39;, &#39;merge-engine&#39; = &#39;deduplicate&#39; ); -- you can use UPDATE t SET name = &#39;a_new&#39; WHERE id = 1; UPDATE t SET s.c2 = &#39;a_new&#39; WHERE s.c1 = 1; Delete From Table#Deletes the rows that match a predicate. When no predicate is provided, deletes all rows.
DELETE FROM my_table WHERE currency = &#39;UNKNOWN&#39;; Merge Into Table#Merges a set of updates, insertions and deletions based on a source table into a target table.
Note:
In update clause, to update primary key columns is not supported when the target table is a primary key table.Example: One
This is a simple demo that, if a row exists in the target table update it, else insert it.
-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key. MERGE INTO target USING source ON target.a = source.a WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT * Example: Two
This is a demo with multiple, conditional clauses.
-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key. MERGE INTO target USING source ON target.a = source.a WHEN MATCHED AND target.a = 5 THEN UPDATE SET b = source.b + target.b -- when matched and meet the condition 1, then update b; WHEN MATCHED AND source.c &gt; &#39;c2&#39; THEN UPDATE SET * -- when matched and meet the condition 2, then update all the columns; WHEN MATCHED THEN DELETE -- when matched, delete this row in target table; WHEN NOT MATCHED AND c &gt; &#39;c9&#39; THEN INSERT (a, b, c) VALUES (a, b * 1.1, c) -- when not matched but meet the condition 3, then transform and insert this row; WHEN NOT MATCHED THEN INSERT * -- when not matched, insert this row without any transformation; Streaming Write#Paimon Structured Streaming only supports the two append and complete modes.// Create a paimon table if not exists. spark.sql(s&#34;&#34;&#34; |CREATE TABLE T (k INT, v STRING) |TBLPROPERTIES (&#39;primary-key&#39;=&#39;k&#39;, &#39;bucket&#39;=&#39;3&#39;) |&#34;&#34;&#34;.stripMargin) // Here we use MemoryStream to fake a streaming source. val inputData = MemoryStream[(Int, String)] val df = inputData.toDS().toDF(&#34;k&#34;, &#34;v&#34;) // Streaming Write to paimon table. val stream = df .writeStream .outputMode(&#34;append&#34;) .option(&#34;checkpointLocation&#34;, &#34;/path/to/checkpoint&#34;) .format(&#34;paimon&#34;) .start(&#34;/path/to/paimon/sink/table&#34;) Schema Evolution#Schema evolution is a feature that allows users to easily modify the current schema of a table to adapt to existing data, or new data that changes over time, while maintaining data integrity and consistency.
Paimon supports automatic schema merging of source data and current table data while data is being written, and uses the merged schema as the latest schema of the table, and it only requires configuring write.merge-schema.
data.write .format(&#34;paimon&#34;) .mode(&#34;append&#34;) .option(&#34;write.merge-schema&#34;, &#34;true&#34;) .save(location) When enable write.merge-schema, Paimon can allow users to perform the following actions on table schema by default:
Adding columns Up-casting the type of column(e.g. Int -&gt; Long) Paimon also supports explicit type conversions between certain types (e.g. String -&gt; Date, Long -&gt; Int), it requires an explicit configuration write.merge-schema.explicit-cast.
Schema evolution can be used in streaming mode at the same time.
val inputData = MemoryStream[(Int, String)] inputData .toDS() .toDF(&#34;col1&#34;, &#34;col2&#34;) .writeStream .format(&#34;paimon&#34;) .option(&#34;checkpointLocation&#34;, &#34;/path/to/checkpoint&#34;) .option(&#34;write.merge-schema&#34;, &#34;true&#34;) .option(&#34;write.merge-schema.explicit-cast&#34;, &#34;true&#34;) .start(location) Here list the configurations.
Scan ModeDescriptionwrite.merge-schemaIf true, merge the data schema and the table schema automatically before write data.write.merge-schema.explicit-castIf true, allow to merge data types if the two types meet the rules for explicit casting.`}),e.add({id:32,href:"/ecosystem/starrocks/",title:"StarRocks",section:"Ecosystem",content:`StarRocks#This documentation is a guide for using Paimon in StarRocks.
Version#Paimon currently supports StarRocks 3.1 and above. Recommended version is StarRocks 3.2.6 or above.
Create Paimon Catalog#Paimon catalogs are registered by executing a CREATE EXTERNAL CATALOG SQL in StarRocks. For example, you can use the following SQL to create a Paimon catalog named paimon_catalog.
CREATE EXTERNAL CATALOG paimon_catalog PROPERTIES( &#34;type&#34; = &#34;paimon&#34;, &#34;paimon.catalog.type&#34; = &#34;filesystem&#34;, &#34;paimon.catalog.warehouse&#34; = &#34;oss://&lt;your_bucket&gt;/user/warehouse/&#34; ); More catalog types and configures can be seen in Paimon catalog.
Query#Suppose there already exists a database named test_db and a table named test_tbl in paimon_catalog, you can query this table using the following SQL:
SELECT * FROM paimon_catalog.test_db.test_tbl; Query System Tables#You can access all kinds of Paimon system tables by StarRocks. For example, you can read the ro (read-optimized) system table to improve reading performance of primary-key tables.
SELECT * FROM paimon_catalog.test_db.test_tbl$ro; For another example, you can query partition files of the table using the following SQL:
SELECT * FROM paimon_catalog.test_db.partition_tbl$partitions; /* +-----------+--------------+--------------------+------------+----------------------------+ | partition | record_count | file_size_in_bytes | file_count | last_update_time | +-----------+--------------+--------------------+------------+----------------------------+ | [1] | 1 | 645 | 1 | 2024-01-01 00:00:00.000000 | +-----------+--------------+--------------------+------------+----------------------------+ */ StarRocks to Paimon type mapping#This section lists all supported type conversion between StarRocks and Paimon. All StarRocks’s data types can be found in this doc StarRocks Data type overview.
StarRocks Data TypePaimon Data TypeAtomic TypeSTRUCTRowTypefalseMAPMapTypefalseARRAYArrayTypefalseBOOLEANBooleanTypetrueTINYINTTinyIntTypetrueSMALLINTSmallIntTypetrueINTIntTypetrueBIGINTBigIntTypetrueFLOATFloatTypetrueDOUBLEDoubleTypetrueCHAR(length)CharType(length)trueVARCHAR(MAX_VARCHAR_LENGTH)VarCharType(VarCharType.MAX_LENGTH)trueVARCHAR(length)VarCharType(length), length is less than VarCharType.MAX_LENGTHtrueDATEDateTypetrueDATETIMETimestampTypetrueDECIMAL(precision, scale)DecimalType(precision, scale)trueVARBINARY(length)VarBinaryType(length)trueDATETIMELocalZonedTimestampTypetrue`}),e.add({id:33,href:"/append-table/streaming/",title:"Streaming",section:"Table w/o PK",content:`Streaming#You can stream write to the Append table in a very flexible way through Flink, or read the Append table through Flink, using it like a queue. The only difference is that its latency is in minutes. Its advantages are very low cost and the ability to push down filters and projection.
Pre small files merging#&ldquo;Pre&rdquo; means that this compact occurs before committing files to the snapshot.
If Flink&rsquo;s checkpoint interval is short (for example, 30 seconds), each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.
In order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.
Post small files merging#&ldquo;Post&rdquo; means that this compact occurs after committing files to the snapshot.
In streaming write job, without bucket definition, there is no compaction in writer, instead, will use Compact Coordinator to scan the small files and pass compaction task to Compact Worker. In streaming mode, if you run insert sql in flink, the topology will be like this:
Do not worry about backpressure, compaction never backpressure.
If you set write-only to true, the Compact Coordinator and Compact Worker will be removed in the topology.
The auto compaction is only supported in Flink engine streaming mode. You can also start a compaction job in Flink by Flink action in Paimon and disable all the other compactions by setting write-only.
Streaming Query#You can stream the Append table and use it like a Message Queue. As with primary key tables, there are two options for streaming reads:
By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest incremental records. You can specify scan.mode, scan.snapshot-id, scan.timestamp-millis and/or scan.file-creation-time-millis to stream read incremental only. Similar to flink-kafka, order is not guaranteed by default, if your data has some sort of order requirement, you also need to consider defining a bucket-key, see Bucketed Append
`}),e.add({id:34,href:"/primary-key-table/",title:"Table with PK",section:"Apache Paimon",content:``}),e.add({id:35,href:"/concepts/basic-concepts/",title:"基础概念 Basic Concepts",section:"概念 Concepts",content:`基础概念#文件布局#一个表的所有文件都存储在同一个基础目录下。Paimon 的文件采用分层结构组织。下图展示了文件布局。从快照文件开始，Paimon 读取器可以递归访问表中的所有记录。
快照（Snapshot）#所有快照文件存储在 snapshot 目录下。
快照文件是一个 JSON 文件，包含该快照的信息，包括：
使用的 schema 文件 包含该快照所有变更的 manifest 列表 快照捕获了表在某一时间点的状态。用户可以通过最新快照访问表的最新数据。通过时间旅行，用户也可以通过更早的快照访问表的历史状态。
Manifest 文件#所有 manifest 列表和 manifest 文件存储在 manifest 目录下。
manifest 列表是 manifest 文件名的列表。
manifest 文件包含有关 LSM 数据文件和变更日志文件的变更信息，例如在对应快照中新建了哪些 LSM 数据文件，删除了哪些文件。
数据文件#数据文件按分区分组。目前 Paimon 支持使用 parquet（默认）、orc 和 avro 作为数据文件格式。
分区#Paimon 采用与 Apache Hive 相同的分区概念来划分数据。
分区是一种可选的方式，根据特定列（如日期、城市、部门）的值将表划分为相关部分。每个表可以有一个或多个分区键来标识特定分区。
通过分区，用户可以高效地操作表中的某一部分记录。
一致性保证#Paimon 写入器使用两阶段提交协议，原子性地提交一批记录到表中。每次提交最多产生两个 快照，具体取决于增量写入和压缩策略。如果只执行增量写入且未触发压缩操作，则只创建增量快照；如果触发压缩操作，则同时创建增量快照和压缩快照。
对于同时修改同一张表的多个写入器，只要它们不修改同一分区，其提交可以并行进行。如果修改同一分区，则只保证快照隔离。也就是说，最终的表状态可能是两次提交的混合，但不会丢失任何更改。
更多信息请参见 dedicated compaction job。
`}),e.add({id:36,href:"/concepts/spec/schema/",title:"模式文件 Schema",section:"规范说明 Specification",content:`Schema#Schema 文件的版本从 0 开始，目前保留所有版本的 schema。可能存在依赖旧 schema 版本的旧文件，因此删除时需谨慎。
Schema 文件是 JSON 格式，内容包括：
fields：数据字段列表，数据字段包含 id、name、type，字段 id 用于支持 schema 演进。 partitionKeys：字段名列表，表的分区定义，不可修改。 primaryKeys：字段名列表，表的主键定义，不可修改。 options：map&lt;string, string&gt;，无序，表的选项，包括很多功能和优化。 示例 Example#{ &#34;version&#34; : 3, &#34;id&#34; : 0, &#34;fields&#34; : [ { &#34;id&#34; : 0, &#34;name&#34; : &#34;order_id&#34;, &#34;type&#34; : &#34;BIGINT NOT NULL&#34; }, { &#34;id&#34; : 1, &#34;name&#34; : &#34;order_name&#34;, &#34;type&#34; : &#34;STRING&#34; }, { &#34;id&#34; : 2, &#34;name&#34; : &#34;order_user_id&#34;, &#34;type&#34; : &#34;BIGINT&#34; }, { &#34;id&#34; : 3, &#34;name&#34; : &#34;order_shop_id&#34;, &#34;type&#34; : &#34;BIGINT&#34; } ], &#34;highestFieldId&#34; : 3, &#34;partitionKeys&#34; : [ ], &#34;primaryKeys&#34; : [ &#34;order_id&#34; ], &#34;options&#34; : { &#34;bucket&#34; : &#34;5&#34; }, &#34;comment&#34; : &#34;&#34;, &#34;timeMillis&#34; : 1720496663041 } 兼容性 Compatibility#针对旧版本：
版本 1：如果 options 中没有 bucket 键，则应添加 bucket -&gt; 1。 版本 1 和 2：如果 options 中没有 file.format 键，则应添加 file.format -&gt; orc。 DataField#DataField 表示表中的一列。
id：int，列的 ID，自动递增，用于 schema 演进。 name：string，列名。 type：数据类型，类似于 SQL 类型字符串。 description：string，描述。 更新 Schema#Updating the schema should generate a new schema file.
warehouse └── default.db └── my_table ├── schema ├── schema-0 ├── schema-1 └── schema-2 Snapshot 中会引用 schema。数值最大的 schema 文件通常是最新的 schema 文件。
旧的 schema 文件不能直接删除，因为可能存在引用旧 schema 文件的旧数据文件。读取表时需要依赖它们来实现 schema 演进的读取。
`}),e.add({id:37,href:"/primary-key-table/merge-engine/aggregation/",title:"Aggregation",section:"Merge Engine",content:`Aggregation#NOTE: Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig.Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.
Each field not part of the primary keys can be given an aggregate function, specified by the fields.&lt;field-name&gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.
FlinkCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( &#39;merge-engine&#39; = &#39;aggregation&#39;, &#39;fields.price.aggregate-function&#39; = &#39;max&#39;, &#39;fields.sales.aggregate-function&#39; = &#39;sum&#39; ); Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records &lt;1, 23.0, 15&gt; and &lt;1, 30.2, 20&gt;, the final result will be &lt;1, 30.2, 35&gt;.
Aggregation Functions#Current supported aggregate functions and data types are:
sum#The sum function aggregates the values across multiple rows. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.
product#The product function can compute product values across multiple lines. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.
count#In scenarios where counting rows that match a specific condition is required, you can use the SUM function to achieve this. By expressing a condition as a Boolean value (TRUE or FALSE) and converting it into a numerical value, you can effectively count the rows. In this approach, TRUE is converted to 1, and FALSE is converted to 0.
For example, if you have a table orders and want to count the number of rows that meet a specific condition, you can use the following query:
SELECT SUM(CASE WHEN condition THEN 1 ELSE 0 END) AS count FROM orders; max#The max function identifies and retains the maximum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.
min#The min function identifies and retains the minimum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.
last_value#The last_value function replaces the previous value with the most recently imported value. It supports all data types.
last_non_null_value#The last_non_null_value function replaces the previous value with the latest non-null value. It supports all data types.
listagg#The listagg function concatenates multiple string values into a single string. It supports STRING data type. Each field not part of the primary keys can be given a list agg delimiter, specified by the fields..list-agg-delimiter table property, otherwise it will use &ldquo;,&rdquo; as default.
bool_and#The bool_and function evaluates whether all values in a boolean set are true. It supports BOOLEAN data type.
bool_or#The bool_or function checks if at least one value in a boolean set is true. It supports BOOLEAN data type.
first_value#The first_value function retrieves the first null value from a data set. It supports all data types.
first_non_null_value#The first_non_null_value function selects the first non-null value in a data set. It supports all data types.
rbm32#The rbm32 function aggregates multiple serialized 32-bit RoaringBitmap into a single RoaringBitmap. It supports VARBINARY data type.
rbm64#The rbm64 function aggregates multiple serialized 64-bit Roaring64Bitmap into a single Roaring64Bitmap. It supports VARBINARY data type.
nested_update#The nested_update function collects multiple rows into one array (so-called &rsquo;nested table&rsquo;). It supports ARRAY data types.
Use fields.&lt;field-name&gt;.nested-key=pk0,pk1,... to specify the primary keys of the nested table. If no keys, row will be appended to array.
An example:
Flink-- orders table CREATE TABLE orders ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING ); -- sub orders that have the same order_id -- belongs to the same order CREATE TABLE sub_orders ( order_id BIGINT, sub_order_id INT, product_name STRING, price BIGINT, PRIMARY KEY (order_id, sub_order_id) NOT ENFORCED ); -- wide table CREATE TABLE order_wide ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING, sub_orders ARRAY&lt;ROW&lt;sub_order_id BIGINT, product_name STRING, price BIGINT&gt;&gt; ) WITH ( &#39;merge-engine&#39; = &#39;aggregation&#39;, &#39;fields.sub_orders.aggregate-function&#39; = &#39;nested_update&#39;, &#39;fields.sub_orders.nested-key&#39; = &#39;sub_order_id&#39; ); -- widen INSERT INTO order_wide SELECT order_id, user_name, address, CAST (NULL AS ARRAY&lt;ROW&lt;sub_order_id BIGINT, product_name STRING, price BIGINT&gt;&gt;) FROM orders UNION ALL SELECT order_id, CAST (NULL AS STRING), CAST (NULL AS STRING), ARRAY[ROW(sub_order_id, product_name, price)] FROM sub_orders; -- query using UNNEST SELECT order_id, user_name, address, sub_order_id, product_name, price FROM order_wide, UNNEST(sub_orders) AS so(sub_order_id, product_name, price) collect#The collect function collects elements into an Array. You can set fields.&lt;field-name&gt;.distinct=true to deduplicate elements. It only supports ARRAY type.
merge_map#The merge_map function merge input maps. It only supports MAP type.
Types of cardinality sketches#Paimon uses the Apache DataSketches library of stochastic streaming algorithms to implement sketch modules. The DataSketches library includes various types of sketches, each one designed to solve a different sort of problem. Paimon supports HyperLogLog (HLL) and Theta cardinality sketches.
HyperLogLog#The HyperLogLog (HLL) sketch aggregator is a very compact sketch algorithm for approximate distinct counting. You can also use the HLL aggregator to calculate a union of HLL sketches.
Theta#The Theta sketch is a sketch algorithm for approximate distinct counting with set operations. Theta sketches let you count the overlap between sets, so that you can compute the union, intersection, or set difference between sketch objects.
Choosing a sketch type#HLL and Theta sketches both support approximate distinct counting; however, the HLL sketch produces more accurate results and consumes less storage space. Theta sketches are more flexible but require significantly more memory.
When choosing an approximation algorithm for your use case, consider the following:
If your use case entails distinct counting and merging sketch objects, use the HLL sketch. If you need to evaluate union, intersection, or difference set operations, use the Theta sketch. You cannot merge HLL sketches with Theta sketches.
hll_sketch#The hll_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.
An example:
Flink-- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( &#39;merge-engine&#39; = &#39;aggregation&#39;, &#39;fields.uv.aggregate-function&#39; = &#39;hll_sketch&#39; ); -- Register the following class as a Flink function with the name &#34;HLL_SKETCH&#34; -- for example: create TEMPORARY function HLL_SKETCH as &#39;HllSketchFunction&#39;; -- which is used to transform input to sketch bytes array: -- -- public static class HllSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- HllSketch hllSketch = new HllSketch(); -- hllSketch.update(user_id); -- return hllSketch.toCompactByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, HLL_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name &#34;HLL_SKETCH_COUNT&#34; -- for example: create TEMPORARY function HLL_SKETCH_COUNT as &#39;HllSketchCountFunction&#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class HllSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return HllSketch.heapify(sketchBytes).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, HLL_SKETCH_COUNT(UV) as uv FROM UV_AGG; theta_sketch#The theta_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.
An example:
Flink-- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( &#39;merge-engine&#39; = &#39;aggregation&#39;, &#39;fields.uv.aggregate-function&#39; = &#39;theta_sketch&#39; ); -- Register the following class as a Flink function with the name &#34;THETA_SKETCH&#34; -- for example: create TEMPORARY function THETA_SKETCH as &#39;ThetaSketchFunction&#39;; -- which is used to transform input to sketch bytes array: -- -- public static class ThetaSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- UpdateSketch updateSketch = UpdateSketch.builder().build(); -- updateSketch.update(user_id); -- return updateSketch.compact().toByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, THETA_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name &#34;THETA_SKETCH_COUNT&#34; -- for example: create TEMPORARY function THETA_SKETCH_COUNT as &#39;ThetaSketchCountFunction&#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class ThetaSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return Sketches.wrapCompactSketch(Memory.wrap(sketchBytes)).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, THETA_SKETCH_COUNT(UV) as uv FROM UV_AGG; For streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer. (&lsquo;input&rsquo; changelog producer is also supported, but only returns input records.)Retraction#Only sum, product, collect, merge_map, nested_update, last_value and last_non_null_value supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.\${field_name}.ignore-retract'='true'.
The last_value and last_non_null_value just set field to null when accept retract messages.
The product will return null for retraction message when accumulator is null.
The collect and merge_map make a best-effort attempt to handle retraction messages, but the results are not guaranteed to be accurate. The following behaviors may occur when processing retraction messages:
It might fail to handle retraction messages if records are disordered. For example, the table uses collect, and the upstreams send +I['A', 'B'] and -U['A'] respectively. If the table receives -U['A'] first, it can do nothing; then it receives +I['A', 'B'], the merge result will be +I['A', 'B'] instead of +I['B'].
The retract message from one upstream will retract the result merged from multiple upstreams. For example, the table uses merge_map, and one upstream sends +I[1-&gt;A], another upstream sends +I[1-&gt;B], -D[1-&gt;B] later. The table will merge two insert values to +I[1-&gt;B] first, and then the -D[1-&gt;B] will retract the whole result, so the final result is an empty map instead of +I[1-&gt;A]
`}),e.add({id:38,href:"/project/contributing/",title:"Contributing",section:"Project",content:`Contributing#Apache Paimon is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.
What do you want to do?#Contributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:
AreaFurther informationReport BugTo report a problem with Paimon, open Paimon’s issues. Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.Contribute CodeRead the Code Contribution GuideCode ReviewsRead the Code Review GuideRelease VersionReleasing a new Paimon version.Support UsersReply to questions on the user mailing list,check the latest issues in Issues for tickets which are actually user questions.Spread the Word About PaimonOrganize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blogpost on the dev@paimon.apache.org mailing list.Any other question? Reach out to thedev@paimon.apache.org mailing list to get help!Code Contribution Guide#Apache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.
Please feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.
1DiscussCreate an Issue or mailing list discussion and reach consensus
To request an issue, please note that it is not just a "please assign it to me", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.
2ImplementCreate the Pull Request and the approach agreed upon in the issue.
1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.
3ReviewWork with the reviewer.
1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.
4MergeA committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.
Code Review Guide#Every review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.
1. Is the Contribution Well-Described?#Check whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.
If the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.
2. Does the Contribution Need Attention from some Specific Committers?#Some changes require attention and approval from specific committers.
If the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.
3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon?#Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated? Code guidelines can be found in the Flink Java Code Style and Quality Guide.
4. Are the documentation updated?#If the pull request introduces a new feature, the feature should be documented.
Become a Committer#When you have made enough contributions, you can be nominated as Paimon&rsquo;s Committer. See Committer.
`}),e.add({id:39,href:"/concepts/rest/dlf/",title:"DLF Token",section:"RESTCatalog",content:`DLF Token#DLF (Data Lake Formation) building is a fully-managed platform for unified metadata and data storage and management, aiming to provide customers with functions such as metadata management, storage management, permission management, storage analysis, and storage optimization.
DLF provides multiple authentication methods for different environments.
The 'warehouse' is your catalog instance name on the server, not the path.Use the access key#CREATE CATALOG \`paimon-rest-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;uri&#39; = &#39;&lt;catalog server url&gt;&#39;, &#39;metastore&#39; = &#39;rest&#39;, &#39;warehouse&#39; = &#39;my_instance_name&#39;, &#39;token.provider&#39; = &#39;dlf&#39;, &#39;dlf.access-key-id&#39;=&#39;&lt;access-key-id&gt;&#39;, &#39;dlf.access-key-secret&#39;=&#39;&lt;access-key-secret&gt;&#39;, ); You can grant specific permissions to a RAM user and use the RAM user&rsquo;s access key for long-term access to your DLF resources. Compared to using the Alibaba Cloud account access key, accessing DLF resources with a RAM user access key is more secure.
Use the STS temporary access token#Through the STS service, you can generate temporary access tokens for users, allowing them to access DLF resources restricted by policies within the validity period.
CREATE CATALOG \`paimon-rest-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;uri&#39; = &#39;&lt;catalog server url&gt;&#39;, &#39;metastore&#39; = &#39;rest&#39;, &#39;warehouse&#39; = &#39;my_instance_name&#39;, &#39;token.provider&#39; = &#39;dlf&#39;, &#39;dlf.access-key-id&#39;=&#39;&lt;access-key-id&gt;&#39;, &#39;dlf.access-key-secret&#39;=&#39;&lt;access-key-secret&gt;&#39;, &#39;dlf.security-token&#39;=&#39;&lt;security-token&gt;&#39; ); In some environments, temporary access token can be periodically refreshed by using a local file:
CREATE CATALOG \`paimon-rest-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;uri&#39; = &#39;&lt;catalog server url&gt;&#39;, &#39;metastore&#39; = &#39;rest&#39;, &#39;warehouse&#39; = &#39;my_instance_name&#39;, &#39;token.provider&#39; = &#39;dlf&#39;, &#39;dlf.token-path&#39; = &#39;my_token_path_in_disk&#39; ); Use the STS token from aliyun ecs role#An instance RAM role refers to a RAM role granted to an ECS instance. This RAM role is a standard service role with the trusted entity being the cloud server. By using an instance RAM role, it is possible to obtain temporary access token (STS Token) within the ECS instance without configuring an AccessKey.
CREATE CATALOG \`paimon-rest-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;uri&#39; = &#39;&lt;catalog server url&gt;&#39;, &#39;metastore&#39; = &#39;rest&#39;, &#39;warehouse&#39; = &#39;my_instance_name&#39;, &#39;token.provider&#39; = &#39;dlf&#39;, &#39;dlf.token-loader&#39; = &#39;ecs&#39; -- optional, loader can obtain it through ecs metadata service -- &#39;dlf.token-ecs-role-name&#39; = &#39;my_ecs_role_name&#39; ); `}),e.add({id:40,href:"/ecosystem/doris/",title:"Doris",section:"Ecosystem",content:`Doris#This documentation is a guide for using Paimon in Doris.
More details can be found in Apache Doris Website
Version#Paimon currently supports Apache Doris 2.0.6 and above.
Create Paimon Catalog#Use CREATE CATALOG statement in Apache Doris to create Paimon Catalog.
Doris support multi types of Paimon Catalogs. Here are some examples:
-- HDFS based Paimon Catalog CREATE CATALOG \`paimon_hdfs\` PROPERTIES ( &#34;type&#34; = &#34;paimon&#34;, &#34;warehouse&#34; = &#34;hdfs://172.21.0.1:8020/user/paimon&#34;, &#34;hadoop.username&#34; = &#34;hadoop&#34; ); -- Aliyun OSS based Paimon Catalog CREATE CATALOG \`paimon_oss\` PROPERTIES ( &#34;type&#34; = &#34;paimon&#34;, &#34;warehouse&#34; = &#34;oss://paimon-bucket/paimonoss&#34;, &#34;oss.endpoint&#34; = &#34;oss-cn-beijing.aliyuncs.com&#34;, &#34;oss.access_key&#34; = &#34;ak&#34;, &#34;oss.secret_key&#34; = &#34;sk&#34; ); -- Hive Metastore based Paimon Catalog CREATE CATALOG \`paimon_hms\` PROPERTIES ( &#34;type&#34; = &#34;paimon&#34;, &#34;paimon.catalog.type&#34; = &#34;hms&#34;, &#34;warehouse&#34; = &#34;hdfs://172.21.0.1:8020/user/zhangdong/paimon2&#34;, &#34;hive.metastore.uris&#34; = &#34;thrift://172.21.0.44:7004&#34;, &#34;hadoop.username&#34; = &#34;hadoop&#34; ); -- Integrate with Aliyun DLF CREATE CATALOG paimon_dlf PROPERTIES ( &#39;type&#39; = &#39;paimon&#39;, &#39;paimon.catalog.type&#39; = &#39;dlf&#39;, &#39;warehouse&#39; = &#39;oss://paimon-bucket/paimonoss/&#39;, &#39;dlf.proxy.mode&#39; = &#39;DLF_ONLY&#39;, &#39;dlf.uid&#39; = &#39;xxxxx&#39;, &#39;dlf.region&#39; = &#39;cn-beijing&#39;, &#39;dlf.access_key&#39; = &#39;ak&#39;, &#39;dlf.secret_key&#39; = &#39;sk&#39; ); See Apache Doris Website for more examples.
Access Paimon Catalog#Query Paimon table with full qualified name
SELECT * FROM paimon_hdfs.paimon_db.paimon_table; Switch to Paimon Catalog and query
SWITCH paimon_hdfs; USE paimon_db; SELECT * FROM paimon_table; Query Optimization#Read optimized for Primary Key Table
Doris can utilize the Read optimized feature for Primary Key Table(release in Paimon 0.6), by reading base data files using native Parquet/ORC reader and delta file using JNI.
Deletion Vectors
Doris(2.1.4+) natively supports Deletion Vectors(released in Paimon 0.8).
Doris to Paimon type mapping#Doris Data TypePaimon Data TypeAtomic TypeBooleanBooleanTypetrueTinyIntTinyIntTypetrueSmallIntSmallIntTypetrueIntIntTypetrueBigIntBigIntTypetrueFloatFloatTypetrueDoubleDoubleTypetrueVarcharVarCharTypetrueCharCharTypetrueBinaryVarBinaryType, BinaryTypetrueDecimal(precision, scale)DecimalType(precision, scale)trueDatetimeTimestampType,LocalZonedTimestampTypetrueDateDateTypetrueArrayArrayTypefalseMapMapTypefalseStructRowTypefalse`}),e.add({id:41,href:"/program-api/java-api/",title:"Java API",section:"Program API",content:`Java API#If possible, recommend using computing engines such as Flink SQL or Spark SQL.Dependency#Maven dependency:
&lt;dependency&gt; &lt;groupId&gt;org.apache.paimon&lt;/groupId&gt; &lt;artifactId&gt;paimon-bundle&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; Or download the jar file: Paimon Bundle.Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.
Create Catalog#Before coming into contact with the Table, you need to create a Catalog.
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static Catalog createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(&#34;...&#34;)); return CatalogFactory.createCatalog(context); } public static Catalog createHiveCatalog() { // Paimon Hive catalog relies on Hive jars // You should add hive classpath or hive bundled jar. Options options = new Options(); options.set(&#34;warehouse&#34;, &#34;...&#34;); options.set(&#34;metastore&#34;, &#34;hive&#34;); options.set(&#34;uri&#34;, &#34;...&#34;); options.set(&#34;hive-conf-dir&#34;, &#34;...&#34;); options.set(&#34;hadoop-conf-dir&#34;, &#34;...&#34;); CatalogContext context = CatalogContext.create(options); return CatalogFactory.createCatalog(context); } } Create Table#You can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(&#34;f0&#34;, &#34;f1&#34;); schemaBuilder.partitionKeys(&#34;f1&#34;); schemaBuilder.column(&#34;f0&#34;, DataTypes.STRING()); schemaBuilder.column(&#34;f1&#34;, DataTypes.INT()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } } } Get Table#The Table interface provides access to the table metadata and tools to read and write table.
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.table.Table; public class GetTable { public static Table getTable() { Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); return catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something throw new RuntimeException(&#34;table not exist&#34;); } } } Batch Read#For relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.
But if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.
The reading is divided into two stages:
Scan Plan: Generate plan splits in a global node (&lsquo;Coordinator&rsquo;, or named &lsquo;Driver&rsquo;). Read Split: Read split in distributed tasks. import org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class ReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (\`withFilter\`) // and projection (\`withProjection\`) if necessary Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[]{0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in &#39;Coordinator&#39; (or named &#39;Driver&#39;) List&lt;Split&gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks // 4. Read a split in task TableRead read = readBuilder.newRead(); RecordReader&lt;InternalRow&gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); } } Batch Write#The writing is divided into two stages:
Write records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (&lsquo;Coordinator&rsquo;, or named &lsquo;Driver&rsquo;, or named &lsquo;Committer&rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages. import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; import java.util.List; public class BatchWrite { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable) Table table = GetTable.getTable(); BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder().withOverwrite(); // 2. Write records in distributed tasks BatchTableWrite write = writeBuilder.newWrite(); GenericRow record1 = GenericRow.of(BinaryString.fromString(&#34;Alice&#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(&#34;Bob&#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(&#34;Emily&#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector. // WriteSelector determines to which logical downstream writers a record should be written to. // If it returns empty, no data distribution is required. write.write(record1); write.write(record2); write.write(record3); List&lt;CommitMessage&gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files // commit.abort(messages); } } Stream Read#The difference of Stream Read is that StreamTableScan can continuously scan and generate splits.
StreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.
import org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class StreamReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (\`withFilter\`) // and projection (\`withProjection\`) if necessary Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[]{0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in &#39;Coordinator&#39; (or named &#39;Driver&#39;) StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List&lt;Split&gt; splits = scan.plan().splits(); // Distribute these splits to different tasks Long state = scan.checkpoint(); // can be restored in scan.restore(state) after fail over // 3. Read a split in task TableRead read = readBuilder.newRead(); RecordReader&lt;InternalRow&gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); Thread.sleep(1000); } } } Stream Write#The difference of Stream Write is that StreamTableCommit can continuously commit.
Key points to achieve exactly-once consistency:
CommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterAndCommit to exclude the committed messages by commitIdentifier. import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; import java.util.List; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable) Table table = GetTable.getTable(); StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId long commitIdentifier = 0; while (true) { GenericRow record1 = GenericRow.of(BinaryString.fromString(&#34;Alice&#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(&#34;Bob&#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(&#34;Emily&#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector. // WriteSelector determines to which logical downstream writers a record should be written to. // If it returns empty, no data distribution is required. write.write(record1); write.write(record2); write.write(record3); List&lt;CommitMessage&gt; messages = write.prepareCommit(false, commitIdentifier); commitIdentifier++; // 3. Collect all CommitMessages to a global node and commit StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failure occurs and you&#39;re not sure if the commit process is successful, // you can use \`filterAndCommit\` to retry the commit process. // Succeeded commits will be automatically skipped. /* Map&lt;Long, List&lt;CommitMessage&gt;&gt; commitIdentifiersAndMessages = new HashMap&lt;&gt;(); commitIdentifiersAndMessages.put(commitIdentifier, messages); commit.filterAndCommit(commitIdentifiersAndMessages); */ Thread.sleep(1000); } } } Data Types#Java Paimon boolean boolean byte byte short short int int long long float float double double string org.apache.paimon.data.BinaryString decimal org.apache.paimon.data.Decimal timestamp org.apache.paimon.data.Timestamp byte[] byte[] array org.apache.paimon.data.InternalArray map org.apache.paimon.data.InternalMap InternalRow org.apache.paimon.data.InternalRow Predicate Types#SQL Predicate Paimon Predicate and org.apache.paimon.predicate.PredicateBuilder.And or org.apache.paimon.predicate.PredicateBuilder.Or is null org.apache.paimon.predicate.PredicateBuilder.IsNull is not null org.apache.paimon.predicate.PredicateBuilder.IsNotNull in org.apache.paimon.predicate.PredicateBuilder.In not in org.apache.paimon.predicate.PredicateBuilder.NotIn = org.apache.paimon.predicate.PredicateBuilder.Equal &lt;&gt; org.apache.paimon.predicate.PredicateBuilder.NotEqual &lt; org.apache.paimon.predicate.PredicateBuilder.LessThan &lt;= org.apache.paimon.predicate.PredicateBuilder.LessOrEqual &gt; org.apache.paimon.predicate.PredicateBuilder.GreaterThan &gt;= org.apache.paimon.predicate.PredicateBuilder.GreaterOrEqual `}),e.add({id:42,href:"/cdc-ingestion/kafka-cdc/",title:"Kafka CDC",section:"CDC Ingestion",content:`Kafka CDC#Prepare Kafka Bundled Jar#flink-sql-connector-kafka-*.jar Supported Formats#Flink provides several Kafka CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.
FormatsSupportedCanal CDCTrueDebezium CDCTrueMaxwell CDCTrueOGG CDCTrueJSONTrueaws-dms-jsonTruedebezium-bsonTrueThe JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don&rsquo;t contain field types; When you write JSON sources into Flink Kafka sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:
Usually, debezium-json contains &lsquo;schema&rsquo; field, from which Paimon will retrieve data types. Make sure your debezium json has this field, or Paimon will use &lsquo;STRING&rsquo; type. If missing field types, Paimon will use &lsquo;STRING&rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization. Synchronizing Tables#By using KafkaSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Kafka&rsquo;s one topic into one Paimon table.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--type_mapping to-string] \\ [--computed_column &lt;&#39;column-name=expr-name(args[, ...])&#39;&gt; [--computed_column ...]] \\ [--kafka_conf &lt;kafka-source-conf&gt; [--kafka_conf &lt;kafka-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--tableThe Paimon table name.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option."decimal-no-change": Ignore decimal type change.--computed_columnThe definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations. --kafka_confThe configuration for Flink Kafka sources. Each configuration should be specified in the format \`key=value\`. \`properties.bootstrap.servers\`, \`topic/topic-pattern\`, \`properties.group.id\`, and \`value.format\` are required configurations, others are optional.See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Kafka topic&rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Kafka topic&rsquo;s tables.
Example 1:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 If the kafka topic doesn&rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.
NOTE: In this case you shouldn&rsquo;t use &ndash;partition_keys or &ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.
Example 2: If you want to synchronize a table which has primary key &lsquo;id INT&rsquo;, and you want to compute a partition key &lsquo;part=date_format(create_time,yyyy-MM-dd)&rsquo;, you can create a such table first (the other columns can be omitted):
CREATE TABLE test_db.test_table ( id INT, -- primary key create_time TIMESTAMP, -- the argument of computed column part part STRING, -- partition key PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --computed_column &#39;part=date_format(create_time,yyyy-MM-dd)&#39; \\ ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use &lsquo;format=json&rsquo; to synchronize such data to the Paimon table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column &#39;pt=date_format(event_tm, yyyyMMdd)&#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=test_log \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf sink.parallelism=4 Synchronizing Databases#By using KafkaSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_database \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ [--table_mapping &lt;table-name&gt;=&lt;paimon-table-name1&gt; [--table_mapping &lt;table-name2&gt;=&lt;paimon-table-name2&gt; ...]] \\ [--table_prefix &lt;paimon-table-prefix&gt;] \\ [--table_suffix &lt;paimon-table-suffix&gt;] \\ [--table_prefix_db &lt;db-name1&gt;=&lt;table-prefix1&gt; [--table_prefix_db &lt;db-name2&gt;=&lt;table-prefix2&gt; ...]] \\ [--table_suffix_db &lt;db-name1&gt;=&lt;table-suffix1&gt; [--table_suffix_db &lt;db-name2&gt;=&lt;table-suffix2&gt; ...]] \\ [--including_tables &lt;table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;table-name|name-regular-expr&gt;] \\ [--including_dbs &lt;database-name|name-regular-expr&gt;] \\ [--excluding_dbs &lt;database-name|name-regular-expr&gt;] \\ [--type_mapping to-string] \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--kafka_conf &lt;kafka-source-conf&gt; [--kafka_conf &lt;kafka-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--ignore_incompatibleIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.--table_mappingThe table name mapping between source database and Paimon. For example, if you want to synchronize a source table named "test" to a Paimon table named "paimon_test", you can specify "--table_mapping test=paimon_test". Multiple mappings could be specified with multiple "--table_mapping" options. "--table_mapping" has higher priority than "--table_prefix" and "--table_suffix".--table_prefixThe prefix of all Paimon tables to be synchronized except those specified by "--table_mapping" or "--table_prefix_db". For example, if you want all synchronized tables to have "ods_" as prefix, you can specify "--table_prefix ods_".--table_suffixThe suffix of all Paimon tables to be synchronized except those specified by "--table_mapping" or "--table_suffix_db". The usage is same as "--table_prefix".--table_prefix_dbThe prefix of the Paimon tables to be synchronized from the specified db. For example, if you want to prefix the tables from db1 with "ods_db1_", you can specify "--table_prefix_db db1=ods_db1_". Multiple mappings could be specified multiple "--table_prefix_db" options. "--table_prefix_db" has higher priority than "--table_prefix".--table_suffix_dbThe suffix of the Paimon tables to be synchronized from the specified db. The usage is same as "--table_prefix_db".--including_tablesIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying "--including_tables test|paimon.*" means to synchronize table 'test' and all tables start with 'paimon'.--excluding_tablesIt is used to specify which source tables are not to be synchronized. The usage is same as "--including_tables". "--excluding_tables" has higher priority than "--including_tables" if you specified both.--including_dbsIt is used to specify the databases within which the tables are to be synchronized. The usage is same as "--including_tables".--excluding_dbsIt is used to specify the databases within which the tables are not to be synchronized. The usage is same as "--excluding_tables". "--excluding_dbs" has higher priority than "--including_dbs" if you specified both.--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option."decimal-no-change": Ignore decimal type change.--computed_columnThe definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations. --eager_initIt is default false. If true, all relevant tables commiter will be initialized eagerly, which means those tables could be forced to create snapshot.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".If the keys are not in source table, the sink table won't set partition keys.--multiple_table_partition_keysThe partition keys for each different Paimon table. If there are multiple partition keys, connect them with comma, for example--multiple_table_partition_keys tableName1=col1,col2.col3--multiple_table_partition_keys tableName2=col4,col5.col6--multiple_table_partition_keys tableName3=col7,col8.col9If the keys are not in source table, the sink table won't set partition keys.--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.Otherwise, the sink table won't set primary keys.--kafka_confThe configuration for Flink Kafka sources. Each configuration should be specified in the format \`key=value\`. \`properties.bootstrap.servers\`, \`topic/topic-pattern\`, \`properties.group.id\`, and \`value.format\` are required configurations, others are optional.See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.This action will build a single combined sink for all tables. For each Kafka topic&rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Kafka topic&rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Kafka record, this action will try to preform schema evolution.
Example
Synchronization from one Kafka topic to Paimon database.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronization from multiple Kafka topics to Paimon database.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order\\;logistic_order\\;user \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Additional kafka_config#There are some useful options to build Flink Kafka Source, but they are not provided by flink-kafka-connector document. They are:
KeyDefaultTypeDescriptionschema.registry.url(none)StringWhen configuring "value.format=debezium-avro" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.`}),e.add({id:43,href:"/iceberg/primary-key-table/",title:"Primary Key Table",section:"Iceberg Metadata",content:"\rPrimary Key Tables\r#\rLet&rsquo;s walk through a simple example, where we query Paimon tables with Iceberg connectors in Flink and Spark. Before trying out this example, make sure that your compute engine already supports Iceberg. Please refer to Iceberg&rsquo;s document if you haven&rsquo;t set up Iceberg.\nFlink: Preparation when using Flink SQL Client Spark: Using Iceberg in Spark 3 Flink SQL\rCREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;&#39; ); CREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( &#39;metadata.iceberg.storage&#39; = &#39;hadoop-catalog&#39;, &#39;compaction.optimization-interval&#39; = &#39;1ms&#39; -- ATTENTION: this option is only for testing, see &#34;timeliness&#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, &#39;SUBMITTED&#39;, CAST(NULL AS DOUBLE)), (2, &#39;COMPLETED&#39;, 200.0), (3, &#39;SUBMITTED&#39;, CAST(NULL AS DOUBLE)); CREATE CATALOG iceberg_catalog WITH ( &#39;type&#39; = &#39;iceberg&#39;, &#39;catalog-type&#39; = &#39;hadoop&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;/iceberg&#39;, &#39;cache-enabled&#39; = &#39;false&#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = &#39;COMPLETED&#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, &#39;COMPLETED&#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = &#39;COMPLETED&#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 1 | COMPLETED | 100.0 | | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ Spark SQL\rStart spark-sql with the following command line.\nspark-sql --jars &lt;path-to-paimon-jar&gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=&lt;path-to-warehouse&gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-&lt;iceberg-version&gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=&lt;path-to-warehouse&gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.\nCREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE ) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;order_id&#39;, &#39;metadata.iceberg.storage&#39; = &#39;hadoop-catalog&#39;, &#39;compaction.optimization-interval&#39; = &#39;1ms&#39; -- ATTENTION: this option is only for testing, see &#34;timeliness&#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, &#39;SUBMITTED&#39;, CAST(NULL AS DOUBLE)), (2, &#39;COMPLETED&#39;, 200.0), (3, &#39;SUBMITTED&#39;, CAST(NULL AS DOUBLE)); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = &#39;COMPLETED&#39;; /* 2 COMPLETED 200.0 */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, &#39;COMPLETED&#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = &#39;COMPLETED&#39;; /* 2 COMPLETED 200.0 1 COMPLETED 100.0 */ Paimon primary key tables organize data files as LSM trees, so data files must be merged in memory before querying. However, Iceberg readers are not able to merge data files, so they can only query data files on the highest level of LSM trees. Data files on the highest level are produced by the full compaction process. So to conclude, for primary key tables, Iceberg readers can only query data after full compaction.\nBy default, there is no guarantee on how frequently Paimon will perform full compaction. You can configure the following table option, so that Paimon is forced to perform full compaction after several commits.\nOption\rDefault\rType\rDescription\rcompaction.optimization-interval\r(none)\rDuration\rFull compaction will be constantly triggered per time interval. First compaction after the job starts will always be full compaction.\rfull-compaction.delta-commits\r(none)\rInteger\rFull compaction will be constantly triggered after delta commits. Only implemented in Flink.\rNote that full compaction is a resource-consuming process, so the value of this table option should not be too small. We recommend full compaction to be performed once or twice per hour.\nDeletion Vector Support\r#\rDeletion vectors in Paimon are used to store deleted records for each file. Under deletion-vector mode, paimon readers can directly filter out unnecessary records during reading phase without merging data. Fortunately, Iceberg has supported deletion vectors in Version 3. This means that if the Iceberg reader can recognize Paimon&rsquo;s deletion vectors, it will be able to read all of Paimon&rsquo;s data, even without the ability to merge data files. With Paimon&rsquo;s deletion vectors synchronized to Iceberg, Iceberg reader and Paimon reader can achieve true real-time synchronization.\nIf the following conditions are met, it will construct metadata about Paimon&rsquo;s deletion vectors for Iceberg.\n&lsquo;deletion-vectors.enabled&rsquo; and &lsquo;deletion-vectors.bitmap64&rsquo; should be set to true. Because only 64-bit bitmap implementation of deletion vector in Paimon is compatible with Iceberg. &lsquo;metadata.iceberg.format-version&rsquo;(default value is 2) should be set to 3. Because Iceberg only supports deletion vector in V3. Version of Iceberg should be 1.8.0+. JDK version should be 11+. Iceberg has stopped supporting JDK 8 since version 1.7.0. Here is an example: Flink SQL\r-- flink version: 1.20.1 CREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;&#39; ); -- Create a paimon table with primary key and enable deletion vector CREATE TABLE paimon_catalog.`default`.T ( pt INT ,k INT ,v INT ,PRIMARY KEY (pt, k) NOT ENFORCED )PARTITIONED BY (pt) WITH ( &#39;metadata.iceberg.storage&#39; = &#39;hadoop-catalog&#39; ,&#39;metadata.iceberg.format-version&#39; = &#39;3&#39; ,&#39;deletion-vectors.enabled&#39; = &#39;true&#39; ,&#39;deletion-vectors.bitmap64&#39; = &#39;true&#39; ); INSERT INTO paimon_catalog.`default`.T VALUES (1, 9, 90), (1, 10, 100), (1, 11, 110), (2, 20, 200) ; -- iceberg version: 1.8.1 CREATE CATALOG iceberg_catalog WITH ( &#39;type&#39; = &#39;iceberg&#39;, &#39;catalog-type&#39; = &#39;hadoop&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;/iceberg&#39;, &#39;cache-enabled&#39; = &#39;false&#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.T; /* +------------+------------+------------+ | pt | k | v | +------------+------------+------------+ | 2 | 20 | 200 | | 1 | 9 | 90 | | 1 | 10 | 100 | | 1 | 11 | 110 | +------------+------------+------------+ */ -- insert some data again, this will generate deletion vectors INSERT INTO paimon_catalog.`default`.T VALUES (1, 10, 101), (2, 20, 201), (1, 12, 121) ; -- select deletion-vector index in paimon SELECT * FROM paimon_catalog.`default`.`T$table_indexes` WHERE index_type=&#39;DELETION_VECTORS&#39;; /* +------------+-----------+-------------------+------------------------ -----+------------+------------+--------------------------------+ | partition | bucket | index_type | file_name | file_size | row_count | dv_ranges | +------------+-----------+-------------------+------------------------ -----+------------+------------+--------------------------------+ | {1} | 0 | DELETION_VECTORS | index-4ae44c5d-2fc6-40b0-9ff0~ | 43 | 1 | [(data-968fdf3a-2f44-41df-89b~ | +------------+-----------+-------------------+------------------------ -----+------------+------------+--------------------------------+ */ -- select in iceberg, the updates was successfully read by iceberg SELECT * FROM iceberg_catalog.`default`.T; /* +------------+------------+------------+ | pt | k | v | +------------+------------+------------+ | 1 | 9 | 90 | | 1 | 11 | 110 | | 2 | 20 | 201 | | 1 | 10 | 101 | | 1 | 12 | 121 | +------------+------------+------------+ */ note1: Upgrade the implementation of deletion vector to 64-bit bitmap if necessary.\rIf your paimon table has already been in deletion-vector mode, but 32-bit bitmap was used for deletion vector. You need to upgrade the implementation of deletion vector to 64-bit bitmap if you want to synchronize deletion-vector metadata to iceberg. You can follow the following steps to upgrade to 64-bit deletion-vector:\nstop all the writing jobs of your paimon table. perform a full compaction to your paimon table. run ALTER TABLE tableName SET ('deletion-vectors.bitmap64' = 'true') to upgrade to 64-bit deletion vector. restart your writing job. If meeting the all the conditions mentioned above, deletion vector metadata will be synchronized to iceberg. note2: Upgrade the format version of iceberg to 3 if necessary.\rYou can upgrade the format version of iceberg from 2 to 3 by setting 'metadata.iceberg.format-version' = '3'. This will recreate the iceberg metadata without using the base metadata.\n"}),e.add({id:44,href:"/append-table/query-performance/",title:"Query Performance",section:"Table w/o PK",content:`Query Performance#Data Skipping By Order#Paimon by default records the maximum and minimum values of each field in the manifest file.
In the query, according to the WHERE condition of the query, together with the statistics in the manifest we can perform file filtering. If the filtering effect is good, the query that would have cost minutes will be accelerated to milliseconds to complete the execution.
Often the data distribution is not always ideal for filtering, so can we sort the data by the field in WHERE condition? You can take a look at Flink COMPACT Action, Flink COMPACT Procedure or Spark COMPACT Procedure.
Data Skipping By File Index#You can use file index too, it filters files by indexing on the reading side.
CREATE TABLE &lt;PAIMON_TABLE&gt; (&lt;COLUMN&gt; &lt;COLUMN_TYPE&gt; , ...) WITH ( &#39;file-index.bloom-filter.columns&#39; = &#39;c1,c2&#39;, &#39;file-index.bloom-filter.c1.items&#39; = &#39;200&#39; ); Define file-index.bloom-filter.columns, Data file index is an external index file and Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, otherwise in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.
Different file indexes may be efficient in different scenarios. For example bloom filter may speed up query in point lookup scenario. Using a bitmap may consume more space but can result in greater accuracy.
Bloom Filter:
file-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.&lt;column_name&gt;.fpp to config false positive probability. file-index.bloom-filter.&lt;column_name&gt;.items to config the expected distinct items in one data file. Bitmap:
file-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap. Bit-Slice Index Bitmap
file-index.bsi.columns: specify the columns that need bsi index. More filter types will be supported&hellip;
If you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.&lt;filter-type&gt;.columns to the table.
How to invoke: see flink procedures
`}),e.add({id:45,href:"/primary-key-table/table-mode/",title:"Table Mode",section:"Table with PK",content:`Table Mode#The file structure of the primary key table is roughly shown in the above figure. The table or partition contains multiple buckets, and each bucket is a separate LSM tree structure that contains multiple files.
The writing process of LSM is roughly as follows: Flink checkpoint flush L0 files, and trigger a compaction as needed to merge the data. According to the different processing ways during writing, there are three modes:
MOR (Merge On Read): Default mode, only minor compactions are performed, and merging are required for reading. COW (Copy On Write): Using 'full-compaction.delta-commits' = '1', full compaction will be synchronized, which means the merge is completed on write. MOW (Merge On Write): Using 'deletion-vectors.enabled' = 'true', in writing phase, LSM will be queried to generate the deletion vector file for the data file, which directly filters out unnecessary lines during reading. The Merge On Write mode is recommended for general primary key tables (merge-engine is default deduplicate).
Merge On Read#MOR is the default mode of primary key table.
When the mode is MOR, it is necessary to merge all files for reading, as all files are ordered and undergo multi way merging, which includes a comparison calculation of the primary key.
There is an obvious issue here, where a single LSM tree can only have a single thread to read, so the read parallelism is limited. If the amount of data in the bucket is too large, it can lead to poor read performance. So in order to read performance, it is recommended to analyze the query requirements table and set the data volume in the bucket to be between 200MB and 1GB. But if the bucket is too small, there will be a lot of small file reads and writes, causing pressure on the file system.
In addition, due to the merging process, Filter based data skipping cannot be performed on non primary key columns, otherwise new data will be filtered out, resulting in incorrect old data.
Write performance: very good. Read performance: not so good. Copy On Write#ALTER TABLE orders SET (&#39;full-compaction.delta-commits&#39; = &#39;1&#39;); Set full-compaction.delta-commits to 1, which means that every write will be fully merged, and all data will be merged to the highest level. When reading, merging is not necessary at this time, and the reading performance is the highest. But every write requires full merging, and write amplification is very severe.
Write performance: very bad. Read performance: very good. Merge On Write#ALTER TABLE orders SET (&#39;deletion-vectors.enabled&#39; = &#39;true&#39;); Thanks to Paimon&rsquo;s LSM structure, it has the ability to be queried by primary key. We can generate deletion vectors files when writing, representing which data in the file has been deleted. This directly filters out unnecessary rows during reading, which is equivalent to merging and does not affect reading performance.
A simple example just like:
Updates data by deleting old record first and then adding new one.
Write performance: good. Read performance: good. Visibility guarantee: Tables in deletion vectors mode, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.MOR Read Optimized#If you don&rsquo;t want to use Deletion Vectors mode, you want to query fast enough in MOR mode, but can only find older data, you can also:
Configure &lsquo;compaction.optimization-interval&rsquo; when writing data. Query from read-optimized system table. Reading from results of optimized files avoids merging records with the same key, thus improving reading performance. You can flexibly balance query performance and data latency when reading.
`}),e.add({id:46,href:"/append-table/",title:"Table w/o PK",section:"Apache Paimon",content:``}),e.add({id:47,href:"/maintenance/write-performance/",title:"Write Performance",section:"Maintenance",content:`Write Performance#Paimon&rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:
Flink Configuration ('flink-conf.yaml'/'config.yaml' or SET in SQL): Increase the checkpoint interval ('execution.checkpointing.interval'), increase max concurrent checkpoints to 3 ('execution.checkpointing.max-concurrent-checkpoints'), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode. Option 'changelog-producer' = 'lookup' or 'full-compaction', and option 'full-compaction.delta-commits' have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.
If you find that the input of the job shows a jagged pattern in the case of backpressure, it may be imbalanced work nodes. You can consider turning on Asynchronous Compaction to observe if the throughput is increased.
Parallelism#It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.
OptionRequiredDefaultTypeDescriptionsink.parallelismNo(none)IntegerDefines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.Local Merging#If your job suffers from primary key data skew (for example, you want to count the number of views for each page in a website, and some particular pages are very popular among the users), you can set 'local-merge-buffer-size' so that input records will be buffered and merged before they&rsquo;re shuffled by bucket and written into sink. This is particularly useful when the same primary key is updated frequently between snapshots.
The buffer will be flushed when it is full. We recommend starting with 64 mb when you are faced with data skew but don&rsquo;t know where to start adjusting buffer size.
(Currently, Local merging not works for CDC ingestion)
File Format#If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.
The advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase. This a tradeoff.
Enable row storage through the following options:
file.format = avro metadata.stats-mode = none The collection of statistical information for row storage is a bit expensive, so I suggest turning off statistical information as well.
If you don&rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.
File Compression#By default, Paimon uses zstd with level 1, you can modify the compression algorithm:
'file.compression.zstd-level': Default zstd level is 1. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.
Stability#If there are too few buckets or resources, full-compaction may cause the checkpoint timeout, Flink&rsquo;s default checkpoint timeout is 10 minutes.
If you expect stability even in this case, you can turn up the checkpoint timeout, for example:
execution.checkpointing.timeout = 60 min Write Initialize#In the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.
Write Memory#There are three main places in Paimon writer that takes up memory:
Writer&rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar ORC file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format. If files are automatically compaction in the write task, dictionaries for certain large columns can significantly consume memory during compaction. To disable dictionary encoding for all fields in Parquet format, set 'parquet.enable.dictionary'= 'false'. To disable dictionary encoding for all fields in ORC format, set orc.dictionary.key.threshold='0'. Additionally,set orc.column.encoding.direct='field1,field2' to disable dictionary encoding for specific columns. If your Flink job does not rely on state, please avoid using managed memory, which you can control with the following Flink parameter:
taskmanager.memory.managed.size=1m Or you can use Flink managed memory for your write buffer to avoid OOM, set table property:
sink.use-managed-memory-allocator=true Commit Memory#Committer node may use a large memory if the amount of data written to the table is particularly large, OOM may occur if the memory is too small. In this case, you need to increase the Committer heap memory, but you may not want to increase the memory of Flink&rsquo;s TaskManager uniformly, which may lead to a waste of memory.
You can use fine-grained-resource-management of Flink to increase committer heap memory only:
Configure Flink Configuration cluster.fine-grained-resource-management.enabled: true. (This is default after Flink 1.18) Configure Paimon Table Options: sink.committer-memory, for example 300 MB, depends on your TaskManager. (sink.committer-cpu is also supported) If you use Flink batch job write data into Paimon or run dedicated compaction, Configure Flink Configuration fine-grained.shuffle-mode.all-blocking: true. `}),e.add({id:48,href:"/concepts/concurrency-control/",title:"并发控制 Concurrency Control",section:"概念 Concepts",content:`并发控制#Paimon 支持多写入任务的乐观并发控制。
每个任务以自己的节奏写入数据，并在提交时基于当前快照应用增量文件（删除或添加文件）生成新的快照。
这里可能出现两种提交失败的情况：
快照冲突：快照 ID 已被抢占，表已由其他任务生成了新的快照。此时可以重新提交。 文件冲突：任务想删除的文件已被其他任务删除，此时任务只能失败。（对于流处理任务，会失败并重启，故意进行一次故障切换） 快照冲突#Paimon 的快照 ID 唯一，只要任务成功将快照文件写入文件系统，即认为提交成功。
Paimon 使用文件系统的重命名机制提交快照，这对于 HDFS 来说是安全的，因为它保证了事务性和原子性重命名。
但对于 OSS、S3 等对象存储，其 'RENAME' 操作没有原子语义。此时需要配置 Hive 或 jdbc metastore，并为目录启用 'lock.enabled' 选项，否则可能会丢失快照。
文件冲突#当 Paimon 提交文件删除（仅为逻辑删除）时，会检查与最新快照的冲突。
如果存在冲突（即该文件已被逻辑删除），该提交节点无法继续，只能故意触发故障切换重启，任务会从文件系统重新获取最新状态，期望解决冲突。
Paimon 会确保此过程无数据丢失或重复，但如果两个流任务同时写入且产生冲突，就会不断重启，这并不好。
冲突本质来源于文件的逻辑删除，而文件删除源自压缩操作，因此只要关闭写任务的压缩（将 'write-only' 设为 true），并启动单独的任务做压缩工作，一切都会很好。
更多信息请参见 dedicated compaction job。
`}),e.add({id:49,href:"/concepts/spec/snapshot/",title:"快照文件 Snapshot",section:"规范说明 Specification",content:`快照文件 Snapshot#每次提交都会生成一个 snapshot 文件，snapshot 文件的版本从 1 开始且必须连续。
EARLIEST 和 LATEST 是 snapshot 列表的起始和结束提示文件，但它们可能不准确。
当提示文件不准确时，读取操作会扫描所有 snapshot 文件以确定起始和结束位置。
warehouse └── default.db └── my_table ├── snapshot ├── EARLIEST ├── LATEST ├── snapshot-1 ├── snapshot-2 └── snapshot-3 写入提交时会预占下一个 snapshot ID，一旦 snapshot 文件成功写入，该提交即对外可见。
Snapshot 文件是 JSON 格式，内容包括：
version：Snapshot 文件版本，目前为 3。 id：snapshot ID，与文件名相同。 schemaId：该提交对应的 schema 版本。 baseManifestList：记录从上一个 snapshot 以来所有变更的 manifest 列表。 deltaManifestList：记录本次 snapshot 中发生的所有新增变更的 manifest 列表。 changelogManifestList：记录本次 snapshot 产生的所有变更日志的 manifest 列表，如果无变更日志则为 null。 indexManifest：记录该表所有索引文件的 manifest，如果无表索引文件则为 null。 commitUser：通常由 UUID 生成，用于流式写入的恢复，一个流写作业对应一个用户。 commitIdentifier：流式写入对应的事务 ID，每个事务可能因不同的 commitKind 导致多次提交。 commitKind：本次 snapshot 中变更的类型，包括 append（追加）、compact（压缩）、overwrite（覆盖）和 analyze（分析）。 timeMillis：提交时间（毫秒）。 logOffsets：提交日志偏移量。 totalRecordCount：本次 snapshot 中所有变更的记录数。 deltaRecordCount：本次 snapshot 中新增变更的记录数。 changelogRecordCount：本次 snapshot 中产生的变更日志记录数。 watermark：输入记录的 watermark，来源于 Flink 的 watermark 机制，无 watermark 时为 Long.MIN_VALUE。 statistics：该表统计信息对应的统计文件名。 `}),e.add({id:50,href:"/concepts/catalog/",title:"Catalog",section:"概念 Concepts",content:`Catalog#Paimon 提供了 Catalog 抽象来管理目录和元数据。Catalog 抽象提供了一系列方法，帮助你更好地与计算引擎集成。我们始终建议使用 Catalog 来访问 Paimon 表。
Catalog 类型#Paimon 目前支持四种类型的元存储（metastore）：
filesystem metastore（默认），在文件系统中存储元数据和表文件。 hive metastore，额外将元数据存储在 Hive metastore 中，用户可以直接通过 Hive 访问表。 jdbc metastore，额外将元数据存储在关系型数据库中，如 MySQL、Postgres 等。 rest metastore，设计用于通过单一客户端轻量访问任意 Catalog 后端。 Filesystem Catalog#元数据和表文件存储在 hdfs:///path/to/warehouse 目录下。
-- Flink SQL CREATE CATALOG my_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39; ); Hive Catalog#使用 Paimon Hive Catalog 时，对 Catalog 的变更会直接影响对应的 Hive metastore。在该 Catalog 中创建的表也可以直接通过 Hive 访问。元数据和表文件存储在 hdfs:///path/to/warehouse，同时 schema 也存储在 Hive metastore 中。
-- Flink SQL CREATE CATALOG my_hive WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;hive&#39;, -- &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39;, default use &#39;hive.metastore.warehouse.dir&#39; in HiveConf ); 默认情况下，Paimon 不会将新创建的分区同步到 Hive metastore，用户在 Hive 中会看到未分区的表。分区下推将通过过滤条件的下推来实现。
如果你希望在 Hive 中看到分区表，并且将新创建的分区同步到 Hive metastore，请将表选项 metastore.partitioned-table 设置为 true。
JDBC Catalog#使用 Paimon JDBC Catalog 时，对 Catalog 的变更会直接存储在关系型数据库中，如 SQLite、MySQL、Postgres 等。
-- Flink SQL CREATE CATALOG my_jdbc WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;metastore&#39; = &#39;jdbc&#39;, &#39;uri&#39; = &#39;jdbc:mysql://&lt;host&gt;:&lt;port&gt;/&lt;databaseName&gt;&#39;, &#39;jdbc.user&#39; = &#39;...&#39;, &#39;jdbc.password&#39; = &#39;...&#39;, &#39;catalog-key&#39;=&#39;jdbc&#39;, &#39;warehouse&#39; = &#39;hdfs:///path/to/warehouse&#39; ); REST Catalog#使用 Paimon REST Catalog 时，对 Catalog 的变更会直接存储在通过 REST API 暴露的远程 Catalog 服务器中。
详见 Paimon REST Catalog。
`}),e.add({id:51,href:"/program-api/catalog-api/",title:"Catalog API",section:"Program API",content:`Catalog API#Create Database#You can use the catalog to create databases. The created databases are persistence in the file system.
import org.apache.paimon.catalog.Catalog; public class CreateDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(&#34;my_db&#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something } } } Determine Whether Database Exists#You can use the catalog to determine whether the database exists
import org.apache.paimon.catalog.Catalog; public class DatabaseExists { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.databaseExists(&#34;my_db&#34;); } } List Databases#You can use the catalog to list databases.
import org.apache.paimon.catalog.Catalog; import java.util.List; public class ListDatabases { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List&lt;String&gt; databases = catalog.listDatabases(); } } Drop Database#You can use the catalog to drop database.
import org.apache.paimon.catalog.Catalog; public class DropDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropDatabase(&#34;my_db&#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } } } Alter Database#You can use the catalog to alter database&rsquo;s properties.(ps: only support hive and jdbc catalog)
import java.util.ArrayList; import org.apache.paimon.catalog.Catalog; public class AlterDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createHiveCatalog(); List&lt;DatabaseChange&gt; changes = new ArrayList&lt;&gt;(); changes.add(DatabaseChange.setProperty(&#34;k1&#34;, &#34;v1&#34;)); changes.add(DatabaseChange.removeProperty(&#34;k2&#34;)); catalog.alterDatabase(&#34;my_db&#34;, changes, true); } catch (Catalog.DatabaseNotExistException e) { // do something } } } Determine Whether Table Exists#You can use the catalog to determine whether the table exists
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.tableExists(identifier); } } List Tables#You can use the catalog to list tables.
import org.apache.paimon.catalog.Catalog; import java.util.List; public class ListTables { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List&lt;String&gt; tables = catalog.listTables(&#34;my_db&#34;); } catch (Catalog.DatabaseNotExistException e) { // do something } } } Drop Table#You can use the catalog to drop table.
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something } } } Rename Table#You can use the catalog to rename a table.
import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); Identifier toTableIdentifier = Identifier.create(&#34;my_db&#34;, &#34;test_table&#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.TableNotExistException e) { // do something } } } Alter Table#You can use the catalog to alter a table, but you need to pay attention to the following points.
Column %s cannot specify NOT NULL in the %s table. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported. import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(&#34;my_db&#34;, &#34;my_table&#34;); Map&lt;String, String&gt; options = new HashMap&lt;&gt;(); options.put(&#34;bucket&#34;, &#34;4&#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(&#34;my_db&#34;, false); try { catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, &#34;col1&#34;, DataTypes.STRING(), &#34;field1&#34;), new DataField(1, &#34;col2&#34;, DataTypes.STRING(), &#34;field2&#34;), new DataField(2, &#34;col3&#34;, DataTypes.STRING(), &#34;field3&#34;), new DataField(3, &#34;col4&#34;, DataTypes.BIGINT(), &#34;field4&#34;), new DataField( 4, &#34;col5&#34;, DataTypes.ROW( new DataField( 5, &#34;f1&#34;, DataTypes.STRING(), &#34;f1&#34;), new DataField( 6, &#34;f2&#34;, DataTypes.STRING(), &#34;f2&#34;), new DataField( 7, &#34;f3&#34;, DataTypes.STRING(), &#34;f3&#34;)), &#34;field5&#34;), new DataField(8, &#34;col6&#34;, DataTypes.STRING(), &#34;field6&#34;)), Lists.newArrayList(&#34;col1&#34;), // partition keys Lists.newArrayList(&#34;col1&#34;, &#34;col2&#34;), // primary key options, &#34;table comment&#34;), false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } // add option SchemaChange addOption = SchemaChange.setOption(&#34;snapshot.time-retained&#34;, &#34;2h&#34;); // add column SchemaChange addColumn = SchemaChange.addColumn(&#34;col1_after&#34;, DataTypes.STRING()); // add a column after col1 SchemaChange.Move after = SchemaChange.Move.after(&#34;col1_after&#34;, &#34;col1&#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(&#34;col7&#34;, DataTypes.STRING(), &#34;&#34;, after); // rename column SchemaChange renameColumn = SchemaChange.renameColumn(&#34;col3&#34;, &#34;col3_new_name&#34;); // drop column SchemaChange dropColumn = SchemaChange.dropColumn(&#34;col6&#34;); // update column comment SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[]{&#34;col4&#34;}, &#34;col4 field&#34;); // update nested column comment SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[]{&#34;col5&#34;, &#34;f1&#34;}, &#34;col5 f1 field&#34;); // update column type SchemaChange updateColumnType = SchemaChange.updateColumnType(&#34;col4&#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(&#34;col4&#34;)); // update column nullability SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[]{&#34;col4&#34;}, false); // update nested column nullability SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[]{&#34;col5&#34;, &#34;f2&#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[]{ addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability }; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something } catch (Catalog.ColumnAlreadyExistException e) { // do something } catch (Catalog.ColumnNotExistException e) { // do something } } } `}),e.add({id:52,href:"/project/committer/",title:"Committer",section:"Project",content:`Committer#Become a Committer#How to become a committer#There is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.
If you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.
Community contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The &ldquo;Apache Way&rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.
Code/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.
Identify promising candidates#While the prior points give ways to identify promising candidates, the following are &ldquo;must haves&rdquo; for any committer candidate:
Being community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.
We trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don&rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.
They have shown to be respectful towards other community members and constructive in discussions.
Committer Rights#JetBrains provides a free license to Apache Committers, allowing them to access all JetBrains IDEs, such as IntelliJ IDEA, PyCharm, and other desktop tools.
Please use your @apache.org email address to All Products Packs for Apache committers.
`}),e.add({id:53,href:"/maintenance/dedicated-compaction/",title:"Dedicated Compaction",section:"Maintenance",content:`Dedicated Compaction#Paimon&rsquo;s snapshot management supports writing with multiple writers.
For S3-like object store, its 'RENAME' does not have atomic semantic. We need to configure Hive metastore and enable 'lock.enabled' option for the catalog.By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon&rsquo;s latest partition, Simultaneously batch job (overwrite) writes records to the historical partition.
So far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated. For example, you don&rsquo;t want to use UNION ALL, you have multiple streaming jobs to write records to a 'partial-update' table. Please refer to the 'Dedicated Compaction Job' below.
Dedicated Compaction Job#By default, Paimon writers will perform compaction as needed during writing records. This is sufficient for most use cases.
Compaction will mark some data files as &ldquo;deleted&rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file, a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.
To avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.
To skip compactions in writers, set the following table property to true.
OptionRequiredDefaultTypeDescriptionwrite-onlyNofalseBooleanIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.To run a dedicated job for compaction, follow these instructions.
Flink SQLRun the following sql:
CALL sys.compact( \`table\` =&gt; &#39;default.T&#39;, partitions =&gt; &#39;p=0&#39;, options =&gt; &#39;sink.parallelism=4&#39;, \`where\` =&gt; &#39;dt&gt;10 and h&lt;20&#39; ); Flink Action JarRun the following command to submit a compaction job for the table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition &lt;partition-name&gt;] \\ [--compact_strategy &lt;minor / full&gt;] \\ [--table_conf &lt;table_conf&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Example: compact table
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --warehouse s3:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition dt=20221126,hh=08 \\ --partition dt=20221127,hh=09 \\ --table_conf sink.parallelism=10 \\ --compact_strategy minor \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** --compact_strategy Determines how to pick files to be merged, the default is determined by the runtime execution mode, streaming-mode use minor strategy and batch-mode use full strategy. full : Only supports batch mode. All files will be selected for merging. minor : Pick the set of files that need to be merged based on specified conditions. You can use -D execution.runtime-mode=batch or -yD execution.runtime-mode=batch (for the ON-YARN scenario) to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.
For more usage of the compact action, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact --help Similarly, the default is synchronous compaction, which may cause checkpoint timeouts. You can configure table_conf to use Asynchronous Compaction.Database Compaction Job#You can run the following command to submit a compaction job for multiple database.
Flink SQLRun the following sql:
CALL sys.compact_database( including_databases =&gt; &#39;includingDatabases&#39;, mode =&gt; &#39;mode&#39;, including_tables =&gt; &#39;includingTables&#39;, excluding_tables =&gt; &#39;excludingTables&#39;, table_options =&gt; &#39;tableOptions&#39; ) -- example CALL sys.compact_database( including_databases =&gt; &#39;db1|db2&#39;, mode =&gt; &#39;combined&#39;, including_tables =&gt; &#39;table_.*&#39;, excluding_tables =&gt; &#39;ignore&#39;, table_options =&gt; &#39;sink.parallelism=4&#39; ) Flink Action Jar&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database \\ --warehouse &lt;warehouse-path&gt; \\ --including_databases &lt;database-name|name-regular-expr&gt; \\ [--including_tables &lt;paimon-table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;paimon-table-name|name-regular-expr&gt;] \\ [--mode &lt;compact-mode&gt;] \\ [--compact_strategy &lt;minor / full&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table_conf&gt; [--table_conf &lt;paimon-table_conf&gt; ...]] --including_databases is used to specify which database is to be compacted. In compact mode, you need to specify a database name, in compact_database mode, you could specify multiple database, regular expression is supported. --including_tables is used to specify which source tables are to be compacted, you must use &lsquo;|&rsquo; to separate multiple tables, the format is databaseName.tableName, regular expression is supported. For example, specifying &ldquo;&ndash;including_tables db1.t1|db2.+&rdquo; means to compact table &lsquo;db1.t1&rsquo; and all tables in the db2 database. --excluding_tables is used to specify which source tables are not to be compacted. The usage is same as &ldquo;&ndash;including_tables&rdquo;. &ldquo;&ndash;excluding_tables&rdquo; has higher priority than &ldquo;&ndash;including_tables&rdquo; if you specified both. --mode is used to specify compaction mode. Possible values: &ldquo;divided&rdquo; (the default mode if you haven&rsquo;t specified one): start a sink for each table, the compaction of the new table requires restarting the job. &ldquo;combined&rdquo;: start a single combined sink for all tables, the new table will be automatically compacted. --catalog_conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations. --table_conf is the configuration for compaction. Each configuration should be specified in the format key=value. Pivotal configuration is listed below: Key Default Type Description continuous.discovery-interval 10 s Duration The discovery interval of continuous reading. sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. You can use -D execution.runtime-mode=batch to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.
If you only want to submit the compaction job and don&rsquo;t want to wait until the job is done, you should submit in detached mode.You can set --mode combined to enable compacting newly added tables without restarting job.Example1: compact database
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** Example2: compact database in combined mode
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --mode combined \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** \\ --table_conf continuous.discovery-interval=***** For more usage of the compact_database action, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database --help Sort Compact#If your table is configured with dynamic bucket primary key table or append table , you can trigger a compact with specified column sort to speed up queries.
Flink SQLRun the following sql:
-- sort compact table CALL sys.compact(\`table\` =&gt; &#39;default.T&#39;, order_strategy =&gt; &#39;zorder&#39;, order_by =&gt; &#39;a,b&#39;) Flink Action Jar&lt;FLINK_HOME&gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --order_strategy &lt;orderType&gt; \\ --order_by &lt;col1,col2,...&gt; \\ [--partition &lt;partition-name&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-dynamic-conf&gt; [--table_conf &lt;paimon-table-dynamic-conf&gt;] ...] There are two new configuration in Sort Compact
ConfigurationDescription--order_strategythe order strategy now support "zorder" and "hilbert" and "order". For example: --order_strategy zorder--order_bySpecify the order columns. For example: --order_by col0, col1The sort parallelism is the same as the sink parallelism, you can dynamically specify it by add conf --table_conf sink.parallelism=&lt;value&gt;.
Sort Compact currently supports only bucket=-1 and batch mode.Historical Partition Compact#You can run the following command to submit a compaction job for partition which has not received any new data for a period of time. Small files in those partitions will be full compacted.
This feature now is only used in batch mode.For Table#This is for one table. Flink SQLRun the following sql:
-- history partition compact table CALL sys.compact(\`table\` =&gt; &#39;default.T&#39;, partition_idle_time =&gt; &#39;1 d&#39;) Flink Action Jar&lt;FLINK_HOME&gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --partition_idle_time &lt;partition-idle-time&gt; \\ [--partition &lt;partition-name&gt;] \\ [--compact_strategy &lt;minor / full&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-dynamic-conf&gt; [--table_conf &lt;paimon-table-dynamic-conf&gt;] ...] There are one new configuration in Historical Partition Compact
--partition_idle_time: this is used to do a full compaction for partition which had not received any new data for &lsquo;partition_idle_time&rsquo;. And only these partitions will be compacted. For Databases#This is for multiple tables in different databases. Flink SQLRun the following sql:
-- history partition compact table CALL sys.compact_database( including_databases =&gt; &#39;includingDatabases&#39;, mode =&gt; &#39;mode&#39;, including_tables =&gt; &#39;includingTables&#39;, excluding_tables =&gt; &#39;excludingTables&#39;, table_options =&gt; &#39;tableOptions&#39;, partition_idle_time =&gt; &#39;partition_idle_time&#39; ); Example: compact historical partitions for tables in database
-- history partition compact table CALL sys.compact_database( includingDatabases =&gt; &#39;test_db&#39;, mode =&gt; &#39;combined&#39;, partition_idle_time =&gt; &#39;1 d&#39; ); Flink Action Jar&lt;FLINK_HOME&gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database \\ --warehouse &lt;warehouse-path&gt; \\ --including_databases &lt;database-name|name-regular-expr&gt; \\ --partition_idle_time &lt;partition-idle-time&gt; \\ [--including_tables &lt;paimon-table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;paimon-table-name|name-regular-expr&gt;] \\ [--mode &lt;compact-mode&gt;] \\ [--compact_strategy &lt;minor / full&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table_conf&gt; [--table_conf &lt;paimon-table_conf&gt; ...]] Example: compact historical partitions for tables in database
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --partition_idle_time 1d \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** `}),e.add({id:54,href:"/primary-key-table/merge-engine/first-row/",title:"First Row",section:"Merge Engine",content:`First Row#By specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.
first-row merge engine only supports none and lookup changelog producer. For streaming queries must be used with the lookup changelog producer.You can not specify sequence.field. Not accept DELETE and UPDATE_BEFORE message. You can config ignore-delete to ignore these two kinds records. Visibility guarantee: Tables with First Row engine, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data. This is of great help in replacing log deduplication in streaming computation.
`}),e.add({id:55,href:"/ecosystem/hive/",title:"Hive",section:"Ecosystem",content:`Hive#This documentation is a guide for using Paimon in Hive.
Version#Paimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.
Execution Engine#Paimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.
Installation#Download the jar file with corresponding version.
Jar Hive 3.1 paimon-hive-connector-3.1-1.2.0.jar Hive 2.3 paimon-hive-connector-2.3-1.2.0.jar Hive 2.2 paimon-hive-connector-2.2-1.2.0.jar Hive 2.1 paimon-hive-connector-2.1-1.2.0.jar Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.2.0.jar You can also manually build bundled jar from the source code.
To build from source code, clone the git repository.
Build bundled jar with the following command. mvn clean install -DskipTests
You can find Hive connector jar in ./paimon-hive/paimon-hive-connector-&lt;hive-version&gt;/target/paimon-hive-connector-&lt;hive-version&gt;-1.2.0.jar.
There are several ways to add this jar to Hive.
You can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-1.2.0.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-1.2.0.jar to enable paimon support in Hive. Note that this method is not recommended. If you&rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class. NOTE:
If you are using HDFS : Make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set. You can set paimon.hadoop-load-default-config =false to disable loading the default value from core-default.xml、hdfs-default.xml, which may lead smaller size for split. With hive cbo, it may lead to some incorrect query results, such as to query struct type with not null predicate, you can disable the cbo by set hive.cbo.enable=false; command. Hive SQL: access Paimon Tables already in Hive metastore#Run the following Hive SQL in Hive CLI to access the created table.
-- Assume that paimon-hive-connector-&lt;hive-version&gt;-1.2.0.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to &#34;default&#34; database if you&#39;re not there by default) SHOW TABLES; /* OK test_table */ -- Read records from test_table SELECT a, b FROM test_table ORDER BY a; /* OK 1	Table 2	Store */ -- Insert records into test table -- Limitations: -- Only support INSERT INTO, not support INSERT OVERWRITE -- It is recommended to write to a non primary key table -- Writing to a primary key table may result in a large number of small files INSERT INTO test_table VALUES (3, &#39;Paimon&#39;); SELECT a, b FROM test_table ORDER BY a; /* OK 1	Table 2	Store 3	Paimon */ -- time travel SET paimon.scan.snapshot-id=1; SELECT a, b FROM test_table ORDER BY a; /* OK 1	Table 2	Store 3	Paimon */ SET paimon.scan.snapshot-id=null; Hive SQL: create new Paimon Tables#You can create new paimon tables in Hive. Run the following Hive SQL in Hive CLI.
-- Assume that paimon-hive-connector-1.2.0.jar is already in auxlib directory. -- Let&#39;s create a new paimon table. SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE hive_test_table( a INT COMMENT &#39;The a field&#39;, b STRING COMMENT &#39;The b field&#39; ) STORED BY &#39;org.apache.paimon.hive.PaimonStorageHandler&#39;; Hive SQL: access Paimon Tables by External Table#To access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.
-- Assume that paimon-hive-connector-1.2.0.jar is already in auxlib directory. -- Let&#39;s use the test_table created in the above section. -- To create an external table, you don&#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough. CREATE EXTERNAL TABLE external_test_table STORED BY &#39;org.apache.paimon.hive.PaimonStorageHandler&#39; LOCATION &#39;/path/to/table/store/warehouse/default.db/test_table&#39;; -- In addition to the way setting location above, you can also place the location setting in TBProperties -- to avoid Hive accessing Paimon&#39;s location through its own file system when creating tables. -- This method is effective in scenarios using Object storage,such as s3. CREATE EXTERNAL TABLE external_test_table STORED BY &#39;org.apache.paimon.hive.PaimonStorageHandler&#39; TBLPROPERTIES ( &#39;paimon_location&#39; =&#39;s3://xxxxx/path/to/table/store/warehouse/default.db/test_table&#39; ); -- Read records from external_test_table SELECT a, b FROM external_test_table ORDER BY a; /* OK 1	Table 2	Store */ -- Insert records into test table INSERT INTO external_test_table VALUES (3, &#39;Paimon&#39;); SELECT a, b FROM external_test_table ORDER BY a; /* OK 1	Table 2	Store 3	Paimon */ Hive Type Conversion#This section lists all supported type conversion between Hive and Paimon. All Hive&rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.
Hive Data TypePaimon Data TypeAtomic TypeStructTypeInfoRowTypefalseMapTypeInfoMapTypefalseListTypeInfoArrayTypefalsePrimitiveTypeInfo("boolean")BooleanTypetruePrimitiveTypeInfo("tinyint")TinyIntTypetruePrimitiveTypeInfo("smallint")SmallIntTypetruePrimitiveTypeInfo("int")IntTypetruePrimitiveTypeInfo("bigint")BigIntTypetruePrimitiveTypeInfo("float")FloatTypetruePrimitiveTypeInfo("double")DoubleTypetrueCharTypeInfo(length)CharType(length)truePrimitiveTypeInfo("string")VarCharType(VarCharType.MAX_LENGTH)trueVarcharTypeInfo(length)VarCharType(length), length is less than VarCharType.MAX_LENGTHtruePrimitiveTypeInfo("date")DateTypetruePrimitiveTypeInfo("timestamp")TimestampTypetrueDecimalTypeInfo(precision, scale)DecimalType(precision, scale)truePrimitiveTypeInfo("binary")VarBinaryType, BinaryTypetrue`}),e.add({id:56,href:"/iceberg/iceberg-tags/",title:"Iceberg Tags",section:"Iceberg Metadata",content:`Iceberg Tags#When enable iceberg compatibility, Paimon Tags will also be synced to Iceberg Tags.
CREATE CATALOG paimon WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;&#39; ); CREATE CATALOG iceberg WITH ( &#39;type&#39; = &#39;iceberg&#39;, &#39;catalog-type&#39; = &#39;hadoop&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;/iceberg&#39;, &#39;cache-enabled&#39; = &#39;false&#39; -- disable iceberg catalog caching to quickly see the result ); -- create tag for paimon table CALL paimon.sys.create_tag(&#39;default.T&#39;, &#39;tag1&#39;, 1); -- query tag in iceberg table SELECT * FROM iceberg.\`default\`.T /*+ OPTIONS(&#39;tag&#39;=&#39;tag1&#39;) */; `}),e.add({id:57,href:"/primary-key-table/merge-engine/",title:"Merge Engine",section:"Table with PK",content:``}),e.add({id:58,href:"/cdc-ingestion/mongo-cdc/",title:"Mongo CDC",section:"CDC Ingestion",content:`Mongo CDC#Prepare MongoDB Bundled Jar#flink-sql-connector-mongodb-cdc-*.jar only cdc 3.1+ is supported
Synchronizing Tables#By using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_table \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition_keys &lt;partition_keys&gt;] \\ [--computed_column &lt;&#39;column-name=expr-name(args[, ...])&#39;&gt; [--computed_column ...]] \\ [--mongodb_conf &lt;mongodb-cdc-source-conf&gt; [--mongodb_conf &lt;mongodb-cdc-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--tableThe Paimon table name.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".--computed_columnThe definitions of computed columns. The argument field is from MongoDB collection field name. See here for a complete list of configurations. --mongodb_confThe configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format "key=value". hosts, username, password, database and collection are required configurations, others are optional. See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.Here are a few points to take note of:
The mongodb_conf introduces the schema.start.mode parameter on top of the MongoDB CDC source configuration.schema.start.mode provides two modes: dynamic (default) and specified. In dynamic mode, MongoDB schema information is parsed at one level, which forms the basis for schema change evolution. In specified mode, synchronization takes place according to specified criteria. This can be done by configuring field.name to specify the synchronization fields and parser.path to specify the JSON parsing path for those fields. The difference between the two is that the specify mode requires the user to explicitly identify the fields to be used and create a mapping table based on those fields. Dynamic mode, on the other hand, ensures that Paimon and MongoDB always keep the top-level fields consistent, eliminating the need to focus on specific fields. Further processing of the data table is required when using values from nested fields. The mongodb_conf introduces the default.id.generation parameter as an enhancement to the MongoDB CDC source configuration. The default.id.generation setting offers two distinct behaviors: when set to true and when set to false. When default.id.generation is set to true, the MongoDB CDC source adheres to the default _id generation strategy, which involves stripping the outer $oid nesting to provide a more straightforward identifier. This mode simplifies the _id representation, making it more direct and user-friendly. On the contrary, when default.id.generation is set to false, the MongoDB CDC source retains the original _id structure, without any additional processing. This mode offers users the flexibility to work with the raw _id format as provided by MongoDB, preserving any nested elements like $oid. The choice between the two hinges on the user&rsquo;s preference: the former for a cleaner, simplified _id and the latter for a direct representation of MongoDB&rsquo;s _id structure. OperatorDescription$The root element to query. This starts all path expressions.@The current node being processed by a filter predicate.*Wildcard. Available anywhere a name or numeric are required...Deep scan. Available anywhere a name is required..Dot-notated child.['{name}' (, '{name}')]Bracket-notated child or children.[{number} (, {number})]Bracket-notated child or children.[start:end]Array index or indexes.[?({expression})]Filter expression. Expression must evaluate to a boolean value.Functions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.
FunctionDescriptionOutput typemin()Provides the min value of an array of numbers.Doublemax()Provides the max value of an array of numbers.Doubleavg()Provides the average value of an array of numbers.Doublestddev()Provides the standard deviation value of an array of numbersDoublelength()Provides the length of an arrayIntegersum()Provides the sum value of an array of numbers.Doublekeys()Provides the property keys (An alternative for terminal tilde ~)Setconcat(X)Provides a concatinated version of the path output with a new item.like inputappend(X)add an item to the json path output arraylike inputappend(X)add an item to the json path output arraylike inputfirst()Provides the first item of an arrayDepends on the arraylast()Provides the last item of an arrayDepends on the arrayindex(X)Provides the item of an array of index: X, if the X is negative, take from backwardsDepends on the arrayPath Examples
{ &#34;store&#34;: { &#34;book&#34;: [ { &#34;category&#34;: &#34;reference&#34;, &#34;author&#34;: &#34;Nigel Rees&#34;, &#34;title&#34;: &#34;Sayings of the Century&#34;, &#34;price&#34;: 8.95 }, { &#34;category&#34;: &#34;fiction&#34;, &#34;author&#34;: &#34;Evelyn Waugh&#34;, &#34;title&#34;: &#34;Sword of Honour&#34;, &#34;price&#34;: 12.99 }, { &#34;category&#34;: &#34;fiction&#34;, &#34;author&#34;: &#34;Herman Melville&#34;, &#34;title&#34;: &#34;Moby Dick&#34;, &#34;isbn&#34;: &#34;0-553-21311-3&#34;, &#34;price&#34;: 8.99 }, { &#34;category&#34;: &#34;fiction&#34;, &#34;author&#34;: &#34;J. R. R. Tolkien&#34;, &#34;title&#34;: &#34;The Lord of the Rings&#34;, &#34;isbn&#34;: &#34;0-395-19395-8&#34;, &#34;price&#34;: 22.99 } ], &#34;bicycle&#34;: { &#34;color&#34;: &#34;red&#34;, &#34;price&#34;: 19.95 } }, &#34;expensive&#34;: 10 } JsonPathResult$.store.book[*].authorProvides the min value of an array of numbers.$..authorAll authors.$.store.*All things, both books and bicycles.$.store..priceProvides the standard deviation value of an array of numbers.$..book[2]The third book.$..book[-2]The second to last book.$..book[0,1]The first two books.$..book[:2]All books from index 0 (inclusive) until index 2 (exclusive).$..book[1:2]All books from index 1 (inclusive) until index 2 (exclusive)$..book[-2:]Last two books$..book[2:]All books from index 2 (inclusive) to last$..book[?(@.isbn)]All books with an ISBN number$.store.book[?(@.price &lt; 10)]All books in store cheaper than 10$..book[?(@.price &lt;= $['expensive'])]All books in store that are not "expensive"$..book[?(@.author =~ /.*REES/i)]All books matching regex (ignore case)$..*Give me every thing$..book.length()The number of booksThe synchronized table is required to have its primary key set as _id. This is because MongoDB&rsquo;s change events are recorded before updates in messages. Consequently, we can only convert them into Flink&rsquo;s UPSERT change log stream. The upstart stream demands a unique key, which is why we must declare _id as the primary key. Declaring other columns as primary keys is not feasible, as delete operations only encompass the _id and sharding key, excluding other keys and values.
MongoDB Change Streams are designed to return simple JSON documents without any data type definitions. This is because MongoDB is a document-oriented database, and one of its core features is the dynamic schema, where documents can contain different fields, and the data types of fields can be flexible. Therefore, the absence of data type definitions in Change Streams is to maintain this flexibility and extensibility. For this reason, we have set all field data types for synchronizing MongoDB to Paimon as String to address the issue of not being able to obtain data types.
If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from MongoDB collection.
Example 1: synchronize collection into one Paimon table
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column &#39;_year=year(age)&#39; \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --mongodb_conf collection=source_table1 \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: Synchronize collection into a Paimon table according to the specified field mapping.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --mongodb_conf collection=source_table1 \\ --mongodb_conf schema.start.mode=specified \\ --mongodb_conf field.name=_id,name,description \\ --mongodb_conf parser.path=$._id,$.name,$.description \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronizing Databases#By using MongoDBSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MongoDB database into one Paimon database.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_database \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ [--table_prefix &lt;paimon-table-prefix&gt;] \\ [--table_suffix &lt;paimon-table-suffix&gt;] \\ [--including_tables &lt;mongodb-table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;mongodb-table-name|name-regular-expr&gt;] \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--mongodb_conf &lt;mongodb-cdc-source-conf&gt; [--mongodb_conf &lt;mongodb-cdc-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--table_prefixThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have "ods_" as prefix, you can specify "--table_prefix ods_".--table_suffixThe suffix of all Paimon tables to be synchronized. The usage is same as "--table_prefix".--including_tablesIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying "--including_tables test|paimon.*" means to synchronize table 'test' and all tables start with 'paimon'.--excluding_tablesIt is used to specify which source tables are not to be synchronized. The usage is same as "--including_tables". "--excluding_tables" has higher priority than "--including_tables" if you specified both.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".If the keys are not in source table, the sink table won't set partition keys.--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.Otherwise, the sink table won't set primary keys.--mongodb_confThe configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format "key=value". hosts, username, password, database are required configurations, others are optional. See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.All collections to be synchronized need to set _id as the primary key. For each MongoDB collection to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MongoDB collection. If the Paimon table already exists, its schema will be compared against the schema of all specified MongoDB collection. Any MongoDB tables created after the commencement of the task will automatically be included.
Example 1: synchronize entire database
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: Synchronize the specified table.
&lt;FLINK_HOME&gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-1.2.0.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables &#39;product|user|address|order|custom&#39; `}),e.add({id:59,href:"/flink/sql-query/",title:"SQL Query",section:"Engine Flink",content:`SQL Query#Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query#Paimon&rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
-- Flink SQL SET &#39;execution.runtime-mode&#39; = &#39;batch&#39;; Batch Time Travel#Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.
Flink (dynamic option)-- read the snapshot with id 1L SELECT * FROM t /*+ OPTIONS(&#39;scan.snapshot-id&#39; = &#39;1&#39;) */; -- read the snapshot from specified timestamp in unix milliseconds SELECT * FROM t /*+ OPTIONS(&#39;scan.timestamp-millis&#39; = &#39;1678883047356&#39;) */; -- read the snapshot from specified timestamp string ,it will be automatically converted to timestamp in unix milliseconds -- Supported formats include：yyyy-MM-dd, yyyy-MM-dd HH:mm:ss, yyyy-MM-dd HH:mm:ss.SSS, use default local time zone SELECT * FROM t /*+ OPTIONS(&#39;scan.timestamp&#39; = &#39;2023-12-09 23:09:12&#39;) */; -- read tag &#39;my-tag&#39; SELECT * FROM t /*+ OPTIONS(&#39;scan.tag-name&#39; = &#39;my-tag&#39;) */; -- read the snapshot from watermark, will match the first snapshot after the watermark SELECT * FROM t /*+ OPTIONS(&#39;scan.watermark&#39; = &#39;1678883047356&#39;) */; Flink 1.18&#43;Flink SQL supports time travel syntax after 1.18.
-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP &#39;2023-01-01 00:00:00&#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP &#39;2023-01-01 00:00:00&#39; + INTERVAL &#39;1&#39; DAY Batch Incremental#Read incremental changes between start snapshot (exclusive) and end snapshot.
For example:
&lsquo;5,10&rsquo; means changes between snapshot 5 and snapshot 10. &lsquo;TAG1,TAG3&rsquo; means changes between TAG1 and TAG3. -- incremental between snapshot ids SELECT * FROM t /*+ OPTIONS(&#39;incremental-between&#39; = &#39;12,20&#39;) */; -- incremental between snapshot time mills SELECT * FROM t /*+ OPTIONS(&#39;incremental-between-timestamp&#39; = &#39;1692169000000,1692169900000&#39;) */; SELECT * FROM t /*+ OPTIONS(&#39;incremental-between-timestamp&#39; = &#39;2025-03-12 00:00:00,2025-03-12 00:08:00&#39;) */; By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.
In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can use audit_log table:
SELECT * FROM t$audit_log /*+ OPTIONS(&#39;incremental-between&#39; = &#39;12,20&#39;) */; Batch Incremental between Auto-created Tags#You can use incremental-between to query incremental changes between two tags. But for auto-created tag, the tag may not be created in-time because of data delay.
For example, assume that tags &lsquo;2024-12-01&rsquo;, &lsquo;2024-12-02&rsquo; and &lsquo;2024-12-04&rsquo; are auto created daily. Data for 12/03 are delayed and ingested with data for 12/04. Now if you want to query the incremental changes between tags, and you don&rsquo;t know the tag of 12/03 is not created, you will use incremental-between with &lsquo;2024-12-01,2024-12-02&rsquo;, &lsquo;2024-12-02,2024-12-03&rsquo; and &lsquo;2024-12-03,2024-12-04&rsquo; respectively, then you will get an error that the tag &lsquo;2024-12-03&rsquo; doesn&rsquo;t exist.
We introduced a new option incremental-to-auto-tag for this scenario. You can only specify the end tag, and Paimon will find an earlier tag and return changes between them. If the tag doesn&rsquo;t exist or the earlier tag doesn&rsquo;t exist, return empty.
For example, when you query &lsquo;incremental-to-auto-tag=2024-12-01&rsquo; or &lsquo;incremental-to-auto-tag=2024-12-03&rsquo;, the result is empty; Query &lsquo;incremental-to-auto-tag=2024-12-02&rsquo;, the result is change between 12/01 and 12/02; Query &lsquo;incremental-to-auto-tag=2024-12-04&rsquo;, the result is change between 12/02 and 12/04.
Streaming Query#By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest changes.
Paimon by default ensures that your startup is properly processed with all data included.
Paimon Source in Streaming mode is unbounded, like a queue that never ends.-- Flink SQL SET &#39;execution.runtime-mode&#39; = &#39;streaming&#39;; You can also do streaming read without the snapshot data, you can use latest scan mode:
-- Continuously reads latest changes without producing a snapshot at the beginning. SELECT * FROM t /*+ OPTIONS(&#39;scan.mode&#39; = &#39;latest&#39;) */; Streaming Time Travel#If you only want to process data for today and beyond, you can do so with partitioned filters:
SELECT * FROM t WHERE dt &gt; &#39;2023-06-26&#39;; If it&rsquo;s not a partitioned table, or you can&rsquo;t filter by partition, you can use Time travel&rsquo;s stream read.
Flink (dynamic option)-- read changes from snapshot id 1L SELECT * FROM t /*+ OPTIONS(&#39;scan.snapshot-id&#39; = &#39;1&#39;) */; -- read changes from snapshot specified timestamp SELECT * FROM t /*+ OPTIONS(&#39;scan.timestamp-millis&#39; = &#39;1678883047356&#39;) */; -- read snapshot id 1L upon first startup, and continue to read the changes SELECT * FROM t /*+ OPTIONS(&#39;scan.mode&#39;=&#39;from-snapshot-full&#39;,&#39;scan.snapshot-id&#39; = &#39;1&#39;) */; Flink 1.18&#43;Flink SQL supports time travel syntax after 1.18.
-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP &#39;2023-01-01 00:00:00&#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP &#39;2023-01-01 00:00:00&#39; + INTERVAL &#39;1&#39; DAY Time travel&rsquo;s stream read rely on snapshots, but by default, snapshot only retains data within 1 hour, which can prevent you from reading older incremental data. So, Paimon also provides another mode for streaming reads, scan.file-creation-time-millis, which provides a rough filtering to retain files generated after timeMillis.
SELECT * FROM t /*+ OPTIONS(&#39;scan.file-creation-time-millis&#39; = &#39;1678883047356&#39;) */; Read Overwrite#Streaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.
Read Parallelism#By default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets, but not greater than scan.infer-parallelism.max.
Disable scan.infer-parallelism, global parallelism will be used for reads.
You can also manually specify the parallelism from scan.parallelism.
KeyDefaultTypeDescriptionscan.infer-parallelismtrueBooleanIf it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).scan.infer-parallelism.max1024IntegerIf scan.infer-parallelism is true, limit the parallelism of source through this option.scan.parallelism(none)IntegerDefine a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.Query Optimization#BatchStreamingIt is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.
The filter functions that can accelerate data skipping are:
= &lt; &lt;= &gt; &gt;= IN (...) LIKE 'abc%' IS NULL Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.
Suppose that a table has the following specification:
CREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., PRIMARY KEY (catalog_id, order_id) NOT ENFORCED -- composite primary key ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.
SELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id&gt;2035 AND order_id&lt;6000; However, the following filter cannot accelerate the query well.
SELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; Dedicated Split Generation#When Paimon table snapshots contain large amount of source splits, Flink jobs reading from this table might endure long initialization time or even OOM in JobManagers. In this case, you can configure 'scan.dedicated-split-generation' = 'true' to avoid such problem. This option would enable executing the source split generation process in a dedicated subtask that runs on TaskManager, instead of in the source coordinator on the JobManager.
Note that this feature could have some side effects on your Flink jobs. For example:
It will change the DAG of the flink job, thus breaking checkpoint compatibility if enabled on an existing job. It may lead to the Flink AdaptiveBatchScheduler inferring a small parallelism for the source reader operator. you can configure scan.infer-parallelism to avoid this possible drawback. The failover strategy of the Flink job would be forced into global failover instead of regional failover, given that the dedicated source split generation task would be connected to all downstream subtasks. So please make sure these side effects are acceptable to you before enabling it.
`}),e.add({id:60,href:"/spark/sql-query/",title:"SQL Query",section:"Engine Spark",content:`SQL Query#Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query#Paimon&rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
-- read all columns SELECT * FROM t; Paimon also supports reading some hidden metadata columns, currently supporting the following columns:
__paimon_file_path: the file path of the record. __paimon_partition: the partition of the record. __paimon_bucket: the bucket of the record. __paimon_row_index: the row index of the record. -- read all columns and the corresponding file path, partition, bucket, rowIndex of the record SELECT *, __paimon_file_path, __paimon_partition, __paimon_bucket, __paimon_row_index FROM t; Note: only append table or deletion vector table support querying metadata columns.Batch Time Travel#Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.
Requires Spark 3.3+.
you can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:
-- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- read the snapshot from specified timestamp SELECT * FROM t TIMESTAMP AS OF &#39;2023-06-01 00:00:00.123&#39;; -- read the snapshot from specified timestamp in unix seconds SELECT * FROM t TIMESTAMP AS OF 1678883047; -- read tag &#39;my-tag&#39; SELECT * FROM t VERSION AS OF &#39;my-tag&#39;; -- read the snapshot from specified watermark. will match the first snapshot after the watermark SELECT * FROM t VERSION AS OF &#39;watermark-1678883047356&#39;; If tag&rsquo;s name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if you have a tag named &lsquo;1&rsquo; based on snapshot 2, the statement SELECT * FROM t VERSION AS OF '1' actually queries snapshot 2 instead of snapshot 1.Batch Incremental#Read incremental changes between start snapshot (exclusive) and end snapshot.
For example:
&lsquo;5,10&rsquo; means changes between snapshot 5 and snapshot 10. &lsquo;TAG1,TAG3&rsquo; means changes between TAG1 and TAG3. By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.
Paimon supports that use Spark SQL to do the incremental query that implemented by Spark Table Valued Function.
-- read the incremental data between snapshot id 12 and snapshot id 20. SELECT * FROM paimon_incremental_query(&#39;tableName&#39;, 12, 20); -- read the incremental data between ts 1692169900000 and ts 1692169900000. SELECT * FROM paimon_incremental_between_timestamp(&#39;tableName&#39;, &#39;1692169000000&#39;, &#39;1692169900000&#39;); SELECT * FROM paimon_incremental_between_timestamp(&#39;tableName&#39;, &#39;2025-03-12 00:00:00&#39;, &#39;2025-03-12 00:08:00&#39;); -- read the incremental data to tag &#39;2024-12-04&#39;. -- Paimon will find an earlier tag and return changes between them. -- If the tag doesn&#39;t exist or the earlier tag doesn&#39;t exist, return empty. SELECT * FROM paimon_incremental_to_auto_tag(&#39;tableName&#39;, &#39;2024-12-04&#39;); In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can query audit_log table.
Streaming Query#Paimon currently supports Spark 3.3+ for streaming read.Paimon supports rich scan mode for streaming read. There is a list:
Scan ModeDescriptionlatestFor streaming sources, continuously reads latest changes without producing a snapshot at the beginning. latest-fullFor streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes.from-timestampFor streaming sources, continuously reads changes starting from timestamp specified by "scan.timestamp-millis", without producing a snapshot at the beginning. from-snapshotFor streaming sources, continuously reads changes starting from snapshot specified by "scan.snapshot-id", without producing a snapshot at the beginning. from-snapshot-fullFor streaming sources, produces from snapshot specified by "scan.snapshot-id" on the table upon first startup, and continuously reads changes.defaultIt is equivalent to from-snapshot if "scan.snapshot-id" is specified. It is equivalent to from-timestamp if "timestamp-millis" is specified. Or, It is equivalent to latest-full.A simple example with default scan mode:
// no any scan-related configs are provided, that will use latest-full scan mode. val query = spark.readStream .format(&#34;paimon&#34;) // by table name .table(&#34;table_name&#34;) // or by location // .load(&#34;/path/to/paimon/source/table&#34;) .writeStream .format(&#34;console&#34;) .start() Paimon Structured Streaming also supports a variety of streaming read modes, it can support many triggers and many read limits.
These read limits are supported:
KeyDefaultTypeDescriptionread.stream.maxFilesPerTrigger(none)IntegerThe maximum number of files returned in a single batch.read.stream.maxBytesPerTrigger(none)LongThe maximum number of bytes returned in a single batch.read.stream.maxRowsPerTrigger(none)LongThe maximum number of rows returned in a single batch.read.stream.minRowsPerTrigger(none)LongThe minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.read.stream.maxTriggerDelayMs(none)LongThe maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.Example: One
Use org.apache.spark.sql.streaming.Trigger.AvailableNow() and maxBytesPerTrigger defined by paimon.
// Trigger.AvailableNow()) processes all available data at the start // of the query in one or multiple batches, then terminates the query. // That set read.stream.maxBytesPerTrigger to 128M means that each // batch processes a maximum of 128 MB of data. val query = spark.readStream .format(&#34;paimon&#34;) .option(&#34;read.stream.maxBytesPerTrigger&#34;, &#34;134217728&#34;) .table(&#34;table_name&#34;) .writeStream .format(&#34;console&#34;) .trigger(Trigger.AvailableNow()) .start() Example: Two
Use org.apache.spark.sql.connector.read.streaming.ReadMinRows.
// It will not trigger a batch until there are more than 5,000 pieces of data, // unless the interval between the two batches is more than 300 seconds. val query = spark.readStream .format(&#34;paimon&#34;) .option(&#34;read.stream.minRowsPerTrigger&#34;, &#34;5000&#34;) .option(&#34;read.stream.maxTriggerDelayMs&#34;, &#34;300000&#34;) .table(&#34;table_name&#34;) .writeStream .format(&#34;console&#34;) .start() Paimon Structured Streaming supports read row in the form of changelog (add rowkind column in row to represent its change type) in two ways:
Direct streaming read with the system audit_log table Set read.changelog to true (default is false), then streaming read with table location Example:
// Option 1 val query1 = spark.readStream .format(&#34;paimon&#34;) .table(&#34;\`table_name$audit_log\`&#34;) .writeStream .format(&#34;console&#34;) .start() // Option 2 val query2 = spark.readStream .format(&#34;paimon&#34;) .option(&#34;read.changelog&#34;, &#34;true&#34;) .table(&#34;table_name&#34;) .writeStream .format(&#34;console&#34;) .start() /* +I 1 Hi +I 2 Hello */ Query Optimization#It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.
The filter functions that can accelerate data skipping are:
= &lt; &lt;= &gt; &gt;= IN (...) LIKE 'abc%' IS NULL Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.
Suppose that a table has the following specification:
CREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., ) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;catalog_id,order_id&#39; ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.
SELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id&gt;2035 AND order_id&lt;6000; However, the following filter cannot accelerate the query well.
SELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; `}),e.add({id:61,href:"/append-table/update/",title:"Update",section:"Table w/o PK",content:`Update#Now, only Spark SQL supports DELETE &amp; UPDATE, you can take a look at Spark Write.
Example:
DELETE FROM my_table WHERE currency = &#39;UNKNOWN&#39;; Update append table has two modes:
COW (Copy on Write): search for the hit files and then rewrite each file to remove the data that needs to be deleted from the files. This operation is costly. MOW (Merge on Write): By specifying 'deletion-vectors.enabled' = 'true', the Deletion Vectors mode can be enabled. Only marks certain records of the corresponding file for deletion and writes the deletion file, without rewriting the entire file. `}),e.add({id:62,href:"/concepts/spec/manifest/",title:"清单文件 Manifest",section:"规范说明 Specification",content:`Manifest#Manifest List#├── manifest └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 Manifest List 包含多个 manifest 文件的元信息。其文件名包含 UUID，是一个 Avro 文件，schema 如下：
_FILE_NAME：STRING，manifest 文件名。 _FILE_SIZE：BIGINT，manifest 文件大小。 _NUM_ADDED_FILES：BIGINT，manifest 中新增文件数量。 _NUM_DELETED_FILES：BIGINT，manifest 中删除文件数量。 _PARTITION_STATS：SimpleStats，分区统计信息，该 manifest 中分区字段的最小值和最大值，有助于查询时跳过某些 manifest 文件。 _SCHEMA_ID：BIGINT，写入该 manifest 文件时的 schema ID。 Manifest#Manifest 包含多个数据文件、变更日志文件或表索引文件的元信息。其文件名包含 UUID，是一个 Avro 文件。
文件的变更记录保存在 manifest 中，文件可以被添加或删除。Manifest 应保持有序，同一个文件可能被多次添加或删除，最终以最新版本为准。 该设计使得提交操作更轻量，支持由压缩操作产生的文件删除。
Data Manifest#Data Manifest 包含多个数据文件或变更日志文件的元信息。
├── manifest └── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 Schema 如下：
_KIND：TINYINT，表示操作类型，ADD（添加）或 DELETE（删除）。 _PARTITION：BYTES，分区规格，BinaryRow 格式。 _BUCKET：INT，该文件所属的桶。 _TOTAL_BUCKETS：INT，写入该文件时的总桶数，用于桶变更后的校验。 _FILE：数据文件元信息。 数据文件元信息包括：
_FILE_NAME：STRING，文件名。 _FILE_SIZE：BIGINT，文件大小。 _ROW_COUNT：BIGINT，该文件中总行数（包含添加和删除）。 _MIN_KEY：STRING，该文件的最小键。 _MAX_KEY：STRING，该文件的最大键。 _KEY_STATS：SimpleStats，键的统计信息。 _VALUE_STATS：SimpleStats，值的统计信息。 _MIN_SEQUENCE_NUMBER：BIGINT，最小序列号。 _MAX_SEQUENCE_NUMBER：BIGINT，最大序列号。 _SCHEMA_ID：BIGINT，写入该文件时的 schema ID。 _LEVEL：INT，文件在 LSM 中的层级。 _EXTRA_FILES：ARRAY，该文件的额外文件，例如数据文件索引文件。 _CREATION_TIME：TIMESTAMP_MILLIS，文件创建时间。 _DELETE_ROW_COUNT：BIGINT，删除的行数，行数计算为 addRowCount + deleteRowCount。 _EMBEDDED_FILE_INDEX：BYTES，如果数据文件索引过小，则存储在 manifest 中。 _FILE_SOURCE：TINYINT，指示该文件是作为追加（APPEND）还是压缩（COMPACT）文件生成。 _VALUE_STATS_COLS：ARRAY，元数据中的统计列。 _EXTERNAL_PATH：该文件的外部路径，如果在仓库中则为 null。 Index Manifest#Index Manifest 包含多个 table-index 文件的元信息。
├── manifest └── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 Schema 如下：
_KIND：TINYINT，操作类型，ADD（添加）或 DELETE（删除）。 _PARTITION：BYTES，分区规格，BinaryRow 格式。 _BUCKET：INT，该文件所属的桶。 _INDEX_TYPE：STRING，索引类型，取值为 &ldquo;HASH&rdquo; 或 &ldquo;DELETION_VECTORS&rdquo;。 _FILE_NAME：STRING，文件名。 _FILE_SIZE：BIGINT，文件大小。 _ROW_COUNT：BIGINT，总行数。 _DELETIONS_VECTORS_RANGES：仅用于 &ldquo;DELETION_VECTORS&rdquo; 类型的元数据，是一个删除向量元数据数组，每个删除向量元数据的 schema 为： f0：对应的删除向量数据文件名。 f1：该删除向量在索引文件中的起始偏移量。 f2：该删除向量在索引文件中的长度。 _CARDINALITY：被删除的行数。 Appendix#SimpleStats#SimpleStats 是嵌套行，schema 如下：
_MIN_VALUES：BYTES，BinaryRow，列的最小值。 _MAX_VALUES：BYTES，BinaryRow，列的最大值。 _NULL_COUNTS：ARRAY，各列的空值数量。 BinaryRow#BinaryRow 以字节数组为底层存储，而非 Java 对象，这能显著减少 Java 对象的序列化和反序列化开销。
一个 Row 包含两部分：固定长度部分和可变长度部分。固定长度部分包含 1 字节的头部信息、null 位集合以及字段值。null 位集合用于跟踪字段是否为 null，并按 8 字节对齐。
字段值存储固定长度的原始类型和可以放入 8 字节内的可变长度值。如果可变长度字段超出 8 字节，则存储可变长度部分的长度和偏移量。
`}),e.add({id:63,href:"/append-table/bucketed/",title:"Bucketed",section:"Table w/o PK",content:`Bucketed Append#You can define the bucket and bucket-key to get a bucketed append table.
Example to create bucketed append table:
FlinkCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( &#39;bucket&#39; = &#39;8&#39;, &#39;bucket-key&#39; = &#39;product_id&#39; ); Streaming#An ordinary Append table has no strict ordering guarantees for its streaming writes and reads, but there are some cases where you need to define a key similar to Kafka&rsquo;s.
Every record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue.
Compaction in Bucket#By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:
KeyDefaultTypeDescriptionwrite-onlyfalseBooleanIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.compaction.min.file-num5IntegerFor file set [f_0,...,f_N], the minimum file number to trigger a compaction for append table.full-compaction.delta-commits(none)IntegerFull compaction will be constantly triggered after delta commits.Streaming Read Order#For streaming reads, records are produced in the following order:
For any two records from two different partitions If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first. For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them. Watermark Definition#You can define watermark for reading Paimon tables:
CREATE TABLE t ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL &#39;5&#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(\`user\`) FROM TABLE( TUMBLE(TABLE t, DESCRIPTOR(order_time), INTERVAL &#39;10&#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:
KeyDefaultTypeDescriptionscan.watermark.alignment.group(none)StringA group of sources to align watermarks.scan.watermark.alignment.max-drift(none)DurationMaximal drift to align watermarks, before we pause consuming from the source/task/partition.Bounded Stream#Streaming Source can also be bounded, you can specify &lsquo;scan.bounded.watermark&rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.
Watermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.
CREATE TABLE kafka_table ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL &#39;5&#39; SECOND ) WITH (&#39;connector&#39; = &#39;kafka&#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(&#39;scan.bounded.watermark&#39;=&#39;...&#39;) */; Batch#Bucketed table can be used to avoid shuffle if necessary in batch query, for example, you can use the following Spark SQL to read a Paimon table:
SET spark.sql.sources.v2.bucketing.enabled = true; CREATE TABLE FACT_TABLE (order_id INT, f1 STRING) TBLPROPERTIES (&#39;bucket&#39;=&#39;10&#39;, &#39;bucket-key&#39; = &#39;order_id&#39;); CREATE TABLE DIM_TABLE (order_id INT, f2 STRING) TBLPROPERTIES (&#39;bucket&#39;=&#39;10&#39;, &#39;primary-key&#39; = &#39;order_id&#39;); SELECT * FROM FACT_TABLE JOIN DIM_TABLE on t1.order_id = t4.order_id; The spark.sql.sources.v2.bucketing.enabled config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.
The costly join shuffle will be avoided if two tables have the same bucketing strategy and same number of buckets.
`}),e.add({id:64,href:"/primary-key-table/changelog-producer/",title:"Changelog Producer",section:"Table with PK",content:`Changelog Producer#Streaming write can continuously produce the latest changes for streaming read.
By specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from table files.
changelog-producer may significantly reduce compaction performance, please do not enable it unless necessary.None#By default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.
However, these merged changes cannot form a complete changelog, because we can&rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to &ldquo;remember&rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.
Consider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.
To conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in &ldquo;normalize&rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided. (You can force removing &ldquo;normalize&rdquo; operator via 'scan.remove-normalize'.)
Input#By specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.
input changelog producer can be used when Paimon writers&rsquo; inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.
Lookup#If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.
By specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing (You can also enable Async Compaction).
Lookup will cache data on the memory and local disk, you can use the following options to tune performance:
OptionDefaultTypeDescriptionlookup.cache-file-retention1 hDurationThe cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.lookup.cache-max-disk-sizeunlimitedMemorySizeMax disk size for lookup cache, you can use this option to limit the use of local disks.lookup.cache-max-memory-size256 mbMemorySizeMax memory size for lookup cache.Lookup changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.
(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).
Full Compaction#You can also consider using &lsquo;full-compaction&rsquo; changelog producer to generate changelog, and is more suitable for scenarios with large latency (For example, 30 minutes).
By specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions. By specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a changelog. Generally speaking, the cost and consumption of full compaction are high, so we recommend using 'lookup' changelog producer.
Full compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.Full-compaction changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.
Changelog Merging#For input, lookup, full-compaction &lsquo;changelog-producer&rsquo;.
If Flink&rsquo;s checkpoint interval is short (for example, 30 seconds) and the number of buckets is large, each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.
In order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.
`}),e.add({id:65,href:"/migration/clone-to-paimon/",title:"Clone To Paimon",section:"Migration",content:`Clone To Paimon#Clone supports cloning tables to Paimon tables.
Clone is OVERWRITE semantic that will overwrite the partitions of the target table according to the data. Clone is reentrant, but it requires existing tables to contain all fields from the source table and have the same partition fields. Currently, clone supports:
Clone Hive tables in Hive Catalog to Paimon Catalog, supports Parquet, ORC, Avro formats, target table will be append table. Clone Hudi tables in Hive Catalog to Paimon Catalog, target table will be append table. The source table below is currently under development:
Clone Paimon tables to Paimon tables, target table can be primary table or append table. Clone Iceberg tables in Hive Catalog to Paimon Catalog, target table will be append table. Clone Hive Table#&lt;FLINK_HOME&gt;/flink run ./paimon-flink-action-1.2.0.jar \\ clone \\ --database default \\ --table hivetable \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://localhost:9088 \\ --target_database test \\ --target_table test_table \\ --target_catalog_conf warehouse=my_warehouse \\ --parallelism 10 \\ --where &lt;filter_spec&gt; You can use filter spec to specify the filtering condition for the partition.
Clone Hive Database#&lt;FLINK_HOME&gt;/flink run ./paimon-flink-action-1.2.0.jar \\ clone \\ --database default \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://localhost:9088 \\ --target_database test \\ --parallelism 10 \\ --target_catalog_conf warehouse=my_warehouse Clone Hudi Tables#Clone Hudi needs dependency: hudi-flink1.18-bundle-0.15.0.jar
The execution method is the same as the Hive table mentioned above.
`}),e.add({id:66,href:"/flink/consumer-id/",title:"Consumer ID",section:"Engine Flink",content:`Consumer ID#Consumer id can help you accomplish the following two things:
Safe consumption: When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration. Resume from breakpoint: When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state. Usage#You can specify the consumer-id when streaming read table.
The consumer will prevent expiration of the snapshot. In order to prevent too many snapshots caused by mistakes, you need to specify 'consumer.expiration-time' to manage the lifetime of consumers.
ALTER TABLE t SET (&#39;consumer.expiration-time&#39; = &#39;1 d&#39;); Then, restart streaming write job of this table, expiration of consumers will be triggered in writing job.
SELECT * FROM t /*+ OPTIONS(&#39;consumer-id&#39; = &#39;myid&#39;, &#39;consumer.mode&#39; = &#39;at-least-once&#39;) */; Ignore Progress#Sometimes, you only want the feature of &lsquo;Safe Consumption&rsquo;. You want to get a new snapshot progress when restarting the stream consumption job , you can enable the 'consumer.ignore-progress' option.
SELECT * FROM t /*+ OPTIONS(&#39;consumer-id&#39; = &#39;myid&#39;, &#39;consumer.ignore-progress&#39; = &#39;true&#39;) */; The startup of this job will retrieve the snapshot that should be read again.
Consumer Mode#By default, the consumption of snapshots is strictly aligned within the checkpoint to make &lsquo;Resume from breakpoint&rsquo; feature exactly-once.
But in some scenarios where you don&rsquo;t need &lsquo;Resume from breakpoint&rsquo;, or you don&rsquo;t need strict &lsquo;Resume from breakpoint&rsquo;, you can consider enabling 'consumer.mode' = 'at-least-once' mode. This mode:
Allow readers consume snapshots at different rates and record the slowest snapshot-id among all readers into the consumer. It doesn&rsquo;t affect the checkpoint time and have good performance. This mode can provide more capabilities, such as watermark alignment. About 'consumer.mode', since the implementation of exactly-once mode and at-least-once mode are completely different, the state of flink is incompatible and cannot be restored from the state when switching modes.Reset Consumer#You can reset or delete a consumer with a given consumer ID and next snapshot ID and delete a consumer with a given consumer ID. First, you need to stop the streaming task using this consumer ID, and then execute the reset consumer action job.
Run the following command:
Flink SQLCALL sys.reset_consumer( \`table\` =&gt; &#39;database_name.table_name&#39;, consumer_id =&gt; &#39;consumer_id&#39;, next_snapshot_id =&gt; &lt;snapshot_id&gt; ); -- No next_snapshot_id if you want to delete the consumer Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ reset-consumer \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --consumer_id &lt;consumer-id&gt; \\ [--next_snapshot &lt;next-snapshot-id&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] ## No next_snapshot if you want to delete the consumer Clear Consumers#You can clear consumers in bulk with a given including consumers and excluding consumers(accept regular expression).
Run the following command:
Flink SQLCALL sys.clear_consumers( \`table\` =&gt; &#39;database_name.table_name&#39;, \`including_consumers\` =&gt; &#39;including_consumers&#39;, \`excluding_consumers\` =&gt; &#39;excluding_consumers&#39; ); -- No including_consumers if you want to clear all consumers except excluding_consumers in the table Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ clear_consumers \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--including_consumers &lt;including-consumers&gt;] \\ [--excluding_consumers &lt;excluding-consumers&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] ## No including_consumers if you want to clear all consumers except excluding_consumers in the table `}),e.add({id:67,href:"/flink/",title:"Engine Flink",section:"Apache Paimon",content:``}),e.add({id:68,href:"/iceberg/hive-catalog/",title:"Hive Catalogs",section:"Iceberg Metadata",content:`Hive Catalog#When creating Paimon table, set 'metadata.iceberg.storage' = 'hive-catalog'. This option value not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive. This Paimon table can be accessed from Iceberg Hive catalog later.
To provide information about Hive metastore, you also need to set some (or all) of the following table options when creating Paimon table.
OptionDefaultTypeDescriptionmetadata.iceberg.uriStringHive metastore uri for Iceberg Hive catalog.metadata.iceberg.hive-conf-dirStringhive-conf-dir for Iceberg Hive catalog.metadata.iceberg.hadoop-conf-dirStringhadoop-conf-dir for Iceberg Hive catalog.metadata.iceberg.manifest-compressionsnappyStringCompression for Iceberg manifest files.metadata.iceberg.manifest-legacy-versionfalseBooleanShould use the legacy manifest version to generate Iceberg's 1.4 manifest files.metadata.iceberg.hive-client-classorg.apache.hadoop.hive.metastore.HiveMetaStoreClientStringHive client class name for Iceberg Hive Catalog.metadata.iceberg.glue.skip-archivefalseBooleanSkip archive for AWS Glue catalog.metadata.iceberg.hive-skip-update-statsfalseBooleanSkip updating Hive stats.AWS Glue Catalog#You can use Hive Catalog to connect AWS Glue metastore, you can use set 'metadata.iceberg.hive-client-class' to 'com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient'.
Note: You can use this repo to build the required jar, include it in your path and configure the AWSCatalogMetastoreClient.
`}),e.add({id:69,href:"/maintenance/manage-snapshots/",title:"Manage Snapshots",section:"Maintenance",content:`Manage Snapshots#This section will describe the management and behavior related to snapshots.
Expire Snapshots#Paimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.
Currently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.
Snapshot expiration is controlled by the following table properties.
OptionRequiredDefaultTypeDescriptionsnapshot.time-retainedNo1 hDurationThe maximum time of completed snapshots to retain.snapshot.num-retained.minNo10IntegerThe minimum number of completed snapshots to retain. Should be greater than or equal to 1.snapshot.num-retained.maxNoInteger.MAX_VALUEIntegerThe maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.snapshot.expire.execution-modeNosyncEnumSpecifies the execution mode of expire.snapshot.expire.limitNo10IntegerThe maximum number of snapshots allowed to expire at a time.When the number of snapshots is less than snapshot.num-retained.min, no snapshots will be expired(even the condition snapshot.time-retained meet), after which snapshot.num-retained.max and snapshot.time-retained will be used to control the snapshot expiration until the remaining snapshot meets the condition.
The following example show more details(snapshot.num-retained.min is 2, snapshot.time-retained is 1h, snapshot.num-retained.max is 5):
snapshot item is described using tuple (snapshotId, corresponding time)
New SnapshotsAll snapshots after expiration checkexplanation(snapshots-1, 2023-07-06 10:00)(snapshots-1, 2023-07-06 10:00)No snapshot expired(snapshots-2, 2023-07-06 10:20)(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20)No snapshot expired(snapshots-3, 2023-07-06 10:40)(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40)No snapshot expired(snapshots-4, 2023-07-06 11:00)(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) No snapshot expired(snapshots-5, 2023-07-06 11:20)(snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20)snapshot-1 was expired because the condition \`snapshot.time-retained\` is not met(snapshots-6, 2023-07-06 11:30)(snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30)snapshot-2 was expired because the condition \`snapshot.time-retained\` is not met(snapshots-7, 2023-07-06 11:35)(snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35)No snapshot expired(snapshots-8, 2023-07-06 11:36)(snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35) (snapshots-8, 2023-07-06 11:36)snapshot-3 was expired because the condition \`snapshot.num-retained.max\` is not metPlease note that too short retain time or too small retain number may result in:
Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files fail to restart. When the job restarts, the snapshot it recorded may have expired. (You can use Consumer Id to protect streaming reading in a small retain time of snapshot expiration). By default, paimon will delete expired snapshots synchronously. When there are too many files that need to be deleted, they may not be deleted quickly and back-pressured to the upstream operator. To avoid this situation, users can use asynchronous expiration mode by setting snapshot.expire.execution-mode to async. However, if your job runs in batch mode, it is not recommended to use asynchronous expiration mode, as the expire task may fail to complete successfully.
Rollback to Snapshot#Rollback a table to a specific snapshot ID.
Flink SQLRun the following command:
CALL sys.rollback_to(\`table\` =&gt; &#39;database_name.table_name&#39;, snapshot_id =&gt; &lt;snasphot-id&gt;); Flink ActionRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ rollback_to \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --version &lt;snapshot-id&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Java APIimport org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback: // snapshot-3 // snapshot-4 // snapshot-5 // snapshot-6 // snapshot-7 table.rollbackTo(5); // after rollback: // snapshot-3 // snapshot-4 // snapshot-5 } } SparkRun the following sql:
CALL sys.rollback(table =&gt; &#39;database_name.table_name&#39;, snapshot =&gt; snasphot_id); Remove Orphan Files#Paimon files are deleted physically only when expiring snapshots. However, it is possible that some unexpected errors occurred when deleting files, so that there may exist files that are not used by Paimon snapshots (so-called &ldquo;orphan files&rdquo;). You can submit a remove_orphan_files job to clean them:
Spark SQL/Flink SQLCALL sys.remove_orphan_files(\`table\` =&gt; &#39;my_db.my_table&#39;, [older_than =&gt; &#39;2023-10-31 12:00:00&#39;]) CALL sys.remove_orphan_files(\`table\` =&gt; &#39;my_db.*&#39;, [older_than =&gt; &#39;2023-10-31 12:00:00&#39;]) Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ remove_orphan_files \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--older_than &lt;timestamp&gt;] \\ [--dry_run &lt;false/true&gt;] \\ [--parallelism &lt;parallelism&gt;] To avoid deleting files that are newly added by other writing jobs, this action only deletes orphan files older than 1 day by default. The interval can be modified by --older_than. For example:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ remove_orphan_files \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table T \\ --older_than &#39;2023-10-31 12:00:00&#39; The table can be * to clean all tables in the database.
`}),e.add({id:70,href:"/cdc-ingestion/pulsar-cdc/",title:"Pulsar CDC",section:"CDC Ingestion",content:`Pulsar CDC#Prepare Pulsar Bundled Jar#flink-connector-pulsar-*.jar Supported Formats#Flink provides several Pulsar CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a pulsar topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Pulsar CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.
FormatsSupportedCanal CDCTrueDebezium CDCTrueMaxwell CDCTrueOGG CDCTrueJSONTrueThe JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don&rsquo;t contain field types; When you write JSON sources into Flink Pulsar sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:
If missing field types, Paimon will use &lsquo;STRING&rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization. Synchronizing Tables#By using PulsarSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Pulsar&rsquo;s one topic into one Paimon table.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_table \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--type_mapping to-string] \\ [--computed_column &lt;&#39;column-name=expr-name(args[, ...])&#39;&gt; [--computed_column ...]] \\ [--pulsar_conf &lt;pulsar-source-conf&gt; [--pulsar_conf &lt;pulsar-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--tableThe Paimon table name.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.--computed_columnThe definitions of computed columns. The argument field is from Pulsar topic's table field name. See here for a complete list of configurations. --pulsar_confThe configuration for Flink Pulsar sources. Each configuration should be specified in the format \`key=value\`. \`topic/topic-pattern\`, \`value.format\`, \`pulsar.client.serviceUrl\`, \`pulsar.admin.adminUrl\`, and \`pulsar.consumer.subscriptionName\` are required configurations, others are optional.See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Pulsar topic&rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Pulsar topic&rsquo;s tables.
Example 1:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column &#39;_year=year(age)&#39; \\ --pulsar_conf topic=order \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 If the Pulsar topic doesn&rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.
NOTE: In this case you shouldn&rsquo;t use &ndash;partition_keys or &ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.
Example 2: If you want to synchronize a table which has primary key &lsquo;id INT&rsquo;, and you want to compute a partition key &lsquo;part=date_format(create_time,yyyy-MM-dd)&rsquo;, you can create a such table first (the other columns can be omitted):
CREATE TABLE test_db.test_table ( id INT, -- primary key create_time TIMESTAMP, -- the argument of computed column part part STRING, -- partition key PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --computed_column &#39;part=date_format(create_time,yyyy-MM-dd)&#39; \\ ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use &lsquo;format=json&rsquo; to synchronize such data to the Paimon table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column &#39;pt=date_format(event_tm, yyyyMMdd)&#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=test_log \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf sink.parallelism=4 Synchronizing Databases#By using PulsarSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.
To use this feature through flink run, run the following shell command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_database \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ [--table_prefix &lt;paimon-table-prefix&gt;] \\ [--table_suffix &lt;paimon-table-suffix&gt;] \\ [--including_tables &lt;table-name|name-regular-expr&gt;] \\ [--excluding_tables &lt;table-name|name-regular-expr&gt;] \\ [--type_mapping to-string] \\ [--partition_keys &lt;partition_keys&gt;] \\ [--primary_keys &lt;primary-keys&gt;] \\ [--pulsar_conf &lt;pulsar-source-conf&gt; [--pulsar_conf &lt;pulsar-source-conf&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] \\ [--table_conf &lt;paimon-table-sink-conf&gt; [--table_conf &lt;paimon-table-sink-conf&gt; ...]] ConfigurationDescription--warehouseThe path to Paimon warehouse.--databaseThe database name in Paimon catalog.--ignore_incompatibleIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.--table_prefixThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have "ods_" as prefix, you can specify "--table_prefix ods_".--table_suffixThe suffix of all Paimon tables to be synchronized. The usage is same as "--table_prefix".--including_tablesIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying "--including_tables test|paimon.*" means to synchronize table 'test' and all tables start with 'paimon'.--excluding_tablesIt is used to specify which source tables are not to be synchronized. The usage is same as "--including_tables". "--excluding_tables" has higher priority than "--including_tables" if you specified both.--type_mappingIt is used to specify how to map MySQL data type to Paimon type.
Supported options:"tinyint1-not-bool": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN."to-nullable": ignores all NOT NULL constraints (except for primary keys).This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation."to-string": maps all MySQL types to STRING."char-to-string": maps MySQL CHAR(length)/VARCHAR(length) types to STRING."longtext-to-bytes": maps MySQL LONGTEXT types to BYTES."bigint-unsigned-to-bigint": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.--partition_keysThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example "dt,hh,mm".If the keys are not in source table, the sink table won't set partition keys.--primary_keysThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example "buyer_id,seller_id".If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.Otherwise, the sink table won't set primary keys.--pulsar_confThe configuration for Flink Pulsar sources. Each configuration should be specified in the format \`key=value\`. \`topic/topic-pattern\`, \`value.format\`, \`pulsar.client.serviceUrl\`, \`pulsar.admin.adminUrl\`, and \`pulsar.consumer.subscriptionName\` are required configurations, others are optional.See its document for a complete list of configurations.--catalog_confThe configuration for Paimon catalog. Each configuration should be specified in the format "key=value". See here for a complete list of catalog configurations.--table_confThe configuration for Paimon table sink. Each configuration should be specified in the format "key=value". See here for a complete list of table configurations.Only tables with primary keys will be synchronized.
This action will build a single combined sink for all tables. For each Pulsar topic&rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Pulsar topic&rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Pulsar record, this action will try to preform schema evolution.
Example
Synchronization from one Pulsar topic to Paimon database.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --pulsar_conf topic=order \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronization from multiple Pulsar topics to Paimon database.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ pulsar_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --pulsar_conf topic=order,logistic_order,user \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Additional pulsar_config#There are some useful options to build Flink Pulsar Source, but they are not provided by flink-pulsar-connector document. They are:
KeyDefaultTypeDescriptionvalue.format(none)StringDefines the format identifier for encoding value data.topic(none)StringTopic name(s) from which the data is read. It also supports topic list by separating topic by semicolon like 'topic-1;topic-2'. Note, only one of "topic-pattern" and "topic" can be specified.topic-pattern(none)StringThe regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of "topic-pattern" and "topic" can be specified.pulsar.startCursor.fromMessageIdEARLIESTStingUsing a unique identifier of a single message to seek the start position. The common format is a triple '&ltlong&gtledgerId,&ltlong&gtentryId,&ltint&gtpartitionIndex'. Specially, you can set it to EARLIEST (-1, -1, -1) or LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).pulsar.startCursor.fromPublishTime(none)LongUsing the message publish time to seek the start position.pulsar.startCursor.fromMessageIdInclusivetrueBooleanWhether to include the given message id. This option only works when the message id is not EARLIEST or LATEST.pulsar.stopCursor.atMessageId(none)StringStop consuming when the message id is equal or greater than the specified message id. Message that is equal to the specified message id will not be consumed. The common format is a triple '&ltlong&gtledgerId,&ltlong&gtentryId,&ltint&gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).pulsar.stopCursor.afterMessageId(none)StringStop consuming when the message id is greater than the specified message id. Message that is equal to the specified message id will be consumed. The common format is a triple '&ltlong&gtledgerId,&ltlong&gtentryId,&ltint&gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).pulsar.stopCursor.atEventTime(none)LongStop consuming when message event time is greater than or equals the specified timestamp. Message that even time is equal to the specified timestamp will not be consumed.pulsar.stopCursor.afterEventTime(none)LongStop consuming when message event time is greater than the specified timestamp. Message that even time is equal to the specified timestamp will be consumed.pulsar.source.unboundedtrueBooleanTo specify the boundedness of a stream.schema.registry.url(none)StringWhen configuring "value.format=debezium-avro" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.`}),e.add({id:71,href:"/program-api/python-api/",title:"Python API",section:"Program API",content:`Java-based Implementation For Python API#Python SDK has defined Python API for Paimon. Currently, there is only a Java-based implementation.
Java-based implementation will launch a JVM and use py4j to execute Java code to read and write Paimon table.
Environment Settings#SDK Installing#SDK is published at pypaimon. You can install by
pip install pypaimon Java Runtime Environment#This SDK needs JRE 1.8. After installing JRE, make sure that at least one of the following conditions is met:
java command is available. You can verify it by java -version. JAVA_HOME and PATH variables are set correctly. For example, you can set: export JAVA_HOME=/path/to/java-directory export PATH=$JAVA_HOME/bin:$PATH Set Environment Variables#Because we need to launch a JVM to access Java code, JVM environment need to be set. Besides, the java code need Hadoop dependencies, so hadoop environment should be set.
Java classpath#The package has set necessary paimon core dependencies (Local/Hadoop FileIO, Avro/Orc/Parquet format support and FileSystem/Jdbc/Hive catalog), so If you just test codes in local or in hadoop environment, you don&rsquo;t need to set classpath.
If you need other dependencies such as OSS/S3 filesystem jars, or special format and catalog ,please prepare jars and set classpath via one of the following ways:
Set system environment variable: export _PYPAIMON_JAVA_CLASSPATH=/path/to/jars/* Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JAVA_CLASSPATH] = &#39;/path/to/jars/*&#39; JVM args (optional)#You can set JVM args via one of the following ways:
Set system environment variable: export _PYPAIMON_JVM_ARGS='arg1 arg2 ...' Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JVM_ARGS] = &#39;arg1 arg2 ...&#39; Hadoop classpath#If the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, then you don&rsquo;t need to set hadoop.
Otherwise, you should set hadoop classpath via one of the following ways:
Set system environment variable: export _PYPAIMON_HADOOP_CLASSPATH=/path/to/jars/* Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_HADOOP_CLASSPATH] = &#39;/path/to/jars/*&#39; If you just want to test codes in local, we recommend to use Flink Pre-bundled hadoop jar.
Create Catalog#Before coming into contact with the Table, you need to create a Catalog.
from pypaimon.py4j import Catalog # Note that keys and values are all string catalog_options = { &#39;metastore&#39;: &#39;filesystem&#39;, &#39;warehouse&#39;: &#39;file:///path/to/warehouse&#39; } catalog = Catalog.create(catalog_options) Create Database &amp; Table#You can use the catalog to create table for writing data.
Create Database (optional)#Table is located in a database. If you want to create table in a new database, you should create it.
catalog.create_database( name=&#39;database_name&#39;, ignore_if_exists=True, # If you want to raise error if the database exists, set False properties={&#39;key&#39;: &#39;value&#39;} # optional database properties ) Create Schema#Table schema contains fields definition, partition keys, primary keys, table options and comment. The field definition is described by pyarrow.Schema. All arguments except fields definition are optional.
Generally, there are two ways to build pyarrow.Schema.
First, you can use pyarrow.schema method directly, for example:
import pyarrow as pa from pypaimon import Schema pa_schema = pa.schema([ (&#39;dt&#39;, pa.string()), (&#39;hh&#39;, pa.string()), (&#39;pk&#39;, pa.int64()), (&#39;value&#39;, pa.string()) ]) schema = Schema( pa_schema=pa_schema, partition_keys=[&#39;dt&#39;, &#39;hh&#39;], primary_keys=[&#39;dt&#39;, &#39;hh&#39;, &#39;pk&#39;], options={&#39;bucket&#39;: &#39;2&#39;}, comment=&#39;my test table&#39; ) See Data Types for all supported pyarrow-to-paimon data types mapping.
Second, if you have some Pandas data, the pa_schema can be extracted from DataFrame:
import pandas as pd import pyarrow as pa from pypaimon import Schema # Example DataFrame data data = { &#39;dt&#39;: [&#39;2024-01-01&#39;, &#39;2024-01-01&#39;, &#39;2024-01-02&#39;], &#39;hh&#39;: [&#39;12&#39;, &#39;15&#39;, &#39;20&#39;], &#39;pk&#39;: [1, 2, 3], &#39;value&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], } dataframe = pd.DataFrame(data) # Get Paimon Schema record_batch = pa.RecordBatch.from_pandas(dataframe) schema = Schema( pa_schema=record_batch.schema, partition_keys=[&#39;dt&#39;, &#39;hh&#39;], primary_keys=[&#39;dt&#39;, &#39;hh&#39;, &#39;pk&#39;], options={&#39;bucket&#39;: &#39;2&#39;}, comment=&#39;my test table&#39; ) Create Table#After building table schema, you can create corresponding table:
schema = ... catalog.create_table( identifier=&#39;database_name.table_name&#39;, schema=schema, ignore_if_exists=True # If you want to raise error if the table exists, set False ) Get Table#The Table interface provides tools to read and write table.
table = catalog.get_table(&#39;database_name.table_name&#39;) Batch Read#Set Read Parallelism#TableRead interface provides parallelly reading for multiple splits. You can set 'max-workers': 'N' in catalog_options to set thread numbers for reading splits. max-workers is 1 by default, that means TableRead will read splits sequentially if you doesn&rsquo;t set max-workers.
Get ReadBuilder and Perform pushdown#A ReadBuilder is used to build reading utils and perform filter and projection pushdown.
table = catalog.get_table(&#39;database_name.table_name&#39;) read_builder = table.new_read_builder() You can use PredicateBuilder to build filters and pushdown them by ReadBuilder:
# Example filter: (&#39;f0&#39; &lt; 3 OR &#39;f1&#39; &gt; 6) AND &#39;f3&#39; = &#39;A&#39; predicate_builder = read_builder.new_predicate_builder() predicate1 = predicate_builder.less_than(&#39;f0&#39;, 3) predicate2 = predicate_builder.greater_than(&#39;f1&#39;, 6) predicate3 = predicate_builder.or_predicates([predicate1, predicate2]) predicate4 = predicate_builder.equal(&#39;f3&#39;, &#39;A&#39;) predicate_5 = predicate_builder.and_predicates([predicate3, predicate4]) read_builder = read_builder.with_filter(predicate_5) See Predicate for all supported filters and building methods.
You can also pushdown projection by ReadBuilder:
# select f3 and f2 columns read_builder = read_builder.with_projection([&#39;f3&#39;, &#39;f2&#39;]) Scan Plan#Then you can step into Scan Plan stage to get splits:
table_scan = read_builder.new_scan() splits = table_scan.plan().splits() Read Splits#Finally, you can read data from the splits to various data format.
Apache Arrow#This requires pyarrow to be installed.
You can read all the data into a pyarrow.Table:
table_read = read_builder.new_read() pa_table = table_read.to_arrow(splits) print(pa_table) # pyarrow.Table # f0: int32 # f1: string # ---- # f0: [[1,2,3],[4,5,6],...] # f1: [[&#34;a&#34;,&#34;b&#34;,&#34;c&#34;],[&#34;d&#34;,&#34;e&#34;,&#34;f&#34;],...] You can also read data into a pyarrow.RecordBatchReader and iterate record batches:
table_read = read_builder.new_read() for batch in table_read.to_arrow_batch_reader(splits): print(batch) # pyarrow.RecordBatch # f0: int32 # f1: string # ---- # f0: [1,2,3] # f1: [&#34;a&#34;,&#34;b&#34;,&#34;c&#34;] Pandas#This requires pandas to be installed.
You can read all the data into a pandas.DataFrame:
table_read = read_builder.new_read() df = table_read.to_pandas(splits) print(df) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... DuckDB#This requires duckdb to be installed.
You can convert the splits into an in-memory DuckDB table and query it:
table_read = read_builder.new_read() duckdb_con = table_read.to_duckdb(splits, &#39;duckdb_table&#39;) print(duckdb_con.query(&#34;SELECT * FROM duckdb_table&#34;).fetchdf()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... print(duckdb_con.query(&#34;SELECT * FROM duckdb_table WHERE f0 = 1&#34;).fetchdf()) # f0 f1 # 0 1 a Ray#This requires ray to be installed.
You can convert the splits into a Ray dataset and handle it by Ray API:
table_read = read_builder.new_read() ray_dataset = table_read.to_ray(splits) print(ray_dataset) # MaterializedDataset(num_blocks=1, num_rows=9, schema={f0: int32, f1: string}) print(ray_dataset.take(3)) # [{&#39;f0&#39;: 1, &#39;f1&#39;: &#39;a&#39;}, {&#39;f0&#39;: 2, &#39;f1&#39;: &#39;b&#39;}, {&#39;f0&#39;: 3, &#39;f1&#39;: &#39;c&#39;}] print(ray_dataset.to_pandas()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... Batch Write#Paimon table write is Two-Phase Commit, you can write many times, but once committed, no more data can be written.
Currently, Python SDK doesn&rsquo;t support writing primary key table with bucket=-1.table = catalog.get_table(&#39;database_name.table_name&#39;) # 1. Create table write and commit write_builder = table.new_batch_write_builder() table_write = write_builder.new_write() table_commit = write_builder.new_commit() # 2. Write data. Support 3 methods: # 2.1 Write pandas.DataFrame dataframe = ... table_write.write_pandas(dataframe) # 2.2 Write pyarrow.Table pa_table = ... table_write.write_arrow(pa_table) # 2.3 Write pyarrow.RecordBatch record_batch = ... table_write.write_arrow_batch(record_batch) # 3. Commit data commit_messages = table_write.prepare_commit() table_commit.commit(commit_messages) # 4. Close resources table_write.close() table_commit.close() By default, the data will be appended to table. If you want to overwrite table, you should use TableWrite#overwrite API:
# overwrite whole table write_builder.overwrite() # overwrite partition &#39;dt=2024-01-01&#39; write_builder.overwrite({&#39;dt&#39;: &#39;2024-01-01&#39;}) Data Types#pyarrow Paimon pyarrow.int8() TINYINT pyarrow.int16() SMALLINT pyarrow.int32() INT pyarrow.int64() BIGINT pyarrow.float16() pyarrow.float32() FLOAT pyarrow.float64() DOUBLE pyarrow.string() STRING pyarrow.boolean() BOOLEAN Predicate#Predicate kind Predicate method p1 and p2 PredicateBuilder.and_predicates([p1, p2]) p1 or p2 PredicateBuilder.or_predicates([p1, p2]) f = literal PredicateBuilder.equal(f, literal) f != literal PredicateBuilder.not_equal(f, literal) f &lt; literal PredicateBuilder.less_than(f, literal) f &lt;= literal PredicateBuilder.less_or_equal(f, literal) f &gt; literal PredicateBuilder.greater_than(f, literal) f &gt;= literal PredicateBuilder.greater_or_equal(f, literal) f is null PredicateBuilder.is_null(f) f is not null PredicateBuilder.is_not_null(f) f.startswith(literal) PredicateBuilder.startswith(f, literal) f.endswith(literal) PredicateBuilder.endswith(f, literal) f.contains(literal) PredicateBuilder.contains(f, literal) f is in [l1, l2] PredicateBuilder.is_in(f, [l1, l2]) f is not in [l1, l2] PredicateBuilder.is_not_in(f, [l1, l2]) lower &lt;= f &lt;= upper PredicateBuilder.between(f, lower, upper) `}),e.add({id:72,href:"/concepts/rest/",title:"RESTCatalog",section:"概念 Concepts",content:``}),e.add({id:73,href:"/ecosystem/trino/",title:"Trino",section:"Ecosystem",content:`Trino#This documentation is a guide for using Paimon in Trino.
Version#Paimon currently supports Trino 440.
Filesystem#From version 0.8, Paimon share Trino filesystem for all actions, which means, you should config Trino filesystem before using trino-paimon. You can find information about how to config filesystems for Trino on Trino official website.
Preparing Paimon Jar File#Download
You can also manually build a bundled jar from the source code. However, there are a few preliminary steps that need to be taken before compiling:
To build from the source code, clone the git repository. Install JDK21 locally, and configure JDK21 as a global environment variable; Then,you can build bundled jar with the following command:
mvn clean install -DskipTests You can find Trino connector jar in ./paimon-trino-&lt;trino-version&gt;/target/paimon-trino-&lt;trino-version&gt;-1.2.0-plugin.tar.gz.
We use hadoop-apache as a dependency for Hadoop, and the default Hadoop dependency typically supports both Hadoop 2 and Hadoop 3. If you encounter an unsupported scenario, you can specify the corresponding Apache Hadoop version.
For example, if you want to use Hadoop 3.3.5-1, you can use the following command to build the jar:
mvn clean install -DskipTests -Dhadoop.apache.version=3.3.5-1 Configure Paimon Catalog#Install Paimon Connector#tar -zxf paimon-trino-&lt;trino-version&gt;-1.2.0-plugin.tar.gz -C \${TRINO_HOME}/plugin NOTE: For JDK 21, when Deploying Trino, should add jvm options: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED
Configure#Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:
connector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:
set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure hadoop-conf-dir in the properties. If you are using a Hadoop filesystem, you can still use trino-hdfs and trino-hive to config it. For example, if you use oss as a storage, you can write in paimon.properties according to Trino Reference:
hive.config.resources=/path/to/core-site.xml Then, config core-site.xml according to Jindo Reference
Kerberos#You can configure kerberos keytab file when using KERBEROS authentication in the properties.
security.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/trino/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Trino.
Create Schema#CREATE SCHEMA paimon.test_db; Create Table#CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = &#39;ORC&#39;, primary_key = ARRAY[&#39;order_key&#39;,&#39;order_date&#39;], partitioned_by = ARRAY[&#39;order_date&#39;], bucket = &#39;2&#39;, bucket_key = &#39;order_key&#39;, changelog_producer = &#39;input&#39; ); Add Column#CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = &#39;ORC&#39;, primary_key = ARRAY[&#39;order_key&#39;,&#39;order_date&#39;], partitioned_by = ARRAY[&#39;order_date&#39;], bucket = &#39;2&#39;, bucket_key = &#39;order_key&#39;, changelog_producer = &#39;input&#39; ); ALTER TABLE paimon.test_db.orders ADD COLUMN shipping_address varchar; Query#SELECT * FROM paimon.test_db.orders; Query with Time Traveling#-- read the snapshot from specified timestamp SELECT * FROM t FOR TIMESTAMP AS OF TIMESTAMP &#39;2023-01-01 00:00:00 Asia/Shanghai&#39;; -- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t FOR VERSION AS OF 1; -- read tag &#39;my-tag&#39; SELECT * FROM t FOR VERSION AS OF &#39;my-tag&#39;; If tag&rsquo;s name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if you have a tag named &lsquo;1&rsquo; based on snapshot 2, the statement SELECT * FROM paimon.test_db.orders FOR VERSION AS OF '1' actually queries snapshot 2 instead of snapshot 1.Insert#INSERT INTO paimon.test_db.orders VALUES (.....); Supports:
primary key table with fixed bucket. non-primary-key table with bucket -1. Trino to Paimon type mapping#This section lists all supported type conversion between Trino and Paimon. All Trino&rsquo;s data types are available in package io.trino.spi.type.
Trino Data TypePaimon Data TypeAtomic TypeRowTypeRowTypefalseMapTypeMapTypefalseArrayTypeArrayTypefalseBooleanTypeBooleanTypetrueTinyintTypeTinyIntTypetrueSmallintTypeSmallIntTypetrueIntegerTypeIntTypetrueBigintTypeBigIntTypetrueRealTypeFloatTypetrueDoubleTypeDoubleTypetrueCharType(length)CharType(length)trueVarCharType(VarCharType.MAX_LENGTH)VarCharType(VarCharType.MAX_LENGTH)trueVarCharType(length)VarCharType(length), length is less than VarCharType.MAX_LENGTHtrueDateTypeDateTypetrueTimestampTypeTimestampTypetrueDecimalType(precision, scale)DecimalType(precision, scale)trueVarBinaryType(length)VarBinaryType(length)trueTimestampWithTimeZoneTypeLocalZonedTimestampTypetrueTmp Dir#Paimon will unzip some jars to the tmp directory for codegen. By default, Trino will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.
You can configure this environment variable when Trino starts:
-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.
`}),e.add({id:74,href:"/concepts/spec/datafile/",title:"数据文件 DataFile",section:"规范说明 Specification",content:`数据文件 DataFile#Partition#通过 Flink SQL 创建分区表示例：
CREATE TABLE part_t ( f0 INT, f1 STRING, dt STRING ) PARTITIONED BY (dt); INSERT INTO part_t VALUES (1, &#39;11&#39;, &#39;20240514&#39;); 文件系统结构如下：
part_t ├── dt=20240514 │ └── bucket-0 │ └── data-ca1c3c38-dc8d-4533-949b-82e195b41bd4-0.orc ├── manifest │ ├── manifest-08995fe5-c2ac-4f54-9a5f-d3af1fcde41d-0 │ ├── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-0 │ └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 Paimon 采用与 Apache Hive 相同的分区概念来划分数据。分区的数据文件会被放置在独立的分区目录中。
Bucket#所有 Paimon 表的存储都依赖于桶（bucket），数据文件存储在桶目录中。Paimon 中各种表类型与桶的关系如下：
主键表（Primary Key Table）： bucket = -1：默认模式，动态桶模式，通过索引文件记录键对应的桶。索引记录主键哈希值与桶的对应关系。 bucket = 10：数据根据桶键的哈希值（默认是主键）分布到对应的桶中。 追加表（Append Table）： bucket = -1：默认模式，忽略桶的概念，虽然所有数据写入桶 0，但读写的并行度不受限制。 bucket = 10：需要定义桶键，数据根据桶键的哈希值分布到对应的桶中。 Data File#数据文件的命名格式为 data-\${uuid}-\${id}.\${format}。对于追加表，文件存储的是表的数据，不包含任何新增列。但对于主键表，每条数据行会存储额外的系统列：
带主键的表数据文件 Table with Primary key Data File#主键列，带有 _KEY_ 前缀，以避免与表中的列名冲突。这是可选的，Paimon 1.0 及以上版本会从 value_columns 中获取主键字段。 _VALUE_KIND：TINYINT，表示行是被删除还是被添加。类似 RocksDB，每行数据可以被删除或添加，用于更新主键表。 _SEQUENCE_NUMBER：BIGINT，该数字用于更新时比较，判断数据的先后顺序。 值列，表中声明的所有列。 例如，表的数据文件示例：
CREATE TABLE T ( a INT PRIMARY KEY NOT ENFORCED, b INT, c INT ); 该文件有 6 列：_KEY_a、_VALUE_KIND、_SEQUENCE_NUMBER、a、b、c。
当启用 data-file.thin-mode 时，文件有 5 列：_VALUE_KIND、_SEQUENCE_NUMBER、a、b、c。
Table w/o Primary key Data File#值列：表中声明的所有列。 例如，表的数据文件：
CREATE TABLE T ( a INT, b INT, c INT ); 该文件有 3 列：a、b、c。
变更日志文件 Changelog File#变更日志文件与数据文件完全相同，仅在主键表中生效。它类似于数据库中的 Binlog，用于记录表中数据的变更。
`}),e.add({id:75,href:"/ecosystem/amoro/",title:"Amoro",section:"Ecosystem",content:`Apache Amoro With Paimon#Apache Amoro(incubating) is a Lakehouse management system built on open data lake formats. Working with compute engines including Flink, Spark, and Trino, Amoro brings pluggable and Table Maintenance features for a Lakehouse to provide out-of-the-box data warehouse experience, and helps data platforms or products easily build infra-decoupled, stream-and-batch-fused and lake-native architecture. AMS(Amoro Management Service) provides Lakehouse management features, like self-optimizing, data expiration, etc. It also provides a unified catalog service for all compute engines, which can also be combined with existing metadata services like HMS(Hive Metastore).
Table Format#Apache Amoro supports all catalog types supported by paimon, including common catalog: Hadoop, Hive, Glue, JDBC, Nessie and other third-party catalog. Amoro supports all storage types supported by Paimon, including common store: Hadoop, S3, GCS, ECS, OSS, and so on.
In the future, Paimon automatic optimization strategy will be supported, and users can achieve the best balance experience by cooperating with Amoro automatic optimization
`}),e.add({id:76,href:"/cdc-ingestion/debezium-bson/",title:"Debezium BSON",section:"CDC Ingestion",content:`Debezium BSON Format#The debezium-bson format is one of the formats supported by Kafka CDC. It is the format obtained by collecting mongodb through debezium, which is similar to debezium-json format. However, MongoDB does not have a fixed schema, and the field types of each document may be different, so the before/after fields in JSON are all string types, while the debezium-json format requires a JSON object type.
Prepare MongoDB BSON Jar#Can be downloaded from the Maven repository
bson-*.jar Introduction#The debezium bson format requires insert/update/delete event messages include the full document, and include a field that represents the state of the document before the change. This requires setting debezium&rsquo;s capture.mode to change_streams_update_full_with_pre_image and capture.mode.full.update.type to post_image. Before version 6.0 of MongoDB, it was not possible to obtain &lsquo;Update Before&rsquo; information. Therefore, using the id field in the Kafka Key as &lsquo;Update before&rsquo; informationHere is a simple example for an update operation captured from a Mongodb customers collection in JSON format:
{ &#34;schema&#34;: { &#34;type&#34;: &#34;struct&#34;, &#34;fields&#34;: [ { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;name&#34;: &#34;io.debezium.data.Json&#34;, &#34;version&#34;: 1, &#34;field&#34;: &#34;before&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;optional&#34;: true, &#34;name&#34;: &#34;io.debezium.data.Json&#34;, &#34;version&#34;: 1, &#34;field&#34;: &#34;after&#34; }, ... ] }, &#34;payload&#34;: { &#34;before&#34;: &#34;{\\&#34;_id\\&#34;: {\\&#34;$oid\\&#34; : \\&#34;596e275826f08b2730779e1f\\&#34;}, \\&#34;name\\&#34; : \\&#34;Anne\\&#34;, \\&#34;create_time\\&#34; : {\\&#34;$numberLong\\&#34; : \\&#34;1558965506000\\&#34;}, \\&#34;tags\\&#34;:[\\&#34;success\\&#34;]}&#34;, &#34;after&#34;: &#34;{\\&#34;_id\\&#34;: {\\&#34;$oid\\&#34; : \\&#34;596e275826f08b2730779e1f\\&#34;}, \\&#34;name\\&#34; : \\&#34;Anne\\&#34;, \\&#34;create_time\\&#34; : {\\&#34;$numberLong\\&#34; : \\&#34;1558965506000\\&#34;}, \\&#34;tags\\&#34;:[\\&#34;passion\\&#34;,\\&#34;success\\&#34;]}&#34;, &#34;source&#34;: { &#34;db&#34;: &#34;inventory&#34;, &#34;rs&#34;: &#34;rs0&#34;, &#34;collection&#34;: &#34;customers&#34;, ... }, &#34;op&#34;: &#34;u&#34;, &#34;ts_ms&#34;: 1558965515240, &#34;ts_us&#34;: 1558965515240142, &#34;ts_ns&#34;: 1558965515240142879 } } This document from the MongoDB collection customers has 4 columns, the _id is a BSON ObjectID, name is a string, create_time is a long, tags is an array of string. The following is the processing result in debezium-bson format:
Document Schema:
Field Name Field Type Key _id STRING Primary Key name STRING create_time STRING tags STRING Records:
RowKind _id name create_time tags -U 596e275826f08b2730779e1f Anne 1558965506000 [&ldquo;success&rdquo;] +U 596e275826f08b2730779e1f Anne 1558965506000 [&ldquo;passion&rdquo;,&ldquo;success&rdquo;] How it works#Because the schema field of the event message does not have the field information of the document, the debezium-bson format does not require event messages to have schema information. The specific operations are as follows:
Parse the before/after fields of the event message into BSONDocument. Recursive traversal all fields of BSONDocument and convert BsonValue to Java Object. All top-level fields of before/after are converted to string type, and _id is fixed to primary key If the top-level fields of before/after is a basic type(such as Integer/Long, etc.), it is directly converted to a string, if not, it is converted to a JSON string Below is a list of top-level field BsonValue conversion examples:
BsonValue TypeJson ValueConversion Result StringBsonString"hello""hello"BsonInt32123"123"BsonInt641735934393769{"$numberLong": "1735934393769"}"1735934393769"BsonDouble{"$numberDouble": "3.14"}{"$numberDouble": "NaN"}{"$numberDouble": "Infinity"}{"$numberDouble": "-Infinity"}"3.14""NaN""Infinity""-Infinity"BsonBooleantruefalse"true""false"BsonArray[1,2,{"$numberLong": "1735934393769"}]"[1,2,1735934393769]"BsonObjectId{"$oid": "596e275826f08b2730779e1f"}"596e275826f08b2730779e1f"BsonDateTime{"$date": 1735934393769 }"1735934393769"BsonNullnullnullBsonUndefined{"$undefined": true}nullBsonBinary{"$binary": "uE2/4v5MSVOiJZkOo3APKQ==", "$type": "0"}"uE2/4v5MSVOiJZkOo3APKQ=="BsonBinary(type=UUID){"$binary": "uE2/4v5MSVOiJZkOo3APKQ==", "$type": "4"}"b84dbfe2-fe4c-4953-a225-990ea3700f29"BsonDecimal128{"$numberDecimal": "3.14"}{"$numberDecimal": "NaN"}"3.14""NaN"BsonRegularExpression{"$regularExpression": {"pattern": "^pass$", "options": "i"}}"/^pass$/i"BsonSymbol{"$symbol": "symbol"}"symbol"BsonTimestamp{"$timestamp": {"t": 1736997330, "i": 2}}"1736997330"BsonMinKey{"$minKey": 1}"BsonMinKey"BsonMaxKey{"$maxKey": 1}"BsonMaxKey"BsonJavaScript{"$code": "function(){}"}"function(){}"BsonJavaScriptWithScope{"$code": "function(){}", "$scope": {"name": "Anne"}}'{"$code": "function(){}", "$scope": {"name": "Anne"}}'BsonDocument{"decimalPi": {"$numberDecimal": "3.14"},"doublePi": {"$numberDouble": "3.14"},"doubleNaN": {"$numberDouble": "NaN"},"decimalNaN": {"$numberDecimal": "NaN"},"long": {"$numberLong": "100"},"bool": true,"array": [{"$numberInt": "1"},{"$numberLong": "2"}]}'{"decimalPi":3.14,"doublePi":3.14,"doubleNaN":"NaN","decimalNaN":"NaN","long":100,"bool":true,"array":[1,2]}'How to use#Use debezium-bson by adding the kafka_conf parameter value.format=debezium-bson. Let’s take table synchronization as an example:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table ods_mongodb_customers \\ --primary_keys _id \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=customers \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=debezium-bson \\ --catalog_conf metastore=filesystem \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 `}),e.add({id:77,href:"/iceberg/ecosystem/",title:"Ecosystem",section:"Iceberg Metadata",content:`Iceberg Ecosystems#AWS Athena#AWS Athena may use old manifest reader to read Iceberg manifest by names, we should let Paimon producing legacy Iceberg manifest list file, you can enable: 'metadata.iceberg.manifest-legacy-version'.
DuckDB#Duckdb may rely on files placed in the root/data directory, while Paimon is usually placed directly in the root directory, so you can configure this parameter for the table to achieve compatibility: 'data-file.path-directory' = 'data'.
Trino Iceberg#In this example, we use Trino Iceberg connector to access Paimon table through Iceberg Hive catalog. Before trying out this example, make sure that you have configured Trino Iceberg connector. See Trino&rsquo;s document for more information.
Let&rsquo;s first create a Paimon table with Iceberg compatibility enabled.
Flink SQLCREATE CATALOG paimon_catalog WITH ( &#39;type&#39; = &#39;paimon&#39;, &#39;warehouse&#39; = &#39;&lt;path-to-warehouse&gt;&#39; ); CREATE TABLE paimon_catalog.\`default\`.animals ( kind STRING, name STRING ) WITH ( &#39;metadata.iceberg.storage&#39; = &#39;hive-catalog&#39;, &#39;metadata.iceberg.uri&#39; = &#39;thrift://&lt;host&gt;:&lt;port&gt;&#39; ); INSERT INTO paimon_catalog.\`default\`.animals VALUES (&#39;mammal&#39;, &#39;cat&#39;), (&#39;mammal&#39;, &#39;dog&#39;), (&#39;reptile&#39;, &#39;snake&#39;), (&#39;reptile&#39;, &#39;lizard&#39;); Spark SQLStart spark-sql with the following command line.
spark-sql --jars &lt;path-to-paimon-jar&gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=&lt;path-to-warehouse&gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-&lt;iceberg-version&gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=&lt;path-to-warehouse&gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.
CREATE TABLE paimon_catalog.\`default\`.animals ( kind STRING, name STRING ) TBLPROPERTIES ( &#39;metadata.iceberg.storage&#39; = &#39;hive-catalog&#39;, &#39;metadata.iceberg.uri&#39; = &#39;thrift://&lt;host&gt;:&lt;port&gt;&#39; ); INSERT INTO paimon_catalog.\`default\`.animals VALUES (&#39;mammal&#39;, &#39;cat&#39;), (&#39;mammal&#39;, &#39;dog&#39;), (&#39;reptile&#39;, &#39;snake&#39;), (&#39;reptile&#39;, &#39;lizard&#39;); Start Trino using Iceberg catalog and query from Paimon table.
SELECT * FROM animals WHERE class = &#39;mammal&#39;; /* kind | name --------+------ mammal | cat mammal | dog */ `}),e.add({id:78,href:"/spark/",title:"Engine Spark",section:"Apache Paimon",content:``}),e.add({id:79,href:"/primary-key-table/sequence-rowkind/",title:"Sequence & Rowkind",section:"Table with PK",content:`Sequence and Rowkind#When creating a table, you can specify the 'sequence.field' by specifying fields to determine the order of updates, or you can specify the 'rowkind.field' to determine the changelog kind of record.
Sequence Field#By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:
FlinkCREATE TABLE my_table ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, update_time TIMESTAMP ) WITH ( &#39;sequence.field&#39; = &#39;update_time&#39; ); The record with the largest sequence.field value will be the last to merge, if the values are the same, the input order will be used to determine which one is the last one. sequence.field supports fields of all data types.
You can define multiple fields for sequence.field, for example 'update_time,flag', multiple fields will be compared in order.
User defined sequence fields conflict with features such as first_row and first_value, which may result in unexpected results.Row Kind Field#By default, the primary key table determines the row kind according to the input row. You can also define the 'rowkind.field' to use a field to extract row kind.
The valid row kind string should be '+I', '-U', '+U' or '-D'.
`}),e.add({id:80,href:"/spark/sql-alter/",title:"SQL Alter",section:"Engine Spark",content:`Altering Tables#Changing/Adding Table Properties#The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET TBLPROPERTIES ( &#39;write-buffer-size&#39; = &#39;256 MB&#39; ); Removing Table Properties#The following SQL removes write-buffer-size table property.
ALTER TABLE my_table UNSET TBLPROPERTIES (&#39;write-buffer-size&#39;); Changing/Adding Table Comment#The following SQL changes comment of table my_table to table comment.
ALTER TABLE my_table SET TBLPROPERTIES ( &#39;comment&#39; = &#39;table comment&#39; ); Removing Table Comment#The following SQL removes table comment.
ALTER TABLE my_table UNSET TBLPROPERTIES (&#39;comment&#39;); Rename Table Name#The following SQL rename the table name to new name.
The simplest sql to call is:
ALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:
ALTER TABLE [catalog.[database.]]test1 RENAME to [database.]test2; But we can&rsquo;t put catalog name before the renamed-to table, it will throw an error if we write sql like this:
ALTER TABLE catalog.database.test1 RENAME to catalog.database.test2; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.Adding New Columns#The following SQL adds two columns c1 and c2 to table my_table.
ALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING ); The following SQL adds a nested column f3 to a struct type.
-- column v previously has type STRUCT&lt;f1: STRING, f2: INT&gt; ALTER TABLE my_table ADD COLUMN v.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the element type of an array type.
-- column v previously has type ARRAY&lt;STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table ADD COLUMN v.element.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the value type of a map type.
-- column v previously has type MAP&lt;INT, STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table ADD COLUMN v.value.f3 STRING; Renaming Column Name#The following SQL renames column c0 in table my_table to c1.
ALTER TABLE my_table RENAME COLUMN c0 TO c1; The following SQL renames a nested column f1 to f100 in a struct type.
-- column v previously has type STRUCT&lt;f1: STRING, f2: INT&gt; ALTER TABLE my_table RENAME COLUMN v.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the element type of an array type.
-- column v previously has type ARRAY&lt;STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table RENAME COLUMN v.element.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the value type of a map type.
-- column v previously has type MAP&lt;INT, STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table RENAME COLUMN v.value.f1 to f100; Dropping Columns#The following SQL drops two columns c1 and c2 from table my_table.
ALTER TABLE my_table DROP COLUMNS (c1, c2); The following SQL drops a nested column f2 from a struct type.
-- column v previously has type STRUCT&lt;f1: STRING, f2: INT&gt; ALTER TABLE my_table DROP COLUMN v.f2; The following SQL drops a nested column f2 from a struct type, which is the element type of an array type.
-- column v previously has type ARRAY&lt;STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table DROP COLUMN v.element.f2; The following SQL drops a nested column f2 from a struct type, which is the value type of a map type.
-- column v previously has type MAP&lt;INT, STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table DROP COLUMN v.value.f2; In hive catalog, you need to ensure:
disable hive.metastore.disallow.incompatible.col.type.changes in your hive server or spark-sql --conf spark.hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your spark. Otherwise this operation may fail, throws an exception like The following columns have types incompatible with theexisting columns in their respective positions.
Dropping Partitions#The following SQL drops the partitions of the paimon table. For spark sql, you need to specify all the partition columns.
ALTER TABLE my_table DROP PARTITION (\`id\` = 1, \`name\` = &#39;paimon&#39;); Changing Column Comment#The following SQL changes comment of column buy_count to buy count.
ALTER TABLE my_table ALTER COLUMN buy_count COMMENT &#39;buy count&#39;; Adding Column Position#ALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b; Changing Column Position#ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b; Changing Column Type#ALTER TABLE my_table ALTER COLUMN col_a TYPE DOUBLE; The following SQL changes the type of a nested column f2 to BIGINT in a struct type.
-- column v previously has type STRUCT&lt;f1: STRING, f2: INT&gt; ALTER TABLE my_table ALTER COLUMN v.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the element type of an array type.
-- column v previously has type ARRAY&lt;STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table ALTER COLUMN v.element.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the value type of a map type.
-- column v previously has type MAP&lt;INT, STRUCT&lt;f1: STRING, f2: INT&gt;&gt; ALTER TABLE my_table ALTER COLUMN v.value.f2 TYPE BIGINT; ALTER DATABASE#The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.
ALTER { DATABASE | SCHEMA | NAMESPACE } my_database SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] ) Altering Database Location#The following SQL sets the location of the specified database to file:/temp/my_database.db.
ALTER DATABASE my_database SET LOCATION &#39;file:/temp/my_database.db&#39; `}),e.add({id:81,href:"/flink/sql-lookup/",title:"SQL Lookup",section:"Engine Flink",content:`Lookup Joins#Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.
Paimon supports lookup joins on tables with primary keys and append tables in Flink. The following example illustrates this feature.
Prepare#First, let&rsquo;s create a Paimon table and update it in real-time.
-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( &#39;type&#39;=&#39;paimon&#39;, &#39;warehouse&#39;=&#39;hdfs://nn:8020/warehouse/path&#39; -- or &#39;file://tmp/foo/bar&#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( &#39;connector&#39; = &#39;kafka&#39;, &#39;topic&#39; = &#39;...&#39;, &#39;properties.bootstrap.servers&#39; = &#39;...&#39;, &#39;format&#39; = &#39;csv&#39; ... ); Normal Lookup#You can now use customers in a lookup join query.
-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Retry Lookup#If the records of orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink&rsquo;s Delayed Retry Strategy For Lookup. Only for Flink 1.16+.
-- enrich each order with customer information SELECT /*+ LOOKUP(&#39;table&#39;=&#39;c&#39;, &#39;retry-predicate&#39;=&#39;lookup_miss&#39;, &#39;retry-strategy&#39;=&#39;fixed_delay&#39;, &#39;fixed-delay&#39;=&#39;1s&#39;, &#39;max-attempts&#39;=&#39;600&#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Async Retry Lookup#The problem with synchronous retry is that one record will block subsequent records, causing the entire job to be blocked. You can consider using async + allow_unordered to avoid blocking, the records that join missing will no longer block other records.
-- enrich each order with customer information SELECT /*+ LOOKUP(&#39;table&#39;=&#39;c&#39;, &#39;retry-predicate&#39;=&#39;lookup_miss&#39;, &#39;output-mode&#39;=&#39;allow_unordered&#39;, &#39;retry-strategy&#39;=&#39;fixed_delay&#39;, &#39;fixed-delay&#39;=&#39;1s&#39;, &#39;max-attempts&#39;=&#39;600&#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(&#39;lookup.async&#39;=&#39;true&#39;, &#39;lookup.async-thread-number&#39;=&#39;16&#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; If the main table (orders) is CDC stream, allow_unordered will be ignored by Flink SQL (only supports append stream), your streaming job may be blocked. You can try to use audit_log system table feature of Paimon to walk around (convert CDC stream to append stream).Large Scale Lookup (Fixed Bucket)#By default, each Flink subtask would store a whole copy of the lookup table. If the amount of data in customers (lookup table) is too large for a single subtask, you can enable the shuffle lookup optimization as follows (For Flink 2.0+ and fixed-bucket Paimon table). This optimization enables sending data of the same bucket to designated subtask(s), so each Flink subtask would only need to store a part of the whole data.
-- enrich each order with customer information SELECT /*+ LOOKUP(&#39;table&#39;=&#39;c&#39;, &#39;shuffle&#39;=&#39;true&#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Dynamic Partition#In traditional data warehouses, each partition often maintains the latest full data, so this partition table only needs to join the latest partition. Paimon has specifically developed the max_pt feature for this scenario.
Create Paimon Partitioned Table
CREATE TABLE customers ( id INT, name STRING, country STRING, zip STRING, dt STRING, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt); Lookup Join
SELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(&#39;scan.partitions&#39;=&#39;max_pt()&#39;, &#39;lookup.dynamic-partition.refresh-interval&#39;=&#39;1 h&#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The Lookup node will automatically refresh the latest partition and query the data of the latest partition.
The option scan.partitions can also specify fixed partitions in the form of key1=value1,key2=value2. Multiple partitions should be separated by semicolon (;). When specifying fixed partitions, this option can also be used in batch joins.
Query Service#You can run a Flink Streaming Job to start query service for the table. When QueryService exists, Flink Lookup Join will prioritize obtaining data from it, which will effectively improve query performance.
Flink SQLCALL sys.query_service(&#39;database_name.table_name&#39;, parallelism); Flink Action&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ query_service \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--parallelism &lt;parallelism&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] `}),e.add({id:82,href:"/concepts/table-types/",title:"表类型 Table Types",section:"概念 Concepts",content:`表类型#Paimon 支持的表类型：#table with pk: 带主键的 Paimon 数据表 table w/o pk: 不带主键的 Paimon 数据表 view: 需要 metastore 的 SQL 视图，是一种虚拟表 format-table: 文件格式表指向一个包含多个相同格式文件的目录，对该表的操作允许读取或写入这些文件，兼容 Hive 表 object table: 为指定对象存储目录中的非结构化数据对象提供元数据索引 materialized-table: 用于简化批处理和流处理数据管道，提供一致的开发体验，详见 Flink Materialized Table Table with PK#See Paimon with Primary key.
主键由一组列组成，每条记录的这些列的值都是唯一的。Paimon 通过在每个 bucket 内对主键排序来强制数据有序，从而支持流式更新和流式变更日志读取。
主键的定义类似于标准 SQL，它确保在批量查询时，对于相同的主键只存在一条数据记录。
Flink SQLCREATE TABLE my_table ( a INT PRIMARY KEY NOT ENFORCED, b STRING ) WITH ( &#39;bucket&#39;=&#39;8&#39; ) Spark SQLCREATE TABLE my_table ( a INT, b STRING ) TBLPROPERTIES ( &#39;primary-key&#39; = &#39;a&#39;, &#39;bucket&#39; = &#39;8&#39; ) Table w/o PK#See Paimon w/o Primary key.
如果表没有定义主键，则称为追加表。相比于带主键的表，它无法直接接收变更日志，不能通过流式 upsert 直接更新数据，只能接收追加的数据。
但是，它同样支持批量 SQL 操作：DELETE、UPDATE 和 MERGE-INTO。
CREATE TABLE my_table ( a INT, b STRING ) View#当元数据存储（metastore）支持视图时（例如 Hive metastore），才支持 View。如果没有 metastore，则只能使用临时视图（temporary View），它仅存在于当前会话中。本章节主要介绍持久视图（persistent views）。
当前视图会保存原始的 SQL。如果需要跨引擎使用视图，可以编写跨引擎的 SQL 语句。例如：
Flink SQLCREATE VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS; SHOW CREATE VIEW my_view; Spark SQLCREATE [OR REPLACE] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS; Format Table#当元数据存储（metastore）支持格式化表（format table）时，例如 Hive metastore，才支持格式化表。元数据存储中的 Hive 表会映射为 Paimon 的格式化表，供计算引擎（Spark、Hive、Flink）进行读写。
格式化表指向一个包含多个相同格式文件的目录，对该表的操作允许对这些文件进行读写，方便检索已有数据和添加新文件。
分区文件格式表类似于标准 Hive 格式。分区会根据目录结构自动发现和推断。
格式化表默认启用，你可以通过配置 Catalog 选项 'format-table.enabled' 来禁用它。
目前仅支持 CSV、Parquet、ORC、JSON 格式。
Flink-CSVCREATE TABLE my_csv_table ( a INT, b STRING ) WITH ( &#39;type&#39;=&#39;format-table&#39;, &#39;file.format&#39;=&#39;csv&#39;, &#39;field-delimiter&#39;=&#39;,&#39; ) Spark-CSVCREATE TABLE my_csv_table ( a INT, b STRING ) USING csv OPTIONS (&#39;field-delimiter&#39; &#39;,&#39;) Flink-ParquetCREATE TABLE my_parquet_table ( a INT, b STRING ) WITH ( &#39;type&#39;=&#39;format-table&#39;, &#39;file.format&#39;=&#39;parquet&#39; ) Spark-ParquetCREATE TABLE my_parquet_table ( a INT, b STRING ) USING parquet Flink-JSONCREATE TABLE my_json_table ( a INT, b STRING ) WITH ( &#39;type&#39;=&#39;format-table&#39;, &#39;file.format&#39;=&#39;json&#39; ) Spark-JSONCREATE TABLE my_json_table ( a INT, b STRING ) USING json Object Table#对象表是指定对象存储目录中非结构化数据对象的虚拟表。用户可以：
使用虚拟文件系统（开发中）读写文件。 或者使用 SQL 计算引擎将其作为结构化文件列表进行读取。 对象表由 Catalog 管理，也可以设置访问权限。目前仅 REST Catalog 支持对象表。
创建对象表的方法：
Flink-SQLCREATE TABLE \`my_object_table\` WITH ( &#39;type&#39; = &#39;object-table&#39; ); Spark-SQLCREATE TABLE \`my_object_table\` TBLPROPERTIES ( &#39;type&#39; = &#39;object-table&#39; ); Materialized Table#物化表旨在简化批处理和流处理数据管道，提供一致的开发体验，详见 Flink 物化表。
目前只有 Flink SQL 集成了物化表，我们计划未来也支持 Spark SQL。
CREATE MATERIALIZED TABLE continuous_users_shops PARTITIONED BY (ds) FRESHNESS = INTERVAL &#39;30&#39; SECOND AS SELECT user_id, ds, SUM (payment_amount_cents) AS payed_buy_fee_sum, SUM (1) AS PV FROM ( SELECT user_id, order_created_at AS ds, payment_amount_cents FROM json_source ) AS tmp GROUP BY user_id, ds; `}),e.add({id:83,href:"/concepts/spec/tableindex/",title:"表索引 Table Index",section:"规范说明 Specification",content:`表索引 Table index#表索引文件存放在 index 目录中。
Dynamic Bucket Index#动态桶索引用于存储主键哈希值与桶的对应关系。
其结构非常简单，文件中只存储哈希值：
HASH_VALUE | HASH_VALUE | HASH_VALUE | HASH_VALUE | &hellip;
HASH_VALUE 是主键的哈希值，4 字节，采用大端（BIG_ENDIAN）存储。
Deletion Vectors#Deletion file 用于存储每个数据文件中被删除记录的位置。主键表中每个桶对应一个 Deletion file 。
Deletion file 是二进制文件，格式如下：
首先，用 1 字节记录版本号，当前版本为 1。 然后，依次记录 &lt;序列化位图大小、序列化位图、序列化位图的校验和&gt;。 大小和校验和均为大端（BIG_ENDIAN）整数。 每个序列化位图的序列化格式由 deletion-vectors.bitmap64 决定。 Paimon 默认使用 32 位位图存储删除记录，但如果设置了 deletion-vectors.bitmap64 为 true，则使用 64 位位图。 两种位图的序列化方式不同。注意，只有 64 位位图实现与 Iceberg 兼容。
32 位位图的序列化（默认）：
先用一个大端整数（BIG_ENDIAN）记录一个固定魔数，当前魔数为 1581511376。 然后记录一个 32 位序列化位图，即一个 RoaringBitmap（org.roaringbitmap.RoaringBitmap）。 64 位位图的序列化：
先用一个小端整数（LITTLE_ENDIAN）记录一个固定魔数，当前魔数为 1681511377。 然后记录一个 64 位序列化位图。该位图支持正的 64 位位置（最高位必须为 0）， 但针对大多数位置位于 32 位以内的情况进行了优化，使用了多个 32 位 Roaring 位图组成的数组。内部位图数组会根据最大位置动态扩展。 64 位位图的序列化流程如下： 先用一个小端长整型（LITTLE_ENDIAN）记录位图数组的大小。 然后依次记录每个位图的索引（小端整型）和该位图的序列化字节。 `}),e.add({id:84,href:"/spark/auxiliary/",title:"Auxiliary",section:"Engine Spark",content:`Auxiliary Statements#Set / Reset#The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values.
To set dynamic options globally, you need add the spark.paimon. prefix. You can also set dynamic table options at this format: spark.paimon.\${catalogName}.\${dbName}.\${tableName}.\${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. Dynamic table options will override global options if there are conflicts.
-- set spark conf SET spark.sql.sources.partitionOverwriteMode=dynamic; -- set paimon conf SET spark.paimon.file.block-size=512M; -- reset conf RESET spark.paimon.file.block-size; -- set scan.snapshot-id=1 for the table default.T in any catalogs SET spark.paimon.*.default.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=1 for the table T in any databases and catalogs SET spark.paimon.*.*.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=2 for the table default.T1 in any catalogs and scan.snapshot-id=1 on other tables SET spark.paimon.scan.snapshot-id=1; SET spark.paimon.*.default.T1.scan.snapshot-id=2; SELECT * FROM default.T1 JOIN default.T2 ON xxxx; Describe table#DESCRIBE TABLE statement returns the basic metadata information of a table or view. The metadata information includes column name, column type and column comment.
-- describe table or view DESCRIBE TABLE my_table; -- describe table or view with additional metadata DESCRIBE TABLE EXTENDED my_table; Show create table#SHOW CREATE TABLE returns the CREATE TABLE statement or CREATE VIEW statement that was used to create a given table or view.
SHOW CREATE TABLE my_table; Show columns#Returns the list of columns in a table. If the table does not exist, an exception is thrown.
SHOW COLUMNS FROM my_table; Show partitions#The SHOW PARTITIONS statement is used to list partitions of a table. An optional partition spec may be specified to return the partitions matching the supplied partition spec.
-- Lists all partitions for my_table SHOW PARTITIONS my_table; -- Lists partitions matching the supplied partition spec for my_table SHOW PARTITIONS my_table PARTITION (dt=&#39;20230817&#39;); Show Table Extended#The SHOW TABLE EXTENDED statement is used to list table or partition information.
-- Lists tables that satisfy regular expressions SHOW TABLE EXTENDED IN db_name LIKE &#39;test*&#39;; -- Lists the specified partition information for the table SHOW TABLE EXTENDED IN db_name LIKE &#39;table_name&#39; PARTITION(pt = &#39;2024&#39;); Show views#The SHOW VIEWS statement returns all the views for an optionally specified database.
-- Lists all views SHOW VIEWS; -- Lists all views that satisfy regular expressions SHOW VIEWS LIKE &#39;test*&#39;; Analyze table#The ANALYZE TABLE statement collects statistics about the table, that are to be used by the query optimizer to find a better query execution plan. Paimon supports collecting table-level statistics and column statistics through analyze.
-- collect table-level statistics ANALYZE TABLE my_table COMPUTE STATISTICS; -- collect table-level statistics and column statistics for col1 ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1; -- collect table-level statistics and column statistics for all columns ANALYZE TABLE my_table COMPUTE STATISTICS FOR ALL COLUMNS; `}),e.add({id:85,href:"/primary-key-table/compaction/",title:"Compaction",section:"Table with PK",content:`Compaction#When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.
To limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.
However, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adopts a compaction strategy similar to Rocksdb&rsquo;s universal compaction.
Compaction solves:
Reduce Level 0 files to avoid poor query performance. Produce changelog via changelog-producer. Produce deletion vectors for MOW mode. Snapshot Expiration, Tag Expiration, Partitions Expiration. Limitation:
There can only be one job working on the same partition&rsquo;s compaction, otherwise it will cause conflicts and one side will throw an exception failure. Writing performance is almost always affected by compaction, so its tuning is crucial.
Asynchronous Compaction#Compaction is inherently asynchronous, but if you want it to be completely asynchronous without blocking writes, expecting a mode for maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:
num-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 lookup-wait = false This configuration will generate more files during peak write periods and gradually merge them for optimal read performance during low write periods.
Dedicated compaction job#In general, if you expect multiple jobs to be written to the same table, you need to separate the compaction. You can use dedicated compaction job.
Record-Level expire#In compaction, you can configure record-Level expire time to expire records, you should configure:
'record-level.expire-time': time retain for records. 'record-level.time-field': time field for record level expire. Expiration happens in compaction, and there is no strong guarantee to expire records in time. You can trigger a full compaction manually to expire records which were not expired in time.
Full Compaction#Paimon Compaction uses Universal-Compaction. By default, when there is too much incremental data, Full Compaction will be automatically performed. You don&rsquo;t usually have to worry about it.
Paimon also provides a configuration that allows for regular execution of Full Compaction.
&lsquo;compaction.optimization-interval&rsquo;: Implying how often to perform an optimization full compaction, this configuration is used to ensure the query timeliness of the read-optimized system table. &lsquo;full-compaction.delta-commits&rsquo;: Full compaction will be constantly triggered after delta commits. Its disadvantage is that it can only perform compaction synchronously, which will affect writing efficiency. Lookup Compaction#When primary key table is configured with lookup changelog producer or first-row merge-engine or has enabled deletion vectors for MOW mode, Paimon will use a radical compaction strategy to force compacting level 0 files to higher levels for every compaction trigger.
Paimon also provides configurations to optimize the frequency of this compaction.
&rsquo;lookup-compact&rsquo;: compact mode used for lookup compaction. Possible values: radical, will use ForceUpLevel0Compaction strategy to radically compact new files; gentle, will use UniversalCompaction strategy to gently compact new files; &rsquo;lookup-compact.max-interval&rsquo;: The max interval for a forced L0 lookup compaction to be triggered in gentle mode. This option is only valid when lookup-compact mode is gentle. By configuring &rsquo;lookup-compact&rsquo; as gentle, new files in L0 will not be compacted immediately, this may greatly reduce the overall resource usage at the expense of worse data freshness in certain cases.
Compaction Options#Number of Sorted Runs to Pause Writing#When the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.
OptionRequiredDefaultTypeDescriptionnum-sorted-run.stop-triggerNo(none)IntegerThe number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size.
OptionRequiredDefaultTypeDescriptionsort-spill-thresholdNo(none)IntegerIf the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.Number of Sorted Runs to Trigger Compaction#Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.
One can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.
OptionRequiredDefaultTypeDescriptionnum-sorted-run.compaction-triggerNo5IntegerThe sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.
`}),e.add({id:86,href:"/iceberg/configurations/",title:"Configurations",section:"Iceberg Metadata",content:`Configurations#Options for Iceberg Compatibility.
KeyDefaultTypeDescriptionmetadata.iceberg.compaction.max.file-num50IntegerIf number of small Iceberg manifest metadata files exceeds this limit, always trigger manifest metadata compaction regardless of their total size.metadata.iceberg.compaction.min.file-num10IntegerMinimum number of Iceberg manifest metadata files to trigger manifest metadata compaction.metadata.iceberg.database(none)StringMetastore database name for Iceberg Catalog. Set this as an iceberg database alias if using a centralized Catalog.metadata.iceberg.delete-after-commit.enabledtrueBooleanWhether to delete old metadata files after each table commitmetadata.iceberg.format-version2IntegerThe format version of iceberg table, the value can be 2 or 3. Note that only version 3 supports deletion vector.metadata.iceberg.glue.skip-archivefalseBooleanSkip archive for AWS Glue catalog.metadata.iceberg.hadoop-conf-dir(none)Stringhadoop-conf-dir for Iceberg Hive catalog.metadata.iceberg.hive-client-class"org.apache.hadoop.hive.metastore.HiveMetaStoreClient"StringHive client class name for Iceberg Hive Catalog.metadata.iceberg.hive-conf-dir(none)Stringhive-conf-dir for Iceberg Hive catalog.metadata.iceberg.hive-skip-update-statsfalseBooleanSkip updating Hive stats.metadata.iceberg.manifest-compression"snappy"StringCompression for Iceberg manifest files.metadata.iceberg.manifest-legacy-versionfalseBooleanShould use the legacy manifest version to generate Iceberg's 1.4 manifest files.metadata.iceberg.previous-versions-max0IntegerThe number of old metadata files to keep after each table commitmetadata.iceberg.storagedisabledEnum
When set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.
Possible values:"disabled": Disable Iceberg compatibility support."table-location": Store Iceberg metadata in each table's directory."hadoop-catalog": Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog."hive-catalog": Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.metadata.iceberg.storage-location(none)Enum
To store Iceberg metadata in a separate directory or under table location
Possible values:"table-location": Store Iceberg metadata in each table's directory. Useful for standalone Iceberg tables or Java API access. Can also be used with Hive Catalog"catalog-location": Store Iceberg metadata in a separate directory. Allows integration with Hive Catalog or Hadoop Catalog.metadata.iceberg.table(none)StringMetastore table name for Iceberg Catalog.Set this as an iceberg table alias if using a centralized Catalog.metadata.iceberg.uri(none)StringHive metastore uri for Iceberg Hive catalog.`}),e.add({id:87,href:"/maintenance/rescale-bucket/",title:"Rescale Bucket",section:"Maintenance",content:`Rescale Bucket#Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.
Rescale Overwrite#-- rescale number of total buckets ALTER TABLE table_identifier SET (&#39;bucket&#39; = &#39;...&#39;); -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec]; Please note that
ALTER TABLE only modifies the table&rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first. For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (&#39;bucket&#39; = &#39;4&#39;); INSERT OVERWRITE my_table PARTITION (dt = &#39;2022-01-01&#39;) SELECT * FROM ...; ALTER TABLE my_table SET (&#39;bucket&#39; = &#39;8&#39;); INSERT OVERWRITE my_table PARTITION (dt = &#39;2022-01-02&#39;) SELECT * FROM ...; During overwrite period, make sure there are no other jobs writing the same table/partition. Note: For the table which enables log system(e.g. Kafka), please rescale the topic&rsquo;s partition as well to keep consistency.Use Case#Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table&rsquo;s DDL and pipeline are listed as follows.
-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( &#39;bucket&#39; = &#39;16&#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( &#39;connector&#39; = &#39;kafka&#39;, &#39;topic&#39; = &#39;...&#39;, &#39;properties.bootstrap.servers&#39; = &#39;...&#39;, &#39;format&#39; = &#39;csv&#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, &#39;yyyy-MM-dd&#39;) AS dt FROM raw_orders WHERE order_status = &#39;verified&#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job&rsquo;s latency keeps increasing. To improve the data freshness, users can
Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\ --savepointPath /tmp/flink-savepoints \\ $JOB_ID Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (&#39;bucket&#39; = &#39;32&#39;); Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET &#39;execution.runtime-mode&#39; = &#39;batch&#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today&#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = &#39;2022-06-22&#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = &#39;2022-06-22&#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (&#39;2022-06-20&#39;, &#39;2022-06-21&#39;, &#39;2022-06-22&#39;); After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET &#39;execution.runtime-mode&#39; = &#39;streaming&#39;; SET &#39;execution.savepoint.path&#39; = &lt;savepointPath&gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, &#39;yyyy-MM-dd&#39;) AS dt FROM raw_orders WHERE order_status = &#39;verified&#39;; `}),e.add({id:88,href:"/flink/sql-alter/",title:"SQL Alter",section:"Engine Flink",content:`Altering Tables#Changing/Adding Table Properties#The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET ( &#39;write-buffer-size&#39; = &#39;256 MB&#39; ); Removing Table Properties#The following SQL removes write-buffer-size table property.
ALTER TABLE my_table RESET (&#39;write-buffer-size&#39;); Changing/Adding Table Comment#The following SQL changes comment of table my_table to table comment.
ALTER TABLE my_table SET ( &#39;comment&#39; = &#39;table comment&#39; ); Removing Table Comment#The following SQL removes table comment.
ALTER TABLE my_table RESET (&#39;comment&#39;); Rename Table Name#The following SQL rename the table name to new name.
ALTER TABLE my_table RENAME TO my_table_new; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.Adding New Columns#The following SQL adds two columns c1 and c2 to table my_table.
To add a column in a row type, see Changing Column Type.ALTER TABLE my_table ADD (c1 INT, c2 STRING); Renaming Column Name#The following SQL renames column c0 in table my_table to c1.
ALTER TABLE my_table RENAME c0 TO c1; Dropping Columns#The following SQL drops two columns c1 and c2 from table my_table.
ALTER TABLE my_table DROP (c1, c2); To drop a column in a row type, see Changing Column Type.In hive catalog, you need to ensure:
disable hive.metastore.disallow.incompatible.col.type.changes in your hive server or set hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your paimon catalog. Otherwise this operation may fail, throws an exception like The following columns have types incompatible with theexisting columns in their respective positions.
Dropping Partitions#The following SQL drops the partitions of the paimon table.
For flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time.
ALTER TABLE my_table DROP PARTITION (\`id\` = 1); ALTER TABLE my_table DROP PARTITION (\`id\` = 1, \`name\` = &#39;paimon&#39;); ALTER TABLE my_table DROP PARTITION (\`id\` = 1), PARTITION (\`id\` = 2); Changing Column Nullability#The following SQL changes nullability of column coupon_info.
CREATE TABLE my_table (id INT PRIMARY KEY NOT ENFORCED, coupon_info FLOAT NOT NULL); -- Change column \`coupon_info\` from NOT NULL to nullable ALTER TABLE my_table MODIFY coupon_info FLOAT; -- Change column \`coupon_info\` from nullable to NOT NULL -- If there are NULL values already, set table option as below to drop those records silently before altering table. SET &#39;table.exec.sink.not-null-enforcer&#39; = &#39;DROP&#39;; ALTER TABLE my_table MODIFY coupon_info FLOAT NOT NULL; Changing nullable column to NOT NULL is only supported by Flink currently.Changing Column Comment#The following SQL changes comment of column buy_count to buy count.
ALTER TABLE my_table MODIFY buy_count BIGINT COMMENT &#39;buy count&#39;; Adding Column Position#To add a new column with specified position, use FIRST or AFTER col_name.
ALTER TABLE my_table ADD c INT FIRST; ALTER TABLE my_table ADD c INT AFTER b; Changing Column Position#To modify an existent column to a new position, use FIRST or AFTER col_name.
ALTER TABLE my_table MODIFY col_a DOUBLE FIRST; ALTER TABLE my_table MODIFY col_a DOUBLE AFTER col_b; Changing Column Type#The following SQL changes type of column col_a to DOUBLE.
ALTER TABLE my_table MODIFY col_a DOUBLE; Paimon also supports changing columns of row type, array type, and map type.
-- col_a previously has type ARRAY&lt;MAP&lt;INT, ROW(f1 INT, f2 STRING)&gt;&gt; -- the following SQL changes f1 to BIGINT, drops f2, and adds f3 ALTER TABLE my_table MODIFY col_a ARRAY&lt;MAP&lt;INT, ROW(f1 BIGINT, f3 DOUBLE)&gt;&gt;; Adding watermark#The following SQL adds a computed column ts from existing column log_ts, and a watermark with strategy ts - INTERVAL '1' HOUR on column ts which is marked as event time attribute of table my_table.
ALTER TABLE my_table ADD ( ts AS TO_TIMESTAMP(log_ts) AFTER log_ts, WATERMARK FOR ts AS ts - INTERVAL &#39;1&#39; HOUR ); Dropping watermark#The following SQL drops the watermark of table my_table.
ALTER TABLE my_table DROP WATERMARK; Changing watermark#The following SQL modifies the watermark strategy to ts - INTERVAL '2' HOUR.
ALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL &#39;2&#39; HOUR; ALTER DATABASE#The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.
ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...); Altering Database Location#The following SQL changes location of database my_database to file:/temp/my_database.
ALTER DATABASE my_database SET (&#39;location&#39; = &#39;file:/temp/my_database&#39;); `}),e.add({id:89,href:"/concepts/spec/fileindex/",title:"文件索引 File Index",section:"规范说明 Specification",content:`File index#定义 file-index.\${index_type}.columns 后，Paimon 会为每个数据文件创建对应的索引文件。
如果索引文件过小，会直接存储在 manifest 中，或者存放在数据文件的目录中。
每个数据文件对应一个索引文件，索引文件有独立的文件定义，可以包含多列的多种类型索引。
Index File#文件索引文件格式：
将所有列信息和偏移量存放在文件头部。
______________________________________ _____________________| magic ｜version｜head length ||--------------------------------------|| column number ||--------------------------------------|| column 1 ｜ index number ||--------------------------------------|| index name 1 ｜start pos ｜length ||--------------------------------------|| index name 2 ｜start pos ｜length ||--------------------------------------|| index name 3 ｜start pos ｜length ||--------------------------------------| HEAD| column 2 ｜ index number ||--------------------------------------|| index name 1 ｜start pos ｜length ||--------------------------------------|| index name 2 ｜start pos ｜length ||--------------------------------------|| index name 3 ｜start pos ｜length ||--------------------------------------|| ... ||--------------------------------------|| ... ||--------------------------------------|| redundant length ｜redundant bytes ||--------------------------------------| ---------------------| BODY || BODY || BODY | BODY| BODY ||______________________________________| _____________________*magic: 8 bytes long, value is 1493475289347502L, BIG_ENDIANversion: 4 bytes int, BIG_ENDIANhead length: 4 bytes int, BIG_ENDIANcolumn number: 4 bytes int, BIG_ENDIANcolumn x name: var bytes, Java modified-utf-8index number: 4 bytes int (how many column items below), BIG_ENDIANindex name x: var bytes, Java modified-utf-8start pos: 4 bytes int, BIG_ENDIANlength: 4 bytes int, BIG_ENDIANredundant length: 4 bytes int (for compatibility with later versions, in this version, content is zero)redundant bytes: var bytes (for compatibility with later version, in this version, is empty)BODY: column index bytes + column index bytes + column index bytes + .......Index: BloomFilter#定义 'file-index.bloom-filter.columns'。
布隆过滤器索引的内容很简单：
numHashFunctions：4 字节整数，BIG_ENDIAN。 布隆过滤器字节数据。 该类使用 64 位长整型哈希。仅存储哈希函数数量（一个整数）和位集合字节。
对于哈希字节类型（如 varchar、binary 等）使用 xxHash；对于数值类型使用 指定的整数哈希算法。
Index: Bitmap#file-index.bitmap.columns：指定需要创建位图索引的列。 file-index.bitmap.&lt;column_name&gt;.index-block-size：配置二级索引块大小，默认值为 16KB。 V2位图文件索引格式（V2）：
Bitmap file index format (V2)+-------------------------------------------------+-----------------｜ version (1 byte) = 2 ｜+-------------------------------------------------+｜ row count (4 bytes int) ｜+-------------------------------------------------+｜ non-null value bitmap number (4 bytes int) ｜+-------------------------------------------------+｜ has null value (1 byte) ｜+-------------------------------------------------+｜ null value offset (4 bytes if has null value) ｜ HEAD+-------------------------------------------------+｜ null bitmap length (4 bytes if has null value) ｜+-------------------------------------------------+｜ bitmap index block number (4 bytes int) ｜+-------------------------------------------------+｜ value 1 | offset 1 ｜+-------------------------------------------------+｜ value 2 | offset 2 ｜+-------------------------------------------------+｜ ... ｜+-------------------------------------------------+｜ bitmap body offset (4 bytes int) ｜+-------------------------------------------------+-----------------｜ bitmap index block 1 ｜+-------------------------------------------------+｜ bitmap index block 2 ｜ INDEX BLOCKS+-------------------------------------------------+｜ ... ｜+-------------------------------------------------+-----------------｜ serialized bitmap 1 ｜+-------------------------------------------------+｜ serialized bitmap 2 ｜+-------------------------------------------------+ BITMAP BLOCKS｜ serialized bitmap 3 ｜+-------------------------------------------------+｜ ... ｜+-------------------------------------------------+-----------------index block format:+-------------------------------------------------+｜ entry number (4 bytes int) ｜+-------------------------------------------------+｜ value 1 | offset 1 | length 1 ｜+-------------------------------------------------+｜ value 2 | offset 2 | length 2 ｜+-------------------------------------------------+｜ ... ｜+-------------------------------------------------+value x: var bytes for any data type (as bitmap identifier)offset: 4 bytes int (when it is negative, it represents that there is only one valueand its position is the inverse of the negative value)length: 4 bytes intV1 (Legacy)(Legacy) Bitmap file index format (V1):
You can configure file-index.bitmap.&lt;column_name&gt;.version to use legacy bitmap version 1.
Bitmap file index format (V1)+-------------------------------------------------+-----------------| version (1 byte) |+-------------------------------------------------+| row count (4 bytes int) |+-------------------------------------------------+| non-null value bitmap number (4 bytes int) |+-------------------------------------------------+| has null value (1 byte) |+-------------------------------------------------+| null value offset (4 bytes if has null value) | HEAD+-------------------------------------------------+| value 1 | offset 1 |+-------------------------------------------------+| value 2 | offset 2 |+-------------------------------------------------+| value 3 | offset 3 |+-------------------------------------------------+| ... |+-------------------------------------------------+-----------------| serialized bitmap 1 |+-------------------------------------------------+| serialized bitmap 2 |+-------------------------------------------------+ BODY| serialized bitmap 3 |+-------------------------------------------------+| ... |+-------------------------------------------------+-----------------*value x: var bytes for any data type (as bitmap identifier)offset: 4 bytes int (when it is negative, it represents that there is only one valueand its position is the inverse of the negative value)所有整数均为 BIG_ENDIAN。
位图索引仅支持以下数据类型：TinyIntType、SmallIntType、IntType、BigIntType、DateType、TimeType、LocalZonedTimestampType、TimestampType、CharType、VarCharType、StringType、BooleanType。
Index: Bit-Slice Index Bitmap#BSI 文件索引是一种数值范围索引，用于加速范围查询，可与位图索引配合使用。
定义 'file-index.bsi.columns'。
BSI 文件索引格式（V1）：
BSI file index format (V1)+-------------------------------------------------+| version (1 byte) |+-------------------------------------------------+| row count (4 bytes int) |+-------------------------------------------------+| has positive value (1 byte) |+-------------------------------------------------+| positive BSI serialized (if has positive value) | +-------------------------------------------------+| has negative value (1 byte) |+-------------------------------------------------+| negative BSI serialized (if has negative value) | +-------------------------------------------------+BSI serialized format (V1):
BSI serialized format (V1)+-------------------------------------------------+| version (1 byte) |+-------------------------------------------------+| min value (8 bytes long) |+-------------------------------------------------+| max value (8 bytes long) |+-------------------------------------------------+| serialized existence bitmap | +-------------------------------------------------+| bit slice bitmap count (4 bytes int) |+-------------------------------------------------+| serialized bit 0 bitmap |+-------------------------------------------------+| serialized bit 1 bitmap |+-------------------------------------------------+| serialized bit 2 bitmap |+-------------------------------------------------+| ... |+-------------------------------------------------+BSI 仅支持以下数据类型：TinyIntType、SmallIntType、IntType、BigIntType、DateType、LocalZonedTimestamp、TimestampType、DecimalType。
`}),e.add({id:90,href:"/concepts/system-tables/",title:"系统表 System Tables",section:"概念 Concepts",content:`System Tables#Paimon 提供了非常丰富的系统表，帮助用户更好地分析和查询 Paimon 表的状态：
查询数据表状态：数据系统表（Data System Table）。 查询整个 Catalog 的全局状态：全局系统表（Global System Table）。 Data System Table#数据系统表包含每个 Paimon 数据表的元数据和信息，如创建的快照及使用的选项。用户可以通过批量查询访问系统表。
目前，Flink、Spark、Trino 和 StarRocks 支持查询系统表。
在某些情况下，表名需要用反引号括起来以避免语法解析冲突，例如三重访问模式：
SELECT * FROM my_catalog.my_db.\`my_table$snapshots\`; Snapshots Table#你可以通过 snapshots 表查询表的快照历史信息，包括快照中发生的记录数。
SELECT * FROM my_table$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | base_manifest_list | delta_manifest_list | changelog_manifest_list | total_record_count | delta_record_count | changelog_record_count | watermark | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | 2 | 2 | 0 | 1666755855600 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | 1 | 1 | 0 | 1666755855148 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ 2 rows in set */ 通过查询 snapshots 表，你可以了解该表的提交和过期信息，并可通过数据进行时间旅行。
Schemas Table#你可以通过 schemas 表查询该表的历史 schema 信息。
SELECT * FROM my_table$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | schema_id | fields | partition_keys | primary_keys | options | comment | update_time | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | 0 | [{&#34;id&#34;:0,&#34;name&#34;:&#34;word&#34;,&#34;typ... | [] | [&#34;word&#34;] | {} | | 2022-10-28 11:44:20.600 | | 1 | [{&#34;id&#34;:0,&#34;name&#34;:&#34;word&#34;,&#34;typ... | [] | [&#34;word&#34;] | {} | | 2022-10-27 11:44:15.600 | | 2 | [{&#34;id&#34;:0,&#34;name&#34;:&#34;word&#34;,&#34;typ... | [] | [&#34;word&#34;] | {} | | 2022-10-26 11:44:10.600 | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ 3 rows in set */ 你可以关联 snapshots 表和 schemas 表，以获取指定快照的字段信息。
SELECT s.snapshot_id, t.schema_id, t.fields FROM my_table$snapshots s JOIN my_table$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table#你可以通过 options 表查询表的选项信息，这些选项是在 DDL 中指定的。未显示的选项则为默认值。你可以参考 Configuration。
SELECT * FROM my_table$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table#如果需要审计表的变更日志，可以使用 audit_log 系统表。通过 audit_log 表，你可以获取表的增量数据中的 rowkind 列。你可以利用该列进行过滤和其他操作来完成审计。
rowkind 有四种取值：
+I：插入操作。 -U：更新操作，表示更新前的旧内容。 +U：更新操作，表示更新后的新内容。 -D：删除操作。 SELECT * FROM my_table$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Binlog Table#你可以通过 binlog 表查询变更日志。在 binlog 系统表中，更新前和更新后的内容会打包在同一行。
目前，binlog 表无法显示 Flink 计算列。
SELECT * FROM T$binlog; /* +------------------+----------------------+-----------------------+ | rowkind | column_0 | column_1 | +------------------+----------------------+-----------------------+ | +I | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ | +U | [col_0_ub, col_0_ua] | [col_1_ub, col_1_ua] | +------------------+----------------------+-----------------------+ | -D | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ */ Read-optimized Table#如果你需要极致的读取性能并且能够接受读取稍微旧一点的数据，可以使用 ro（只读优化）系统表。
只读优化系统表通过只扫描不需要合并的文件来提升读取性能。
对于带主键的表，ro 系统表只扫描最顶层的文件。
也就是说，ro 系统表只输出最新一次全量压缩的结果。
不同的桶可能在不同时间进行全量压缩，
因此不同键的值可能来自不同的快照。对于追加表，由于所有文件均可直接读取且无需合并，
ro 系统表表现得就像普通的追加表。
SELECT * FROM my_table$ro; Files Table#你可以查询指定快照的表文件信息。
-- Query the files of latest snapshot SELECT * FROM my_table$files; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {2} | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| | {5} | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} | 1691551246788 | 1691551246152 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246722 | 1691551246273 |2023-02-24T16:06:21.166| | {4} | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} | 1691551246321 | 1691551246109 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 6 rows in set */ -- You can also query the files with specific snapshot SELECT * FROM my_table$files /*+ OPTIONS(&#39;scan.snapshot-id&#39;=&#39;1&#39;) */; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 3 rows in set */ Tags Table#你可以通过 tags 表查询表的标签历史信息，包括标签基于的快照以及快照的一些历史信息。你还可以获取所有标签名称，并通过标签名称进行时间旅行，访问特定标签的数据。
SELECT * FROM my_table$tags; /* +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag_name | snapshot_id | schema_id | commit_time | record_count | branches | +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag1 | 1 | 0 | 2023-06-28 14:55:29.344 | 3 | [] | | tag3 | 3 | 0 | 2023-06-28 14:58:24.691 | 7 | [branch-1] | +----------+-------------+-----------+-------------------------+--------------+--------------+ 2 rows in set */ Branches Table#你可以查询表的分支信息。
SELECT * FROM my_table$branches; /* +----------------------+-------------------------+ | branch_name | create_time | +----------------------+-------------------------+ | branch1 | 2024-07-18 20:31:39.084 | | branch2 | 2024-07-18 21:11:14.373 | +----------------------+-------------------------+ 2 rows in set */ Consumers Table#你可以查询包含下一快照的所有消费者信息。
SELECT * FROM my_table$consumers; /* +-------------+------------------+ | consumer_id | next_snapshot_id | +-------------+------------------+ | id1 | 1 | | id2 | 3 | +-------------+------------------+ 2 rows in set */ Manifests Table#你可以查询当前表的最新快照或指定快照中包含的所有清单（manifest）文件。
-- Query the manifest of latest snapshot SELECT * FROM my_table$manifests; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | | manifest-f4dcab43-ef6b-4713... | 1648 | 1 | 0 | 0 | {20230115, 00} | {20230316, 23} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 2 rows in set */ -- You can also query the manifest with specified snapshot SELECT * FROM my_table$manifests /*+ OPTIONS(&#39;scan.snapshot-id&#39;=&#39;1&#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified tagName SELECT * FROM my_table$manifests /*+ OPTIONS(&#39;scan.tag-name&#39;=&#39;tag1&#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified timestamp in unix milliseconds SELECT * FROM my_table$manifests /*+ OPTIONS(&#39;scan.timestamp-millis&#39;=&#39;1678883047356&#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ Aggregation fields Table#你可以通过 aggregation fields 表查询表的历史聚合信息。
SELECT * FROM my_table$aggregation_fields; /* +------------+-----------------+--------------+--------------------------------+---------+ | field_name | field_type | function | function_options | comment | +------------+-----------------+--------------+--------------------------------+---------+ | product_id | BIGINT NOT NULL | [] | [] | &lt;NULL&gt; | | price | INT | [true,count] | [fields.price.ignore-retrac... | &lt;NULL&gt; | | sales | BIGINT | [sum] | [fields.sales.aggregate-fun... | &lt;NULL&gt; | +------------+-----------------+--------------+--------------------------------+---------+ 3 rows in set */ Partitions Table#你可以查询表的分区文件信息。
SELECT * FROM my_table$partitions; /* +---------------+----------------+--------------------+--------------------+------------------------+ | partition | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+----------------+--------------------+--------------------+------------------------+ | {1} | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+----------------+--------------------+--------------------+------------------------+ */ Buckets Table#你可以查询表的桶文件信息。
SELECT * FROM my_table$buckets; /* +---------------+--------+----------------+--------------------+--------------------+------------------------+ | partition | bucket | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+--------+----------------+--------------------+--------------------+------------------------+ | [1] | 0 | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+--------+----------------+--------------------+--------------------+------------------------+ */ Statistic Table#你可以通过 statistic 表查询统计信息。
SELECT * FROM T$statistics; /* +--------------+------------+-----------------------+------------------+----------+ | snapshot_id | schema_id | mergedRecordCount | mergedRecordSize | colstat | +--------------+------------+-----------------------+------------------+----------+ | 2 | 0 | 2 | 2 | {} | +--------------+------------+-----------------------+------------------+----------+ 1 rows in set */ Table Indexes Table#你可以通过 indexes 表查询为动态桶表（index_type = HASH）和删除向量（index_type = DELETION_VECTORS）生成的表索引文件。
SELECT * FROM my_table$table_indexes; /* +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | partition | bucket | index_type | file_name | file_size | row_count | dv_ranges | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | {2024-10-01} | 0 | HASH | index-70abfebf-149e-4796-9f... | 12 | 3 | &lt;NULL&gt; | | {2024-10-01} | 0 | DELETION_VECTORS | index-633857e7-cdce-47d2-87... | 33 | 1 | [(data-346cb9c8-4032-4d66-a... | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ 2 rows in set */ Global System Table#全局系统表包含 Paimon 中所有表的统计信息。为了方便查询，我们创建了一个名为 sys 的引用系统数据库。
我们可以通过 Flink SQL 显示所有全局系统表：
USE sys; SHOW TABLES; ALL Options Table#该表类似于 Options 表，但它显示的是所有数据库中所有表的选项信息。
SELECT * FROM sys.all_table_options; /* +---------------+--------------------------------+--------------------------------+------------------+ | database_name | table_name | key | value | +---------------+--------------------------------+--------------------------------+------------------+ | my_db | Orders_orc | bucket | -1 | | my_db | Orders2 | bucket | -1 | | my_db | Orders2 | sink.parallelism | 7 | | my_db2| OrdersSum | bucket | 1 | +---------------+--------------------------------+--------------------------------+------------------+ 7 rows in set */ Catalog Options Table#你可以通过 catalog options 表查询 Catalog 的选项信息，未显示的选项为默认值。你可以参考 Configuration。
SELECT * FROM sys.catalog_options; /* +-----------+---------------------------+ | key | value | +-----------+---------------------------+ | warehouse | hdfs:///path/to/warehouse | +-----------+---------------------------+ 1 rows in set */ `}),e.add({id:91,href:"/maintenance/manage-tags/",title:"Manage Tags",section:"Maintenance",content:`Manage Tags#Paimon&rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.
To solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.
Automatic Creation#Paimon supports automatic creation of tags in writing job.
Step 1: Choose Creation Mode
You can set creation mode by table option 'tag.automatic-creation'. Supported values are:
process-time: Create TAG based on the time of the machine. watermark: Create TAG based on the watermark of the Sink input. batch: In a batch processing scenario, a tag is generated after the current task is completed. If you choose Watermark, you may need to specify the time zone of watermark, if watermark is not in the UTC time zone, please configure 'sink.watermark-time-zone'.Step 2: Choose Creation Period
What frequency is used to generate tags. You can choose 'daily', 'hourly' and 'two-hours' for 'tag.creation-period'.
If you need to wait for late data, you can configure a delay time: 'tag.creation-delay'.
Step 3: Automatic deletion of tags
You can configure 'tag.num-retained-max' or tag.default-time-retained to delete tags automatically.
Example, configure table to create a tag at 0:10 every day, with a maximum retention time of 3 months:
-- Flink SQL CREATE TABLE t ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, ... ) WITH ( &#39;tag.automatic-creation&#39; = &#39;process-time&#39;, &#39;tag.creation-period&#39; = &#39;daily&#39;, &#39;tag.creation-delay&#39; = &#39;10 m&#39;, &#39;tag.num-retained-max&#39; = &#39;90&#39; ); INSERT INTO t SELECT ...; -- Spark SQL -- Read latest snapshot SELECT * FROM t; -- Read Tag snapshot SELECT * FROM t VERSION AS OF &#39;2023-07-26&#39;; -- Read Incremental between Tags SELECT * FROM paimon_incremental_query(&#39;t&#39;, &#39;2023-07-25&#39;, &#39;2023-07-26&#39;); See Query Tables to see more query for Spark.
Create Tags#You can create a tag with given name and snapshot ID.
Flink SQLRun the following command:
CALL sys.create_tag(\`table\` =&gt; &#39;database_name.table_name&#39;, tag =&gt; &#39;tag_name&#39;, [snapshot_id =&gt; &lt;snapshot-id&gt;]); If snapshot_id unset, snapshot_id defaults to the latest.
Flink ActionRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ create_tag \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --tag_name &lt;tag-name&gt; \\ [--snapshot &lt;snapshot_id&gt;] \\ [--time_retained &lt;time-retained&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] If snapshot unset, snapshot_id defaults to the latest.
Java APIimport org.apache.paimon.table.Table; public class CreateTag { public static void main(String[] args) { Table table = ...; table.createTag(&#34;my-tag&#34;, 1); table.createTag(&#34;my-tag-retained-12-hours&#34;, 1, Duration.ofHours(12)); } } SparkRun the following sql:
CALL sys.create_tag(table =&gt; &#39;test.t&#39;, tag =&gt; &#39;test_tag&#39;, snapshot =&gt; 2); To create a tag with retained 1 day, run the following sql:
CALL sys.create_tag(table =&gt; &#39;test.t&#39;, tag =&gt; &#39;test_tag&#39;, snapshot =&gt; 2, time_retained =&gt; &#39;1 d&#39;); To create a tag based on the latest snapshot id, run the following sql:
CALL sys.create_tag(table =&gt; &#39;test.t&#39;, tag =&gt; &#39;test_tag&#39;); Delete Tags#You can delete a tag by its name.
Flink SQLRun the following command:
CALL sys.delete_tag(\`table\` =&gt; &#39;database_name.table_name&#39;, tag =&gt; &#39;tag_name&#39;); Flink ActionRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ delete_tag \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --tag_name &lt;tag-name&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Java APIimport org.apache.paimon.table.Table; public class DeleteTag { public static void main(String[] args) { Table table = ...; table.deleteTag(&#34;my-tag&#34;); } } SparkRun the following sql:
CALL sys.delete_tag(table =&gt; &#39;test.t&#39;, tag =&gt; &#39;test_tag&#39;); Rollback to Tag#Rollback table to a specific tag. All snapshots and tags whose snapshot id is larger than the tag will be deleted (and the data will be deleted too).
Flink SQLRun the following command:
CALL sys.rollback_to(\`table\` =&gt; &#39;database_name.table_name&#39;, tag =&gt; &#39;tag_name&#39;); Flink ActionRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ rollback_to \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --version &lt;tag-name&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Java APIimport org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback: // snapshot-3 [expired] -&gt; tag3 // snapshot-4 [expired] // snapshot-5 -&gt; tag5 // snapshot-6 // snapshot-7 table.rollbackTo(&#34;tag3&#34;); // after rollback: // snapshot-3 -&gt; tag3 } } SparkRun the following sql:
CALL sys.rollback(table =&gt; &#39;test.t&#39;, version =&gt; &#39;2&#39;); `}),e.add({id:92,href:"/primary-key-table/query-performance/",title:"Query Performance",section:"Table with PK",content:`Query Performance#Table Mode#The table schema has the greatest impact on query performance. See Table Mode.
For Merge On Read table, the most important thing you should pay attention to is the number of buckets, which will limit the concurrency of reading data.
For MOW (Deletion Vectors) or COW table or Read Optimized table, there is no limit to the concurrency of reading data, and they can also utilize some filtering conditions for non-primary-key columns.
Data Skipping By Primary Key Filter#For a regular bucketed table (For example, bucket = 5), the filtering conditions of the primary key will greatly accelerate queries and reduce the reading of a large number of files.
Data Skipping By File Index#You can use file index to table with Deletion Vectors enabled, it filters files by index on the read side.
CREATE TABLE &lt;PAIMON_TABLE&gt; WITH ( &#39;deletion-vectors.enabled&#39; = &#39;true&#39;, &#39;file-index.bloom-filter.columns&#39; = &#39;c1,c2&#39;, &#39;file-index.bloom-filter.c1.items&#39; = &#39;200&#39; ); Supported filter types:
Bloom Filter:
file-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.&lt;column_name&gt;.fpp to config false positive probability. file-index.bloom-filter.&lt;column_name&gt;.items to config the expected distinct items in one data file. Bitmap:
file-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap. Bit-Slice Index Bitmap
file-index.bsi.columns: specify the columns that need bsi index. More filter types will be supported&hellip;
If you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.&lt;filter-type&gt;.columns to the table.
How to invoke: see flink procedures
`}),e.add({id:93,href:"/concepts/data-types/",title:"数据类型 Data Types",section:"概念 Concepts",content:`Data Types#数据类型描述了表生态系统中值的逻辑类型。它可用于声明操作的输入和/或输出类型。
Paimon 支持的所有数据类型如下：
DataTypeDescriptionBOOLEAN布尔类型，支持三值逻辑：TRUE、FALSE 和 UNKNOWN。CHAR
CHAR(n)定长字符类型。
该类型可以用 CHAR(n) 声明，其中 n 是代码点的数量。n 的取值范围是 1 到 2,147,483,647（含）。如果未指定长度，默认 n 为 1。VARCHAR
VARCHAR(n)
STRING变长字符类型。
该类型可以用 VARCHAR(n) 声明，其中 n 是最大代码点数量。n 的取值范围是 1 到 2,147,483,647（含）。如果未指定长度，默认 n 为 1。
STRING 是 VARCHAR(2147483647) 的同义词。BINARY
BINARY(n)
定长二进制字符串类型（即字节序列）。
该类型可以用 BINARY(n) 声明，其中 n 是字节数。n 的取值范围是 1 到 2,147,483,647（含）。如果未指定长度，默认 n 为 1。VARBINARY
VARBINARY(n)
BYTES变长二进制字符串类型（即字节序列）。
该类型可以用 VARBINARY(n) 声明，其中 n 是最大字节数。n 的取值范围是 1 到 2,147,483,647（含）。如果未指定长度，默认 n 为 1。
BYTES 是 VARBINARY(2147483647) 的同义词。DECIMAL
DECIMAL(p)
DECIMAL(p, s)固定精度和小数位数的十进制数字类型。
该类型可以用 DECIMAL(p, s) 声明，其中 p 是数字的总位数（精度），s 是小数点右侧的位数（刻度）。p 的取值范围是 1 到 38（含），s 的取值范围是 0 到 p（含）。默认 p 值为 10，默认 s 值为 0。TINYINT1字节带符号整数类型，取值范围从 -128 到 127。SMALLINT2字节带符号整数类型，取值范围从 -32,768 到 32,767。INT4字节带符号整数类型，取值范围从 -2,147,483,648 到 2,147,483,647。BIGINT8字节带符号整数类型，取值范围从 -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807。FLOAT4字节单精度浮点数类型。
与 SQL 标准相比，该类型不接受参数。DOUBLE8字节双精度浮点数类型。DATE日期类型，由年-月-日组成，取值范围从 0000-01-01 到 9999-12-31。
与 SQL 标准相比，日期范围从公元0000年开始。TIME
TIME(p)无时区的时间类型，由小时:分钟:秒[.小数]组成，精度最高到纳秒，取值范围从 00:00:00.000000000 到 23:59:59.999999999。
该类型可以用 TIME(p) 声明，其中 p 是小数秒的位数（精度）。p 的取值范围是 0 到 9（含）。如果未指定精度，默认 p 为 0。TIMESTAMP
TIMESTAMP(p)无时区的时间戳类型，由年-月-日 时:分:秒[.小数]组成，精度最高到纳秒，取值范围从 0000-01-01 00:00:00.000000000 到 9999-12-31 23:59:59.999999999。
该类型可以用 TIMESTAMP(p) 声明，其中 p 是小数秒的位数（精度）。p 的取值范围是 0 到 9（含）。如果未指定精度，默认 p 为 6。TIMESTAMP WITH LOCAL TIME ZONE
TIMESTAMP(p) WITH LOCAL TIME ZONE带本地时区的时间戳类型，由年-月-日 时:分:秒[.小数] 时区组成，精度最高到纳秒，取值范围从 0000-01-01 00:00:00.000000000 +14:59 到 9999-12-31 23:59:59.999999999 -14:59。
该类型填补了无时区和必须时区时间戳类型之间的空白，允许根据配置的会话时区来解释 UTC 时间戳。转换为整数表示自纪元以来的秒数，转换为长整型表示自纪元以来的毫秒数。ARRAY&lt;t&gt;元素类型相同的数组类型。
与 SQL 标准相比，数组的最大基数不可指定，固定为 2,147,483,647。同时，支持任意有效类型作为子类型。
该类型可以用 ARRAY&lt;t&gt; 声明，其中 t 是数组元素的数据类型。MAP&lt;kt, vt&gt;关联数组类型，将键（包括 NULL）映射到值（包括 NULL）。映射不能包含重复键，每个键最多映射一个值。
元素类型无限制，确保唯一性是用户的责任。
该类型可以用 MAP&lt;kt, vt&gt; 声明，其中 kt 是键的数据类型，vt 是值的数据类型。MULTISET&lt;t&gt;多重集合（袋子）类型。与集合不同，允许每个元素有多个实例。每个唯一值（包括 NULL）对应某个数量。
元素类型无限制，确保唯一性是用户的责任。
该类型可以用 MULTISET&lt;t&gt; 声明，其中 t 是包含元素的数据类型。ROW&lt;n0 t0, n1 t1, ...&gt;
ROW&lt;n0 t0 'd0', n1 t1 'd1', ...&gt;字段序列类型。
字段包含字段名、字段类型和可选描述。表的行的最具体类型是行类型，行的每列对应行类型中具有相同序号的字段。
与 SQL 标准相比，字段描述为可选，简化了复杂结构的处理。
行类型类似于其他非标准框架中的 STRUCT 类型。
该类型可以用 ROW&lt;n0 t0 'd0', n1 t1 'd1', ...&gt; 声明，其中 n 是字段唯一名称，t 是字段逻辑类型，d 是字段描述。`}),e.add({id:94,href:"/maintenance/metrics/",title:"Metrics",section:"Maintenance",content:`Paimon Metrics#Paimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.
In Paimon&rsquo;s metrics system, metrics are updated and reported at table granularity.
There are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.
Gauge: Provides a value of any type at a point in time. Counter: Used to count values by incrementing and decrementing. Histogram: Measure the statistical distribution of a set of values including the min, max, mean, standard deviation and percentile. Paimon has supported built-in metrics to measure operations of commits, scans, writes and compactions, which can be bridged to any computing engine that supports, like Flink, Spark etc.
Metrics List#Below is lists of Paimon built-in metrics. They are summarized into types of scan metrics, commit metrics, write metrics, write buffer metrics and compaction metrics.
Scan Metrics#Metrics NameTypeDescriptionlastScanDurationGaugeThe time it took to complete the last scan.scanDurationHistogramDistributions of the time taken by the last few scans.lastScannedManifestsGaugeNumber of scanned manifest files in the last scan.lastScanSkippedTableFilesGaugeTotal skipped table files in the last scan.lastScanResultedTableFilesGaugeResulted table files in the last scan.Commit Metrics#Metrics NameTypeDescriptionlastCommitDurationGaugeThe time it took to complete the last commit.commitDurationHistogramDistributions of the time taken by the last few commits.lastCommitAttemptsGaugeThe number of attempts the last commit made.lastTableFilesAddedGaugeNumber of added table files in the last commit, including newly created data files and compacted after.lastTableFilesDeletedGaugeNumber of deleted table files in the last commit, which comes from compacted before.lastTableFilesAppendedGaugeNumber of appended table files in the last commit, which means the newly created data files.lastTableFilesCommitCompactedGaugeNumber of compacted table files in the last commit, including compacted before and after.lastChangelogFilesAppendedGaugeNumber of appended changelog files in last commit.lastChangelogFileCommitCompactedGaugeNumber of compacted changelog files in last commit.lastGeneratedSnapshotsGaugeNumber of snapshot files generated in the last commit, maybe 1 snapshot or 2 snapshots.lastDeltaRecordsAppendedGaugeDelta records count in last commit with APPEND commit kind.lastChangelogRecordsAppendedGaugeChangelog records count in last commit with APPEND commit kind.lastDeltaRecordsCommitCompactedGaugeDelta records count in last commit with COMPACT commit kind.lastChangelogRecordsCommitCompactedGaugeChangelog records count in last commit with COMPACT commit kind.lastPartitionsWrittenGaugeNumber of partitions written in the last commit.lastBucketsWrittenGaugeNumber of buckets written in the last commit.lastCompactionInputFileSizeGaugeTotal size of the input files for the last compaction.lastCompactionOutputFileSizeGaugeTotal size of the output files for the last compaction.Write Buffer Metrics#Metrics NameTypeDescriptionnumWritersGaugeNumber of writers in this parallelism.bufferPreemptCountGaugeThe total number of memory preempted.usedWriteBufferSizeByteGaugeCurrent used write buffer size in byte.totalWriteBufferSizeByteGaugeThe total write buffer size configured in byte.Compaction Metrics#Metrics NameTypeDescriptionmaxLevel0FileCountGaugeThe maximum number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.avgLevel0FileCountGaugeThe average number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.compactionThreadBusyGaugeThe maximum business of compaction threads in this task. Currently, there is only one compaction thread in each parallelism, so value of business ranges from 0 (idle) to 100 (compaction running all the time).avgCompactionTimeGaugeThe average runtime of compaction threads, calculated based on recorded compaction time data in milliseconds. The value represents the average duration of compaction operations. Higher values indicate longer average compaction times, which may suggest the need for performance optimization.compactionCompletedCountCounterThe total number of compactions that have completed.compactionQueuedCountCounterThe total number of compactions that are queued/running.maxCompactionInputSizeGaugeThe maximum input file size for this task's compaction.avgCompactionInputSize/td>GaugeThe average input file size for this task's compaction.maxCompactionOutputSizeGaugeThe maximum output file size for this task's compaction.avgCompactionOutputSizeGaugeThe average output file size for this task's compaction.maxTotalFileSizeGaugeThe maximum total file size of an active (currently being written) bucket.avgTotalFileSizeGaugeThe average total file size of all active (currently being written) buckets.Bridging To Flink#Paimon has implemented bridging metrics to Flink&rsquo;s metrics system, which can be reported by Flink, and the lifecycle of metric groups are managed by Flink.
Please join the &lt;scope&gt;.&lt;infix&gt;.&lt;metric_name&gt; to get the complete metric identifier when using Flink to access Paimon, metric_name can be got from Metric List.
For example, the identifier of metric lastPartitionsWritten for table word_count in Flink job named insert_word_count is:
localhost.taskmanager.localhost:60340-775a20.insert_word_count.Global Committer : word_count.0.paimon.table.word_count.commit.lastPartitionsWritten.
From Flink Web-UI, go to the committer operator&rsquo;s metrics, it&rsquo;s shown as:
0.Global_Committer___word_count.paimon.table.word_count.commit.lastPartitionsWritten.
Please refer to System Scope to understand Flink scope Scan metrics are only supported by Flink versions &gt;= 1.18 ScopeInfixScan Metrics&lt;host&gt;.jobmanager.&lt;job_name&gt;&lt;source_operator_name&gt;.coordinator. enumerator.paimon.table.&lt;table_name&gt;.scanCommit Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;committer_operator_name&gt;.&lt;subtask_index&gt;paimon.table.&lt;table_name&gt;.commitWrite Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;writer_operator_name&gt;.&lt;subtask_index&gt;paimon.table.&lt;table_name&gt;.partition.&lt;partition_string&gt;.bucket.&lt;bucket_index&gt;.writerWrite Buffer Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;writer_operator_name&gt;.&lt;subtask_index&gt;paimon.table.&lt;table_name&gt;.writeBufferCompaction Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;writer_operator_name&gt;.&lt;subtask_index&gt;paimon.table.&lt;table_name&gt;.partition.&lt;partition_string&gt;.bucket.&lt;bucket_index&gt;.compactionFlink Source Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;source_operator_name&gt;.&lt;subtask_index&gt;-Flink Sink Metrics&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;committer_operator_name&gt;.&lt;subtask_index&gt;-Flink Connector Standard Metrics#When using Flink to read and write, Paimon has implemented some key standard Flink connector metrics to measure the source latency and output of sink, see FLIP-33: Standardize Connector Metrics. Flink source / sink metrics implemented are listed here.
Source Metrics (Flink)#Metrics NameLevelTypeDescriptioncurrentEmitEventTimeLagFlink Source OperatorGaugeTime difference between sending the record out of source and file creation.currentFetchEventTimeLagFlink Source OperatorGaugeTime difference between reading the data file and file creation.Please note that if you specified consumer-id in your streaming query, the level of source metrics should turn into the reader operator, which is behind the Monitor operator.Sink Metrics (Flink)#Metrics NameLevelTypeDescriptionnumBytesOutTableCounterThe total number of output bytes.numBytesOutPerSecondTableMeterThe output bytes per second.numRecordsOutTableCounterThe total number of output records.numRecordsOutPerSecondTableMeterThe output records per second.`}),e.add({id:95,href:"/concepts/functions/",title:"函数 Functions",section:"概念 Concepts",content:`Functions#Paimon 引入了一种 Function 抽象，旨在为计算引擎提供标准格式的函数支持，解决以下问题：
统一的列级过滤与处理：
支持在列级别进行操作，包括数据的加密与解密等任务。
参数化视图能力：
支持在视图中进行参数化操作，增强数据检索过程的动态性和可用性。
支持的函数类型#目前，Paimon 支持三种类型的函数：
文件函数（File Function）：
用户可以在文件中定义函数，为函数定义提供灵活且模块化的支持。
Lambda 函数（Lambda Function）：
支持用户使用 Java Lambda 表达式定义函数，实现内联、简洁、函数式的操作方式。
SQL 函数（SQL Function）：
用户可以直接在 SQL 中定义函数，与基于 SQL 的数据处理无缝集成。
File Function Usage in Flink#Paimon 函数可在 Apache Flink 中用于执行复杂的数据操作。以下是在 Flink 环境中创建、修改和删除函数的 SQL 命令：
Create Function#使用 FLink SQL创建一个新的函数
-- Flink SQL CREATE FUNCTION mydb.parse_str AS &#39;com.streaming.flink.udf.StrUdf&#39; LANGUAGE JAVA USING JAR &#39;oss://my_bucket/my_location/udf.jar&#39; [, JAR &#39;oss://my_bucket/my_location/a.jar&#39;]; 这条语句在 mydb 数据库中创建了一个基于 Java 的用户自定义函数（UDF），函数名为 parse_str，该函数使用了来自对象存储位置的指定 JAR 文件：
Alter Function#要在 Flink SQL 中修改一个已存在的函数，可以使用如下语句：
-- Flink SQL ALTER FUNCTION mydb.parse_str AS &#39;com.streaming.flink.udf.StrUdf2&#39; LANGUAGE JAVA; 以下命令将 parse_str 函数的实现更改为使用新的 Java 类定义。
Drop Function#从 Flink SQL 中移除一个函数的方法：
-- Flink SQL DROP FUNCTION mydb.parse_str; 该语句会删除 mydb 数据库中已存在的 parse_str 函数，释放其所占用的功能。
Lambda Function Usage in Spark#Create Function#-- Spark SQL CALL sys.create_function(\`function\` =&gt; &#39;my_db.area_func&#39;, \`inputParams\` =&gt; &#39;[{&#34;id&#34;: 0, &#34;name&#34;:&#34;length&#34;, &#34;type&#34;:&#34;INT&#34;}, {&#34;id&#34;: 1, &#34;name&#34;:&#34;width&#34;, &#34;type&#34;:&#34;INT&#34;}]&#39;, \`returnParams\` =&gt; &#39;[{&#34;id&#34;: 0, &#34;name&#34;:&#34;area&#34;, &#34;type&#34;:&#34;BIGINT&#34;}]&#39;, \`deterministic\` =&gt; true, \`comment\` =&gt; &#39;comment&#39;, \`options\` =&gt; &#39;k1=v1,k2=v2&#39; ); Alter Function#-- Spark SQL CALL sys.alter_function(\`function\` =&gt; &#39;my_db.area_func&#39;, \`change\` =&gt; &#39;{&#34;action&#34; : &#34;addDefinition&#34;, &#34;name&#34; : &#34;spark&#34;, &#34;definition&#34; : {&#34;type&#34; : &#34;lambda&#34;, &#34;definition&#34; : &#34;(Integer length, Integer width) -&gt; { return (long) length * width; }&#34;, &#34;language&#34;: &#34;JAVA&#34; } }&#39; ); -- Spark SQL select paimon.my_db.area_func(1, 2); Drop Function#-- Spark SQL CALL sys.drop_function(\`function\` =&gt; &#39;my_db.area_func&#39;); `}),e.add({id:96,href:"/maintenance/manage-privileges/",title:"Manage Privileges",section:"Maintenance",content:`Manage Privileges#Paimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.
Currently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.
This privilege system only prevents unwanted users from accessing tables through catalogs. It does not block access through temporary table (by specifying table path on filesystem), nor does it prevent user from directly modifying data files on filesystem. If you need more serious protection, use a filesystem with access management instead.Basic Concepts#We now introduce the basic concepts of the privilege system.
Object#An object is an entity to which access can be granted. Unless allowed by a grant, access is denied.
Currently, the privilege system in Paimon has three types of objects: CATALOG, DATABASE and TABLE. Objects have a logical hierarchy, which is related to the concept they represent. For example:
If a user is granted a privilege on the catalog, he will also have this privilege on all databases and all tables in the catalog. If a user is granted a privilege on the database, he will also have this privilege on all tables in that database. If a user is revoked a privilege from the catalog, he will also lose this privilege on all databases and all tables in the catalog. If a user is revoked a privilege from the database, he will also lose this privilege on all tables in that database. Privilege#A privilege is a defined level of access to an object. Multiple privileges can be used to control the granularity of access granted on an object. Privileges are object-specific. Different objects may have different privileges.
Currently, we support the following privileges.
Privilege Description Can be Granted on SELECT Queries data in a table. TABLE, DATABASE, CATALOG INSERT Inserts, updates or drops data in a table. Creates or drops tags and branches in a table. TABLE, DATABASE, CATALOG ALTER_TABLE Alters metadata of a table, including table name, column names, table options, etc. TABLE, DATABASE, CATALOG DROP_TABLE Drops a table. TABLE, DATABASE, CATALOG CREATE_TABLE Creates a table in a database. DATABASE, CATALOG DROP_DATABASE Drops a database. DATABASE, CATALOG CREATE_DATABASE Creates a database in the catalog. CATALOG ADMIN Creates or drops privileged users, grants or revokes privileges from users in a catalog. CATALOG User#The entity to which privileges can be granted. Users are authenticated by their password.
When the privilege system is enabled, two special users will be created automatically.
The root user, which is identified by the provided root password when enabling the privilege system. This user always has all privileges in the catalog. The anonymous user. This is the default user if no username and password is provided when creating the catalog. Enable Privileges#Paimon currently only supports file-based privilege system. Only catalogs with 'metastore' = 'filesystem' (the default value) or 'metastore' = 'hive' support such privilege system.
To enable the privilege system on a filesystem / Hive catalog, do the following steps.
Flink 1.18&#43;Run the following Flink SQL.
-- use the catalog where you want to enable the privilege system USE CATALOG \`my-catalog\`; -- initialize privilege system by providing a root password -- change &#39;root-password&#39; to the password you want CALL sys.init_file_based_privilege(&#39;root-password&#39;); After the privilege system is enabled, please re-create the catalog and authenticate as root to create other users and grant them privileges.
Privilege system does not affect existing catalogs. That is, these catalogs can still access and modify the tables freely. Please drop and re-create all catalogs with the desired warehouse path if you want to use the privilege system in these catalogs.Accessing Privileged Catalogs#To access a privileged catalog and to be authenticated as a user, you need to define user and password catalog options when creating the catalog. For example, the following SQL creates a catalog while trying to be authenticated as root, whose password is mypassword.
FlinkCREATE CATALOG \`my-catalog\` WITH ( &#39;type&#39; = &#39;paimon&#39;, -- ... &#39;user&#39; = &#39;root&#39;, &#39;password&#39; = &#39;mypassword&#39; ); Creating Users#You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.
Do the following steps to create a user in the privilege system.
Flink 1.18&#43;Run the following Flink SQL.
-- use the catalog where you want to create a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG \`my-catalog\`; -- create a user authenticated by the specified password -- change &#39;user&#39; and &#39;password&#39; to the username and password you want CALL sys.create_privileged_user(&#39;user&#39;, &#39;password&#39;); Dropping Users#You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.
Do the following steps to drop a user in the privilege system.
Flink 1.18&#43;Run the following Flink SQL.
-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG \`my-catalog\`; -- change &#39;user&#39; to the username you want to drop CALL sys.drop_privileged_user(&#39;user&#39;); Granting Privileges to Users#You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.
Do the following steps to grant a user with privilege in the privilege system.
Flink 1.18&#43;Run the following Flink SQL.
-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG \`my-catalog\`; -- you can change &#39;user&#39; to the username you want, and &#39;SELECT&#39; to other privilege you want -- grant &#39;user&#39; with privilege &#39;SELECT&#39; on the whole catalog CALL sys.grant_privilege_to_user(&#39;user&#39;, &#39;SELECT&#39;); -- grant &#39;user&#39; with privilege &#39;SELECT&#39; on database my_db CALL sys.grant_privilege_to_user(&#39;user&#39;, &#39;SELECT&#39;, &#39;my_db&#39;); -- grant &#39;user&#39; with privilege &#39;SELECT&#39; on table my_db.my_tbl CALL sys.grant_privilege_to_user(&#39;user&#39;, &#39;SELECT&#39;, &#39;my_db&#39;, &#39;my_tbl&#39;); Revoking Privileges to Users#You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.
Do the following steps to revoke a privilege from user in the privilege system.
Flink 1.18&#43;Run the following Flink SQL.
-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG \`my-catalog\`; -- you can change &#39;user&#39; to the username you want, and &#39;SELECT&#39; to other privilege you want -- revoke &#39;user&#39; with privilege &#39;SELECT&#39; on the whole catalog CALL sys.revoke_privilege_from_user(&#39;user&#39;, &#39;SELECT&#39;); -- revoke &#39;user&#39; with privilege &#39;SELECT&#39; on database my_db CALL sys.revoke_privilege_from_user(&#39;user&#39;, &#39;SELECT&#39;, &#39;my_db&#39;); -- revoke &#39;user&#39; with privilege &#39;SELECT&#39; on table my_db.my_tbl CALL sys.revoke_privilege_from_user(&#39;user&#39;, &#39;SELECT&#39;, &#39;my_db&#39;, &#39;my_tbl&#39;); `}),e.add({id:97,href:"/concepts/spec/",title:"规范说明 Specification",section:"概念 Concepts",content:``}),e.add({id:98,href:"/maintenance/manage-branches/",title:"Manage Branches",section:"Maintenance",content:`Manage Branches#In streaming data processing, it&rsquo;s difficult to correct data for it may affect the existing data, and users will see the streaming provisional results, which is not expected.
We suppose the branch that the existing workflow is processing on is &lsquo;main&rsquo; branch, by creating custom data branch, it can help to do experimental tests and data validating for the new job on the existing table, which doesn&rsquo;t need to stop the existing reading / writing workflows and no need to copy data from the main branch.
By merge or replace branch operations, users can complete the correcting of data.
Create Branches#Paimon supports creating branch from a specific tag, or just creating an empty branch which means the initial state of the created branch is like an empty table.
Flink SQLRun the following sql:
-- create branch named &#39;branch1&#39; from tag &#39;tag1&#39; CALL sys.create_branch(&#39;default.T&#39;, &#39;branch1&#39;, &#39;tag1&#39;); -- create empty branch named &#39;branch1&#39; CALL sys.create_branch(&#39;default.T&#39;, &#39;branch1&#39;); Flink Action JarRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ create_branch \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --branch_name &lt;branch-name&gt; \\ [--tag_name &lt;tag-name&gt;] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Spark SQLRun the following sql:
-- create branch named &#39;branch1&#39; from tag &#39;tag1&#39; CALL sys.create_branch(&#39;default.T&#39;, &#39;branch1&#39;, &#39;tag1&#39;); -- create empty branch named &#39;branch1&#39; CALL sys.create_branch(&#39;default.T&#39;, &#39;empty_branch&#39;); Delete Branches#You can delete branch by its name.
Note: The Delete Branches operation only deletes the metadata file. If you want to clear the data written during the branch, use remove_orphan_filesFlink SQLRun the following sql:
CALL sys.delete_branch(&#39;default.T&#39;, &#39;branch1&#39;); Flink Action JarRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ delete_branch \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --branch_name &lt;branch-name&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Spark SQLRun the following sql:
CALL sys.delete_branch(&#39;default.T&#39;, &#39;branch1&#39;); Read / Write With Branch#You can read or write with branch as below.
Flink-- read from branch &#39;branch1&#39; SELECT * FROM \`t$branch_branch1\`; SELECT * FROM \`t$branch_branch1\` /*+ OPTIONS(&#39;consumer-id&#39; = &#39;myid&#39;) */; -- write to branch &#39;branch1&#39; INSERT INTO \`t$branch_branch1\` SELECT ... Spark SQL-- read from branch &#39;branch1&#39; SELECT * FROM \`t$branch_branch1\`; -- write to branch &#39;branch1&#39; INSERT INTO \`t$branch_branch1\` SELECT ... Spark DataFrame-- read from branch &#39;branch1&#39; spark.read.format(&#34;paimon&#34;).option(&#34;branch&#34;, &#34;branch1&#34;).table(&#34;t&#34;) Fast Forward#Fast-Forward the custom branch to main will delete all the snapshots, tags and schemas in the main branch that are created after the branch&rsquo;s initial tag. And copy snapshots, tags and schemas from the branch to the main branch.
If your branch modifies the schema, after Fast Forward, if it is Spark SQL, you can execute REFRESH TABLE my_table to clean up the cache to avoid inconsistencies caused by caching.
Flink SQLCALL sys.fast_forward(&#39;default.T&#39;, &#39;branch1&#39;); Flink Action JarRun the following command:
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ fast_forward \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --branch_name &lt;branch-name&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] Batch Reading from Fallback Branch#You can set the table option scan.fallback-branch so that when a batch job reads from the current branch, if a partition does not exist, the reader will try to read this partition from the fallback branch. For streaming read jobs, this feature is currently not supported, and will only produce results from the current branch.
What&rsquo;s the use case of this feature? Say you have created a Paimon table partitioned by date. You have a long-running streaming job which inserts records into Paimon, so that today&rsquo;s data can be queried in time. You also have a batch job which runs at every night to insert corrected records of yesterday into Paimon, so that the preciseness of the data can be promised.
When you query from this Paimon table, you would like to first read from the results of batch job. But if a partition (for example, today&rsquo;s partition) does not exist in its result, then you would like to read from the results of streaming job. In this case, you can create a branch for streaming job, and set scan.fallback-branch to this streaming branch.
Let&rsquo;s look at an example.
Flink-- create Paimon table CREATE TABLE T ( dt STRING NOT NULL, name STRING NOT NULL, amount BIGINT ) PARTITIONED BY (dt); -- create a branch for streaming job CALL sys.create_branch(&#39;default.T&#39;, &#39;test&#39;); -- set primary key and bucket number for the branch ALTER TABLE \`T$branch_test\` SET ( &#39;primary-key&#39; = &#39;dt,name&#39;, &#39;bucket&#39; = &#39;2&#39;, &#39;changelog-producer&#39; = &#39;lookup&#39; ); -- set fallback branch ALTER TABLE T SET ( &#39;scan.fallback-branch&#39; = &#39;test&#39; ); -- write records into the streaming branch INSERT INTO \`T$branch_test\` VALUES (&#39;20240725&#39;, &#39;apple&#39;, 4), (&#39;20240725&#39;, &#39;peach&#39;, 10), (&#39;20240726&#39;, &#39;cherry&#39;, 3), (&#39;20240726&#39;, &#39;pear&#39;, 6); -- write records into the default branch INSERT INTO T VALUES (&#39;20240725&#39;, &#39;apple&#39;, 5), (&#39;20240725&#39;, &#39;banana&#39;, 7); SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | | 20240726 | cherry | 3 | | 20240726 | pear | 6 | +------------------+------------------+--------+ */ -- reset fallback branch ALTER TABLE T RESET ( &#39;scan.fallback-branch&#39; ); -- now it only reads from default branch SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | +------------------+------------------+--------+ */ `}),e.add({id:99,href:"/maintenance/manage-partitions/",title:"Manage Partitions",section:"Maintenance",content:`Manage Partitions#Paimon provides multiple ways to manage partitions, including expire historical partitions by different strategies or mark a partition done to notify the downstream application that the partition has finished writing.
Expiring Partitions#You can set partition.expiration-time when creating a partitioned table. Paimon streaming sink will periodically check the status of partitions and delete expired partitions according to time.
How to determine whether a partition has expired: you can set partition.expiration-strategy when creating a partitioned table, this strategy determines how to extract the partition time and compare it with the current time to see if survival time has exceeded the partition.expiration-time. Expiration strategy supported values are:
values-time : The strategy compares the time extracted from the partition value with the current time, this strategy as the default. update-time : The strategy compares the last update time of the partition with the current time. What is the scenario for this strategy: Your partition value is non-date formatted. You only want to keep data that has been updated in the last n days/months/years. Data initialization imports a large amount of historical data. Note: After the partition expires, it is logically deleted and the latest snapshot cannot query its data. But the files in the file system are not immediately physically deleted, it depends on when the corresponding snapshot expires. See Expire Snapshots.An example for single partition field:
values-time strategy.
CREATE TABLE t (...) PARTITIONED BY (dt) WITH ( &#39;partition.expiration-time&#39; = &#39;7 d&#39;, &#39;partition.expiration-check-interval&#39; = &#39;1 d&#39;, &#39;partition.timestamp-formatter&#39; = &#39;yyyyMMdd&#39; -- this is required in \`values-time\` strategy. ); -- Let&#39;s say now the date is 2024-07-09，so before the date of 2024-07-02 will expire. insert into t values(&#39;pk&#39;, &#39;2024-07-01&#39;); -- An example for multiple partition fields CREATE TABLE t (...) PARTITIONED BY (other_key, dt) WITH ( &#39;partition.expiration-time&#39; = &#39;7 d&#39;, &#39;partition.expiration-check-interval&#39; = &#39;1 d&#39;, &#39;partition.timestamp-formatter&#39; = &#39;yyyyMMdd&#39;, &#39;partition.timestamp-pattern&#39; = &#39;$dt&#39; ); update-time strategy.
CREATE TABLE t (...) PARTITIONED BY (dt) WITH ( &#39;partition.expiration-time&#39; = &#39;7 d&#39;, &#39;partition.expiration-check-interval&#39; = &#39;1 d&#39;, &#39;partition.expiration-strategy&#39; = &#39;update-time&#39; ); -- The last update time of the partition is now, so it will not expire. insert into t values(&#39;pk&#39;, &#39;2024-01-01&#39;); -- Support non-date formatted partition. insert into t values(&#39;pk&#39;, &#39;par-1&#39;); More options:
OptionDefaultTypeDescriptionpartition.expiration-strategyvalues-timeStringSpecifies the expiration strategy for partition expiration. Possible values:values-time: The strategy compares the time extracted from the partition value with the current time.update-time: The strategy compares the last update time of the partition with the current time.partition.expiration-check-interval1 hDurationThe check interval of partition expiration.partition.expiration-time(none)DurationThe expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.partition.timestamp-formatter(none)StringThe formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.partition.timestamp-pattern(none)StringYou can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.end-input.check-partition-expirefalseBooleanWhether check partition expire after batch mode or bounded stream job finish.Partition Mark Done#You can use the option 'partition.mark-done-action' to configure the action when a partition needs to be mark done.
success-file: add &lsquo;_success&rsquo; file to directory. done-partition: add &lsquo;xxx.done&rsquo; partition to metastore. mark-event: mark partition event to metastore. http-report: report partition mark done to remote http server. custom: use policy class to create a mark-partition policy. These actions can be configured at the same time: &lsquo;done-partition,success-file,mark-event,custom&rsquo;. Paimon partition mark done can be triggered both by streaming write and batch write.
Streaming Mark Done#You can use the options 'partition.idle-time-to-done' to set a partition idle time to done duration. When a partition has no new data after this time duration, the mark done action will be triggered to indicate that the data is ready.
By default, Flink will use process time as idle time to trigger partition mark done. You can also use watermark to trigger partition mark done. This will make the partition mark done time more accurate when data is delayed. You can enable this by setting 'partition.mark-done-action.mode' = 'watermark'.
Batch Mark Done#For batch mode, you can trigger partition mark done when end input by setting 'partition.end-input-to-done'='true'.
`}),e.add({id:100,href:"/ecosystem/",title:"Ecosystem",section:"Apache Paimon",content:``}),e.add({id:101,href:"/cdc-ingestion/",title:"CDC Ingestion",section:"Apache Paimon",content:``}),e.add({id:102,href:"/maintenance/",title:"Maintenance",section:"Apache Paimon",content:``}),e.add({id:103,href:"/program-api/",title:"Program API",section:"Apache Paimon",content:``}),e.add({id:104,href:"/migration/",title:"Migration",section:"Apache Paimon",content:``}),e.add({id:105,href:"/flink/procedures/",title:"Procedures",section:"Engine Flink",content:`Procedures#Flink 1.18 and later versions support Call Statements, which make it easier to manipulate data and metadata of Paimon table by writing SQLs instead of submitting Flink jobs.
In 1.18, the procedure only supports passing arguments by position. You must pass all arguments in order, and if you don&rsquo;t want to pass some arguments, you must use '' as placeholder. For example, if you want to compact table default.t with parallelism 4, but you don&rsquo;t want to specify partitions and sort strategy, the call statement should be CALL sys.compact('default.t', '', '', '', 'sink.parallelism=4').
In higher versions, the procedure supports passing arguments by name. You can pass arguments in any order and any optional argument can be omitted. For the above example, the call statement is CALL sys.compact(\`table\` =&gt; 'default.t', options =&gt; 'sink.parallelism=4').
Specify partitions: we use string to represent partition filter. &ldquo;,&rdquo; means &ldquo;AND&rdquo; and &ldquo;;&rdquo; means &ldquo;OR&rdquo;. For example, if you want to specify two partitions date=01 and date=02, you need to write &lsquo;date=01;date=02&rsquo;; If you want to specify one partition with date=01 and day=01, you need to write &lsquo;date=01,day=01&rsquo;.
Table options syntax: we use string to represent table options. The format is &lsquo;key1=value1,key2=value2&hellip;&rsquo;.
All available procedures are listed below.
Procedure NameUsageExplanationExamplecompact-- Use named argumentCALL [catalog.]sys.compact(\`table\` => 'table', partitions => 'partitions', order_strategy => 'order_strategy', order_by => 'order_by', options => 'options', \`where\` => 'where', partition_idle_time => 'partition_idle_time',compact_strategy => 'compact_strategy') -- Use indexed argumentCALL [catalog.]sys.compact('table') CALL [catalog.]sys.compact('table', 'partitions') CALL [catalog.]sys.compact('table', 'order_strategy', 'order_by') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time', 'compact_strategy') To compact a table. Arguments:table(required): the target table identifier.partitions(optional): partition filter.order_strategy(optional): 'order' or 'zorder' or 'hilbert' or 'none'.order_by(optional): the columns need to be sort. Left empty if 'order_strategy' is 'none'.options(optional): additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.where(optional): partition predicate(Can't be used together with "partitions"). Note: as where is a keyword,a pair of backticks need to add around like \`where\`.partition_idle_time(optional): this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.compact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.-- use partition filter CALL sys.compact(\`table\` => 'default.T', partitions => 'p=0', order_strategy => 'zorder', order_by => 'a,b', options => 'sink.parallelism=4') -- use partition predicate CALL sys.compact(\`table\` => 'default.T', \`where\` => 'dt>10 and h<20', order_strategy => 'zorder', order_by => 'a,b', options => 'sink.parallelism=4')compact_database-- Use named argumentCALL [catalog.]sys.compact_database(including_databases => 'includingDatabases', mode => 'mode', including_tables => 'includingTables', excluding_tables => 'excludingTables', table_options => 'tableOptions', partition_idle_time => 'partitionIdleTime',compact_strategy => 'compact_strategy') -- Use indexed argumentCALL [catalog.]sys.compact_database() CALL [catalog.]sys.compact_database('includingDatabases') CALL [catalog.]sys.compact_database('includingDatabases', 'mode') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime')CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime', 'compact_strategy')To compact databases. Arguments:includingDatabases: to specify databases. You can use regular expression.mode: compact mode. "divided": start a sink for each table, detecting the new table requires restarting the job;"combined" (default): start a single combined sink for all tables, the new table will be automatically detected.includingTables: to specify tables. You can use regular expression.excludingTables: to specify tables that are not compacted. You can use regular expression.tableOptions: additional dynamic options of the table.partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted.compact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.CALL sys.compact_database(including_databases => 'db1|db2', mode => 'combined', including_tables => 'table_.*', excluding_tables => 'ignore', table_options => 'sink.parallelism=4',compat_strategy => 'full')create_tag-- Use named argument-- based on the specified snapshot CALL [catalog.]sys.create_tag(\`table\` => 'identifier', tag => 'tagName', snapshot_id => snapshotId) -- based on the latest snapshot CALL [catalog.]sys.create_tag(\`table\` => 'identifier', tag => 'tagName') -- Use indexed argument-- based on the specified snapshot CALL [catalog.]sys.create_tag('identifier', 'tagName', snapshotId) -- based on the latest snapshot CALL [catalog.]sys.create_tag('identifier', 'tagName')To create a tag based on given snapshot. Arguments:table: the target table identifier. Cannot be empty.tagName: name of the new tag.snapshotId (Long): id of the snapshot which the new tag is based on.time_retained: The maximum time retained for newly created tags.CALL sys.create_tag(\`table\` => 'default.T', tag => 'my_tag', snapshot_id => cast(10 as bigint), time_retained => '1 d')create_tag_from_timestamp-- Create a tag from the first snapshot whose commit-time greater than the specified timestamp. -- Use named argumentCALL [catalog.]sys.create_tag_from_timestamp(\`table\` => 'identifier', tag => 'tagName', timestamp => timestamp, time_retained => time_retained) -- Use indexed argumentCALL [catalog.]sys.create_tag_from_timestamp('identifier', 'tagName', timestamp, time_retained)To create a tag based on given timestamp. Arguments:table: the target table identifier. Cannot be empty.tag: name of the new tag.timestamp (Long): Find the first snapshot whose commit-time greater than this timestamp.time_retained : The maximum time retained for newly created tags.-- for Flink 1.18CALL sys.create_tag_from_timestamp('default.T', 'my_tag', 1724404318750, '1 d')-- for Flink 1.19 and laterCALL sys.create_tag_from_timestamp(\`table\` => 'default.T', \`tag\` => 'my_tag', \`timestamp\` => 1724404318750, time_retained => '1 d')create_tag_from_watermark-- Create a tag from the first snapshot whose watermark greater than the specified timestamp.-- Use named argumentCALL [catalog.]sys.create_tag_from_watermark(\`table\` => 'identifier', tag => 'tagName', watermark => watermark, time_retained => time_retained) -- Use indexed argumentCALL [catalog.]sys.create_tag_from_watermark('identifier', 'tagName', watermark, time_retained)To create a tag based on given watermark timestamp. Arguments:table: the target table identifier. Cannot be empty.tag: name of the new tag.watermark (Long): Find the first snapshot whose watermark greater than the specified watermark.time_retained : The maximum time retained for newly created tags.-- for Flink 1.18CALL sys.create_tag_from_watermark('default.T', 'my_tag', 1724404318750, '1 d')-- for Flink 1.19 and laterCALL sys.create_tag_from_watermark(\`table\` => 'default.T', \`tag\` => 'my_tag', \`watermark\` => 1724404318750, time_retained => '1 d')delete_tag-- Use named argumentCALL [catalog.]sys.delete_tag(\`table\` => 'identifier', tag => 'tagName') -- Use indexed argumentCALL [catalog.]sys.delete_tag('identifier', 'tagName')To delete a tag. Arguments:table: the target table identifier. Cannot be empty.tagName: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.CALL sys.delete_tag(\`table\` => 'default.T', tag => 'my_tag')replace_tag-- Use named argument-- replace tag with new time retained CALL [catalog.]sys.replace_tag(\`table\` => 'identifier', tag => 'tagName', time_retained => 'timeRetained') -- replace tag with new snapshot id and time retained CALL [catalog.]sys.replace_tag(\`table\` => 'identifier', snapshot_id => 'snapshotId') -- Use indexed argument-- replace tag with new snapshot id and time retained CALL [catalog.]sys.replace_tag('identifier', 'tagName', 'snapshotId', 'timeRetained') To replace an existing tag with new tag info. Arguments:table: the target table identifier. Cannot be empty.tag: name of the existed tag. Cannot be empty.snapshot(Long): id of the snapshot which the tag is based on, it is optional.time_retained: The maximum time retained for the existing tag, it is optional.-- for Flink 1.18CALL sys.replace_tag('default.T', 'my_tag', 5, '1 d')-- for Flink 1.19 and laterCALL sys.replace_tag(\`table\` => 'default.T', tag => 'my_tag', snapshot_id => 5, time_retained => '1 d')expire_tagsCALL [catalog.]sys.expire_tags('identifier', 'older_than')To expire tags by time. Arguments:table: the target table identifier. Cannot be empty.older_than: tagCreateTime before which tags will be removed.CALL sys.expire_tags(table => 'default.T', older_than => '2024-09-06 11:00:00')merge_into-- for Flink 1.18CALL [catalog.]sys.merge_into('identifier','targetAlias','sourceSqls','sourceTable','mergeCondition','matchedUpsertCondition','matchedUpsertSetting','notMatchedInsertCondition','notMatchedInsertValues','matchedDeleteCondition')-- for Flink 1.19 and later CALL [catalog.]sys.merge_into(target_table => 'identifier',target_alias => 'targetAlias',source_sqls => 'sourceSqls',source_table => 'sourceTable',merge_condition => 'mergeCondition',matched_upsert_condition => 'matchedUpsertCondition',matched_upsert_setting => 'matchedUpsertSetting',not_matched_insert_condition => 'notMatchedInsertCondition',not_matched_insert_values => 'notMatchedInsertValues',matched_delete_condition => 'matchedDeleteCondition',not_matched_by_source_upsert_condition => 'notMatchedBySourceUpsertCondition',not_matched_by_source_upsert_setting => 'notMatchedBySourceUpsertSetting',not_matched_by_source_delete_condition => 'notMatchedBySourceDeleteCondition') To perform "MERGE INTO" syntax. See merge_into action fordetails of arguments.-- for matched order rows,-- increase the price,-- and if there is no match, -- insert the order from-- the source table-- for Flink 1.18CALL sys.merge_into('default.T','','','default.S','T.id=S.order_id','','price=T.price+20','','*','')-- for Flink 1.19 and later CALL sys.merge_into(target_table => 'default.T',source_table => 'default.S',merge_condition => 'T.id=S.order_id',matched_upsert_setting => 'price=T.price+20',not_matched_insert_values => '*')remove_orphan_files-- Use named argumentCALL [catalog.]sys.remove_orphan_files(\`table\` => 'identifier', older_than => 'olderThan', dry_run => 'dryRun', mode => 'mode') -- Use indexed argumentCALL [catalog.]sys.remove_orphan_files('identifier')CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan')CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun')CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism')CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism','mode')To remove the orphan data files and metadata files. Arguments:table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.olderThan: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.dryRun: when true, view only orphan files, don't actually remove files. Default is false.parallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.mode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.CALL sys.remove_orphan_files(\`table\` => 'default.T', older_than => '2023-10-31 12:00:00')CALL sys.remove_orphan_files(\`table\` => 'default.*', older_than => '2023-10-31 12:00:00')CALL sys.remove_orphan_files(\`table\` => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => true)CALL sys.remove_orphan_files(\`table\` => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => false, parallelism => '5')CALL sys.remove_orphan_files(\`table\` => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => false, parallelism => '5', mode => 'local')remove_unexisting_files-- Use named argumentCALL [catalog.]sys.remove_unexisting_files(\`table\` => 'identifier', dry_run => 'dryRun', parallelism => 'parallelism') -- Use indexed argumentCALL [catalog.]sys.remove_unexisting_files('identifier')CALL [catalog.]sys.remove_unexisting_files('identifier', 'dryRun', 'parallelism')Procedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments:table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.dry_run (optional): only check what files will be removed, but not really remove them. Default is false.parallelism (optional): number of parallelisms to check files in the manifests.Note that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.-- remove unexisting data files in the table \`mydb.myt\`CALL sys.remove_unexisting_files(\`table\` => 'mydb.myt')-- only check what files will be removed, but not really remove them (dry run)CALL sys.remove_unexisting_files(\`table\` => 'mydb.myt', \`dry_run\` = true)reset_consumer-- Use named argumentCALL [catalog.]sys.reset_consumer(\`table\` => 'identifier', consumer_id => 'consumerId', next_snapshot_id => 'nextSnapshotId') -- Use indexed argument-- reset the new next snapshot id in the consumerCALL [catalog.]sys.reset_consumer('identifier', 'consumerId', nextSnapshotId)-- delete consumerCALL [catalog.]sys.reset_consumer('identifier', 'consumerId')To reset or delete consumer. Arguments:table: the target table identifier. Cannot be empty.consumerId: consumer to be reset or deleted.nextSnapshotId (Long): the new next snapshot id of the consumer.CALL sys.reset_consumer(\`table\` => 'default.T', consumer_id => 'myid', next_snapshot_id => cast(10 as bigint))clear_consumers-- Use named argumentCALL [catalog.]sys.clear_consumers(\`table\` => 'identifier', including_consumers => 'includingConsumers', excluding_consumers => 'excludingConsumers') -- Use indexed argument-- clear all consumers in the tableCALL [catalog.]sys.clear_consumers('identifier')-- clear some consumers in the table (accept regular expression)CALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers')-- exclude some consumers (accept regular expression)CALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers', 'excludingConsumers')To reset or delete consumer. Arguments:table: the target table identifier. Cannot be empty.includingConsumers: consumers to be cleared.excludingConsumers: consumers which not to be cleared.CALL sys.clear_consumers(\`table\` => 'default.T')CALL sys.clear_consumers(\`table\` => 'default.T', including_consumers => 'myid.*')CALL sys.clear_consumers(table => 'default.T', including_consumers => '', excluding_consumers => 'myid1.*')CALL sys.clear_consumers(table => 'default.T', including_consumers => 'myid.*', excluding_consumers => 'myid1.*')rollback_to-- for Flink 1.18-- rollback to a snapshotCALL [catalog.]sys.rollback_to('identifier', snapshotId)-- rollback to a tagCALL [catalog.]sys.rollback_to('identifier', 'tagName')-- for Flink 1.19 and later-- rollback to a snapshotCALL [catalog.]sys.rollback_to(\`table\` => 'identifier', snapshot_id => snapshotId)-- rollback to a tagCALL [catalog.]sys.rollback_to(\`table\` => 'identifier', tag => 'tagName')To rollback to a specific version of target table. Argument:table: the target table identifier. Cannot be empty.snapshotId (Long): id of the snapshot that will roll back to.tagName: name of the tag that will roll back to.-- for Flink 1.18CALL sys.rollback_to('default.T', 10)-- for Flink 1.19 and laterCALL sys.rollback_to(\`table\` => 'default.T', snapshot_id => 10)rollback_to_timestamp-- for Flink 1.18-- rollback to the snapshot which earlier or equal than timestamp.CALL [catalog.]sys.rollback_to_timestamp('identifier', timestamp)-- for Flink 1.19 and later-- rollback to the snapshot which earlier or equal than timestamp.CALL [catalog.]sys.rollback_to_timestamp(\`table\` => 'default.T', \`timestamp\` => timestamp)To rollback to the snapshot which earlier or equal than timestamp. Argument:table: the target table identifier. Cannot be empty.timestamp (Long): Roll back to the snapshot which earlier or equal than timestamp.-- for Flink 1.18CALL sys.rollback_to_timestamp('default.T', 10)-- for Flink 1.19 and laterCALL sys.rollback_to_timestamp(\`table\` => 'default.T', timestamp => 1730292023000)rollback_to_watermark-- for Flink 1.18-- rollback to the snapshot which earlier or equal than watermark.CALL [catalog.]sys.rollback_to_watermark('identifier', watermark)-- for Flink 1.19 and later-- rollback to the snapshot which earlier or equal than watermark.CALL [catalog.]sys.rollback_to_watermark(\`table\` => 'default.T', \`watermark\` => watermark)To rollback to the snapshot which earlier or equal than watermark. Argument:table: the target table identifier. Cannot be empty.watermark (Long): Roll back to the snapshot which earlier or equal than watermark.-- for Flink 1.18CALL sys.rollback_to_watermark('default.T', 1730292023000)-- for Flink 1.19 and laterCALL sys.rollback_to_watermark(\`table\` => 'default.T', watermark => 1730292023000)purge_files-- clear table with purge files.CALL [catalog.]sys.purge_files('identifier')To clear table with purge files. Argument:table: the target table identifier. Cannot be empty.CALL sys.purge_files('default.T')migrate_database-- for Flink 1.18-- migrate all hive tables in database to paimon tables.CALL [catalog.]sys.migrate_database('connector', 'dbIdentifier', 'options'[, &ltparallelism&gt])-- for Flink 1.19 and later-- migrate all hive tables in database to paimon tables.CALL [catalog.]sys.migrate_database(connector => 'connector', source_database => 'dbIdentifier', options => 'options'[, &ltparallelism => parallelism&gt])To migrate all hive tables in database to paimon table. Argument:connector: the origin database's type to be migrated, such as hive. Cannot be empty.source_database: name of the origin database to be migrated. Cannot be empty.options: the table options of the paimon table to migrate.parallelism: the parallelism for migrate process, default is core numbers of machine.-- for Flink 1.18CALL sys.migrate_database('hive', 'db01', 'file.format=parquet', 6)-- for Flink 1.19 and laterCALL sys.migrate_database(connector => 'hive', source_database => 'db01', options => 'file.format=parquet', parallelism => 6)migrate_table-- migrate hive table to a paimon table.CALL [catalog.]sys.migrate_table(connector => 'connector', source_table => 'tableIdentifier', options => 'options'[, &ltparallelism => parallelism&gt])To migrate hive table to a paimon table. Argument:connector: the origin table's type to be migrated, such as hive. Cannot be empty.source_table: name of the origin table to be migrated. Cannot be empty.target_table: name of the target paimon table to migrate. If not set would keep the same name with origin tableoptions: the table options of the paimon table to migrate.parallelism: the parallelism for migrate process, default is core numbers of machine.delete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is trueCALL sys.migrate_table(connector => 'hive', source_table => 'db01.t1', options => 'file.format=parquet', parallelism => 6)migrate_iceberg_table-- Use named argumentCALL sys.migrate_iceberg_table(source_table => 'database_name.table_name', iceberg_options => 'iceberg_options', options => 'paimon_options', parallelism => parallelism);-- Use indexed argumentCALL sys.migrate_iceberg_table('source_table','iceberg_options', 'options', 'parallelism');To migrate iceberg table to paimon. Arguments:source_table: string type, is used to specify the source iceberg table to migrate, it's required.iceberg_options: string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it's required.options: string type, is used to specify the additional options for the target paimon table, it's optional.parallelism: integer type, is used to specify the parallelism of the migration job, it's optional.CALL sys.migrate_iceberg_table(source_table => 'iceberg_db.iceberg_tbl',iceberg_options => 'metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse');expire_snapshots-- Use named argumentCALL [catalog.]sys.expire_snapshots(\`table\` => 'identifier', retain_max => 'retain_max', retain_min => 'retain_min', older_than => 'older_than', max_deletes => 'max_deletes', options => 'key1=value1,key2=value2') -- Use indexed argument-- for Flink 1.18CALL [catalog.]sys.expire_snapshots(table, retain_max)-- for Flink 1.19 and laterCALL [catalog.]sys.expire_snapshots(table, retain_max, retain_min, older_than, max_deletes)To expire snapshots. Argument:table: the target table identifier. Cannot be empty.retain_max: the maximum number of completed snapshots to retain.retain_min: the minimum number of completed snapshots to retain.order_than: timestamp before which snapshots will be removed.max_deletes: the maximum number of snapshots that can be deleted at once.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.-- for Flink 1.18CALL sys.expire_snapshots('default.T', 2)-- for Flink 1.19 and laterCALL sys.expire_snapshots(\`table\` => 'default.T', retain_max => 2)CALL sys.expire_snapshots(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00')CALL sys.expire_snapshots(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00', retain_min => 10)CALL sys.expire_snapshots(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00', max_deletes => 10, options => 'snapshot.expire.limit=1')expire_changelogs-- Use named argumentCALL [catalog.]sys.expire_changelogs(\`table\` => 'identifier', retain_max => 'retain_max', retain_min => 'retain_min', older_than => 'older_than', max_deletes => 'max_deletes') delete_all => 'delete_all') -- Use indexed argument-- for Flink 1.18CALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes)CALL [catalog.]sys.expire_changelogs(table, delete_all)-- for Flink 1.19 and laterCALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes, delete_all)To expire changelogs. Argument:table: the target table identifier. Cannot be empty.retain_max: the maximum number of completed changelogs to retain.retain_min: the minimum number of completed changelogs to retain.order_than: timestamp before which changelogs will be removed.max_deletes: the maximum number of changelogs that can be deleted at once.delete_all: whether to delete all separated changelogs.-- for Flink 1.18CALL sys.expire_changelogs('default.T', 4, 2, '2024-01-01 12:00:00', 2)CALL sys.expire_changelogs('default.T', true)-- for Flink 1.19 and laterCALL sys.expire_changelogs(\`table\` => 'default.T', retain_max => 2)CALL sys.expire_changelogs(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00')CALL sys.expire_changelogs(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00', retain_min => 10)CALL sys.expire_changelogs(\`table\` => 'default.T', older_than => '2024-01-01 12:00:00', max_deletes => 10)CALL sys.expire_changelogs(\`table\` => 'default.T', delete_all => true)expire_partitionsCALL [catalog.]sys.expire_partitions(table, expiration_time, timestamp_formatter, expire_strategy, options)To expire partitions. Argument:table: the target table identifier. Cannot be empty.expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.timestamp_formatter: the formatter to format timestamp from string.timestamp_pattern: the pattern to get a timestamp from partitions.expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.max_expires: The maximum of limited expired partitions, it is optional.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.-- for Flink 1.18CALL sys.expire_partitions('default.T', '1 d', 'yyyy-MM-dd', '$dt', 'values-time')-- for Flink 1.19 and laterCALL sys.expire_partitions(\`table\` => 'default.T', expiration_time => '1 d', timestamp_formatter => 'yyyy-MM-dd', expire_strategy => 'values-time')CALL sys.expire_partitions(\`table\` => 'default.T', expiration_time => '1 d', timestamp_formatter => 'yyyy-MM-dd HH:mm', timestamp_pattern => '$dt $hm', expire_strategy => 'values-time', options => 'partition.expiration-max-num=2')repair-- repair all databases and tables in catalogCALL [catalog.]sys.repair()-- repair all tables in a specific databaseCALL [catalog.]sys.repair('databaseName')-- repair a tableCALL [catalog.]sys.repair('databaseName.tableName')-- repair database and table in a string if you specify multiple tags, delimiter is ','CALL [catalog.]sys.repair('databaseName01,database02.tableName01,database03')Synchronize information from the file system to Metastore. Argument:empty: all databases and tables in catalog.databaseName : the target database name.tableName: the target table identifier.CALL sys.repair(\`table\` => 'test_db.T')rewrite_file_index-- Use named argumentCALL [catalog.]sys.rewrite_file_index(&lt\`table\` => identifier&gt [, &ltpartitions => partitions&gt])-- Use indexed argumentCALL [catalog.]sys.rewrite_file_index(&ltidentifier&gt [, &ltpartitions&gt])Rewrite the file index for the table. Argument:table: &ltdatabaseName&gt.&lttableName&gt.partitions : specific partitions.-- rewrite the file index for the whole tableCALL sys.rewrite_file_index(\`table\` => 'test_db.T')-- repair all tables in a specific partitionCALL sys.rewrite_file_index(\`table\` => 'test_db.T', partitions => 'pt=a')create_branch-- Use named argumentCALL [catalog.]sys.create_branch(\`table\` => 'identifier', branch => 'branchName', tag => 'tagName')-- Use indexed argument-- based on the specified tag CALL [catalog.]sys.create_branch('identifier', 'branchName', 'tagName')-- based on the specified branch's tag CALL [catalog.]sys.create_branch('branch_table', 'branchName', 'tagName')-- create empty branch CALL [catalog.]sys.create_branch('identifier', 'branchName')To create a branch based on given tag, or just create empty branch. Arguments:table: the target table identifier or branch identifier. Cannot be empty.branchName: name of the new branch.tagName: name of the tag which the new branch is based on.CALL sys.create_branch(\`table\` => 'default.T', branch => 'branch1', tag => 'tag1')-- based on the specified branch's tag CALL sys.create_branch(\`table\` => 'default.T$branch_existBranchName', branch => 'branch1', tag => 'tag1')CALL sys.create_branch(\`table\` => 'default.T', branch => 'branch1')delete_branch-- Use named argumentCALL [catalog.]sys.delete_branch(\`table\` => 'identifier', branch => 'branchName')-- Use indexed argumentCALL [catalog.]sys.delete_branch('identifier', 'branchName')To delete a branch. Arguments:table: the target table identifier. Cannot be empty.branchName: name of the branch to be deleted. If you specify multiple branches, delimiter is ','.CALL sys.delete_branch(\`table\` => 'default.T', branch => 'branch1')fast_forward-- Use named argumentCALL [catalog.]sys.fast_forward(\`table\` => 'identifier', branch => 'branchName')-- Use indexed argumentCALL [catalog.]sys.fast_forward('identifier', 'branchName')To fast_forward a branch to main branch. Arguments:table: the target table identifier. Cannot be empty.branchName: name of the branch to be merged.CALL sys.fast_forward(\`table\` => 'default.T', branch => 'branch1')refresh_object_tableCALL [catalog.]sys.refresh_object_table('identifier')To refresh_object_table a object table. Arguments:table: the target table identifier. Cannot be empty.CALL sys.refresh_object_table('default.T')compact_manifestCALL [catalog.]sys.compact_manifest(\`table\` => 'identifier')CALL [catalog.]sys.compact_manifest(\`table\` => 'identifier', 'options' => 'key1=value1,key2=value2')To compact_manifest the manifests. Arguments:table: the target table identifier. Cannot be empty.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.CALL sys.compact_manifest(\`table\` => 'default.T')rescaleCALL [catalog.]sys.rescale(\`table\` => 'identifier', \`bucket_num\` => bucket_num, \`partition\` => 'partition', \`scan_parallelism\` => 'scan_parallelism', \`sink_parallelism\` => 'sink_parallelism')Rescale one partition of a table. Arguments:table: The target table identifier. Cannot be empty.bucket_num: Resulting bucket number after rescale. The default value of argument bucket_num is the current bucket number of the table. Cannot be empty for postpone bucket tables.partition: What partition to rescale. For partitioned table this argument cannot be empty.scan_parallelism: Parallelism of source operator. The default value is the current bucket number of the partition.sink_parallelism: Parallelism of sink operator. The default value is equal to bucket_num.CALL sys.rescale(\`table\` => 'default.T', \`bucket_num\` => 16, \`partition\` => 'dt=20250217,hh=08')alter_view_dialect-- add dialect in the viewCALL [catalog.]sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query')CALL [catalog.]sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'add', \`query\` => 'query')-- update dialect in the viewCALL [catalog.]sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query')CALL [catalog.]sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'update', \`query\` => 'query')-- drop dialect in the viewCALL [catalog.]sys.alter_view_dialect('view_identifier', 'drop', 'flink')CALL [catalog.]sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'drop')To alter view dialect. Arguments:view: the target view identifier. Cannot be empty.action: define change action like: add, update, drop. Cannot be empty.engine: when engine which is not flink need define it.query: query for the dialect when action is add and update it couldn't be empty.-- add dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'add', \`query\` => 'query')-- update dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'update', \`query\` => 'query')-- drop dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'drop', 'flink')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'drop')create_functionCALL [catalog.]sys.create_function('function_identifier','[{"id": 0, "name":"length", "type":"INT"}, {"id": 1, "name":"width", "type":"INT"}]','[{"id": 0, "name":"area", "type":"BIGINT"}]',true, 'comment', 'k1=v1,k2=v2')To create a function. Arguments:function: the target function identifier. Cannot be empty.inputParams: inputParams of the function.returnParams: returnParams of the function.deterministic: Whether the function is deterministic.comment: The comment for the function.options: the additional dynamic options of the function.CALL sys.create_function(\`function\` => 'function_identifier',inputParams => '[{"id": 0, "name":"length", "type":"INT"}, {"id": 1, "name":"width", "type":"INT"}]',returnParams => '[{"id": 0, "name":"area", "type":"BIGINT"}]',deterministic => true,comment => 'comment',options => 'k1=v1,k2=v2')alter_functionCALL [catalog.]sys.alter_function('function_identifier','{"action" : "addDefinition", "name" : "flink", "definition" : {"type" : "file", "fileResources" : [{"resourceType": "JAR", "uri": "oss://mybucket/xxxx.jar"}], "language": "JAVA", "className": "xxxx", "functionName": "functionName" } }')To alter a function. Arguments:function: the target function identifier. Cannot be empty.change: change of the function.CALL sys.alter_function(\`function\` => 'function_identifier',\`change\` => '{"action" : "addDefinition", "name" : "flink", "definition" : {"type" : "file", "fileResources" : [{"resourceType": "JAR", "uri": "oss://mybucket/xxxx.jar"}], "language": "JAVA", "className": "xxxx", "functionName": "functionName" } }')drop_functionCALL [catalog.]sys.drop_function('function_identifier')To drop a function. Arguments:function: the target function identifier. Cannot be empty.CALL sys.drop_function(\`function\` => 'function_identifier')`}),e.add({id:106,href:"/flink/action-jars/",title:"Action Jars",section:"Engine Flink",content:`Action Jars#After the Flink Local Cluster has been started, you can execute the action jar by using the following command.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ &lt;action&gt; &lt;args&gt; The following command is used to compact a table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ compact \\ --path &lt;TABLE_PATH&gt; Merging into table#Paimon supports &ldquo;MERGE INTO&rdquo; via submitting the &lsquo;merge_into&rsquo; job through flink run.
Important table properties setting:
Only primary key table supports this feature. The action won&rsquo;t produce UPDATE_BEFORE, so it&rsquo;s not recommended to set &lsquo;changelog-producer&rsquo; = &lsquo;input&rsquo;. The design referenced such syntax:
MERGE INTO target-table USING source_table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not_matched_condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge_into action use &ldquo;upsert&rdquo; semantics instead of &ldquo;update&rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, &ldquo;upsert&rdquo; is useful.
Run the following command to submit a &lsquo;merge_into&rsquo; job for the table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;target-table&gt; \\ [--target_as &lt;target-table-alias&gt;] \\ --source_table &lt;source_table-name&gt; \\ [--source_sql &lt;sql&gt; ...]\\ --on &lt;merge-condition&gt; \\ --merge_actions &lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete&gt; \\ --matched_upsert_condition &lt;matched-condition&gt; \\ --matched_upsert_set &lt;upsert-changes&gt; \\ --matched_delete_condition &lt;matched-condition&gt; \\ --not_matched_insert_condition &lt;not-matched-condition&gt; \\ --not_matched_insert_values &lt;insert-values&gt; \\ --not_matched_by_source_upsert_condition &lt;not-matched-by-source-condition&gt; \\ --not_matched_by_source_upsert_set &lt;not-matched-upsert-changes&gt; \\ --not_matched_by_source_delete_condition &lt;not-matched-by-source-condition&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] You can pass sqls by &#39;--source_sql &lt;sql&gt; [, --source_sql &lt;sql&gt; ...]&#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table T \\ --source_table S \\ --on &#34;T.id = S.order_id&#34; \\ --merge_actions \\ matched-upsert,matched-delete \\ --matched_upsert_condition &#34;T.price &gt; 100&#34; \\ --matched_upsert_set &#34;mark = &#39;important&#39;&#34; \\ --matched_delete_condition &#34;T.price &lt; 10&#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table T \\ --source_table S \\ --on &#34;T.id = S.order_id&#34; \\ --merge_actions \\ matched-upsert,not-matched-insert \\ --matched_upsert_set &#34;price = T.price + 20&#34; \\ --not_matched_insert_values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is &#39;trivial&#39;, delete them: ./flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table T \\ --source_table S \\ --on &#34;T.id = S.order_id&#34; \\ --merge_actions \\ not-matched-by-source-upsert,not-matched-by-source-delete \\ --not_matched_by_source_upsert_condition &#34;T.mark &lt;&gt; &#39;trivial&#39;&#34; \\ --not_matched_by_source_upsert_set &#34;price = T.price - 20&#34; \\ --not_matched_by_source_delete_condition &#34;T.mark = &#39;trivial&#39;&#34; -- A --source_sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table T \\ --source_sql &#34;CREATE CATALOG test_cat WITH (...)&#34; \\ --source_sql &#34;CREATE TEMPORARY VIEW test_cat.\`default\`.S AS SELECT order_id, price, &#39;important&#39; FROM important_order&#34; \\ --source_table test_cat.default.S \\ --on &#34;T.id = S.order_id&#34; \\ --merge_actions not-matched-insert\\ --not_matched_insert_values * The term &lsquo;matched&rsquo; explanation:
matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not_matched_condition (source - target). not matched by source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source). Parameters format:
matched_upsert_changes:
col = &lt;source_table&gt;.col | expression [, &hellip;] (Means setting &lt;target_table&gt;.col with given value. Do not add &lsquo;&lt;target_table&gt;.&rsquo; before &lsquo;col&rsquo;.)
Especially, you can use &lsquo;*&rsquo; to set columns with all source columns (require target table&rsquo;s schema is equal to source&rsquo;s). not_matched_upsert_changes is similar to matched_upsert_changes, but you cannot reference source table&rsquo;s column or use &lsquo;*&rsquo;. insert_values:
col1, col2, &hellip;, col_end
Must specify values of all columns. For each column, you can reference &lt;source_table&gt;.col or use an expression.
Especially, you can use &lsquo;*&rsquo; to insert with all source columns (require target table&rsquo;s schema is equal to source&rsquo;s). not_matched_condition cannot use target table&rsquo;s columns to construct condition expression. not_matched_by_source_condition cannot use source table&rsquo;s columns to construct condition expression. Target alias cannot be duplicated with existed table name. If the source table is not in the current catalog and current database, the source-table-name must be qualified (database.table or catalog.database.table if created a new catalog). For examples:
(1) If source table &lsquo;my_source&rsquo; is in &lsquo;my_db&rsquo;, qualify it:
--source_table &ldquo;my_db.my_source&rdquo;
(2) Example for sqls:
When sqls changed current catalog and database, it&rsquo;s OK to not qualify the source table name:
--source_sql &ldquo;CREATE CATALOG my_cat WITH (&hellip;)&rdquo;
--source_sql &ldquo;USE CATALOG my_cat&rdquo;
--source_sql &ldquo;CREATE DATABASE my_db&rdquo;
--source_sql &ldquo;USE my_db&rdquo;
--source_sql &ldquo;CREATE TABLE S &hellip;&rdquo;
--source_table S
but you must qualify it in the following case:
--source_sql &ldquo;CREATE CATALOG my_cat WITH (&hellip;)&rdquo;
--source_sql &ldquo;CREATE TABLE my_cat.\`default\`.S &hellip;&rdquo;
--source_table my_cat.default.S
You can use just &lsquo;S&rsquo; as source table name in following arguments. At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally in Shell, please quote them with &quot;&quot; to escape blank spaces and use &lsquo;\\&rsquo; to escape special characters in statement. For example:
--source_sql &ldquo;CREATE TABLE T (k INT) WITH (&lsquo;special-key&rsquo; = &lsquo;123\\!&rsquo;)&rdquo; For more information of &lsquo;merge_into&rsquo;, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ merge_into --help Deleting from table#In Flink 1.16 and previous versions, Paimon only supports deleting records via submitting the &lsquo;delete&rsquo; job through flink run.
Run the following command to submit a &lsquo;delete&rsquo; job for the table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ delete \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ --where &lt;filter_spec&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] filter_spec is equal to the &#39;WHERE&#39; clause in SQL DELETE statement. Examples: age &gt;= 18 AND age &lt;= 60 animal &lt;&gt; &#39;cat&#39; id &gt; (SELECT count(*) FROM employee) For more information of &lsquo;delete&rsquo;, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ delete --help Drop Partition#Run the following command to submit a &lsquo;drop_partition&rsquo; job for the table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ drop_partition \\ --warehouse &lt;warehouse-path&gt; \\ --database &lt;database-name&gt; \\ --table &lt;table-name&gt; \\ [--partition &lt;partition_spec&gt; [--partition &lt;partition_spec&gt; ...]] \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] partition_spec: key1=value1,key2=value2... For more information of &lsquo;drop_partition&rsquo;, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ drop_partition --help Rewrite File Index#Run the following command to submit a &lsquo;rewrite_file_index&rsquo; job for the table.
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ rewrite_file_index \\ --warehouse &lt;warehouse-path&gt; \\ --identifier &lt;database.table&gt; \\ [--catalog_conf &lt;paimon-catalog-conf&gt; [--catalog_conf &lt;paimon-catalog-conf&gt; ...]] For more information of &lsquo;rewrite_file_index&rsquo;, see
&lt;FLINK_HOME&gt;/bin/flink run \\ /path/to/paimon-flink-action-1.2.0.jar \\ rewrite_file_index --help `}),e.add({id:107,href:"/iceberg/",title:"Iceberg Metadata",section:"Apache Paimon",content:``}),e.add({id:108,href:"/spark/procedures/",title:"Procedures",section:"Engine Spark",content:`Procedures#This section introduce all available spark procedures about paimon.
Procedure NameExplanationExamplecompactTo compact files. Argument:table: the target table identifier. Cannot be empty.partitions: partition filter. the comma (",") represents "AND", the semicolon (";") represents "OR". If you want to compact one partition with date=01 and day=01, you need to write 'date=01,day=01'. Left empty for all partitions. (Can't be used together with "where")where: partition predicate. Left empty for all partitions. (Can't be used together with "partitions") order_strategy: 'order' or 'zorder' or 'hilbert' or 'none'. Left empty for 'none'.order_columns: the columns need to be sort. Left empty if 'order_strategy' is 'none'.options: additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.compact_strategy: this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.SET spark.sql.shuffle.partitions=10; --set the compact parallelism CALL sys.compact(table => 'T', partitions => 'p=0;p=1', order_strategy => 'zorder', order_by => 'a,b') CALL sys.compact(table => 'T', where => 'p>0 and p<3', order_strategy => 'zorder', order_by => 'a,b') CALL sys.compact(table => 'T', where => 'dt>10 and h<20', order_strategy => 'zorder', order_by => 'a,b', options => 'sink.parallelism=4') CALL sys.compact(table => 'T', partition_idle_time => '60s')CALL sys.compact(table => 'T', compact_strategy => 'minor')expire_snapshotsTo expire snapshots. Argument:table: the target table identifier. Cannot be empty.retain_max: the maximum number of completed snapshots to retain.retain_min: the minimum number of completed snapshots to retain.older_than: timestamp before which snapshots will be removed.max_deletes: the maximum number of snapshots that can be deleted at once.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.CALL sys.expire_snapshots(table => 'default.T', retain_max => 10, options => 'snapshot.expire.limit=1')expire_partitionsTo expire partitions. Argument:table: the target table identifier. Cannot be empty.expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.timestamp_formatter: the formatter to format timestamp from string.timestamp_pattern: the pattern to get a timestamp from partitions.expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.max_expires: The maximum of limited expired partitions, it is optional.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.CALL sys.expire_partitions(table => 'default.T', expiration_time => '1 d', timestamp_formatter => 'yyyy-MM-dd', timestamp_pattern => '$dt', expire_strategy => 'values-time', options => 'partition.expiration-max-num=2')create_tagTo create a tag based on given snapshot. Arguments:table: the target table identifier. Cannot be empty.tag: name of the new tag. Cannot be empty.snapshot(Long): id of the snapshot which the new tag is based on.time_retained: The maximum time retained for newly created tags.-- based on snapshot 10 with 1d CALL sys.create_tag(table => 'default.T', tag => 'my_tag', snapshot => 10, time_retained => '1 d') -- based on the latest snapshot CALL sys.create_tag(table => 'default.T', tag => 'my_tag')create_tag_from_timestampTo create a tag based on given timestamp. Arguments:table: the target table identifier. Cannot be empty.tag: name of the new tag.timestamp (Long): Find the first snapshot whose commit-time is greater than this timestamp.time_retained : The maximum time retained for newly created tags.CALL sys.create_tag_from_timestamp(\`table\` => 'default.T', \`tag\` => 'my_tag', \`timestamp\` => 1724404318750, time_retained => '1 d')rename_tagRename a tag with a new tag name. Arguments:table: the target table identifier. Cannot be empty.tag: name of the tag. Cannot be empty.target_tag: the new tag name to rename. Cannot be empty.CALL sys.rename_tag(table => 'default.T', tag => 'tag1', target_tag => 'tag2')replace_tagReplace an existing tag with new tag info. Arguments:table: the target table identifier. Cannot be empty.tag: name of the existed tag. Cannot be empty.snapshot(Long): id of the snapshot which the tag is based on, it is optional.time_retained: The maximum time retained for the existing tag, it is optional.CALL sys.replace_tag(table => 'default.T', tag_name => 'tag1', snapshot => 10, time_retained => '1 d')delete_tagTo delete a tag. Arguments:table: the target table identifier. Cannot be empty.tag: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.CALL sys.delete_tag(table => 'default.T', tag => 'my_tag')expire_tagsTo expire tags by time. Arguments:table: the target table identifier. Cannot be empty.older_than: tagCreateTime before which tags will be removed.CALL sys.expire_tags(table => 'default.T', older_than => '2024-09-06 11:00:00')rollbackTo rollback to a specific version of target table, note version/snapshot/tag must set one of them. Argument:table: the target table identifier. Cannot be empty.version: id of the snapshot or name of tag that will roll back to, version would be Deprecated.snapshot: snapshot that will roll back to.tag: tag that will roll back to.CALL sys.rollback(table => 'default.T', version => 'my_tag')CALL sys.rollback(table => 'default.T', version => 10)CALL sys.rollback(table => 'default.T', tag => 'tag1')CALL sys.rollback(table => 'default.T', snapshot => 2)rollback_to_timestampTo rollback to the snapshot which earlier or equal than timestamp. Argument:table: the target table identifier. Cannot be empty.timestamp: roll back to the snapshot which earlier or equal than timestamp.CALL sys.rollback_to_timestamp(table => 'default.T', timestamp => 1730292023000)rollback_to_watermarkTo rollback to the snapshot which earlier or equal than watermark. Argument:table: the target table identifier. Cannot be empty.watermark: roll back to the snapshot which earlier or equal than watermark.CALL sys.rollback_to_watermark(table => 'default.T', watermark => 1730292023000)purge_filesTo clear table with purge files. Argument:table: the target table identifier. Cannot be empty.CALL sys.purge_files(table => 'default.T')migrate_databaseMigrate all hive tables in database to paimon tables. Arguments:source_type: the origin database's type to be migrated, such as hive. Cannot be empty.database: name of the origin database to be migrated. Cannot be empty.options: the table options of the paimon table to migrate.options_map: Options map for adding key-value options which is a map.parallelism: the parallelism for migrate process, default is core numbers of machine.CALL sys.migrate_database(source_type => 'hive', database => 'db01', options => 'file.format=parquet', options_map => map('k1','v1'), parallelism => 6)migrate_tableMigrate hive table to a paimon table. Arguments:source_type: the origin table's type to be migrated, such as hive. Cannot be empty.table: name of the origin table to be migrated. Cannot be empty.options: the table options of the paimon table to migrate.target_table: name of the target paimon table to migrate. If not set would keep the same name with origin tabledelete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is trueoptions_map: Options map for adding key-value options which is a map.parallelism: the parallelism for migrate process, default is core numbers of machine.CALL sys.migrate_table(source_type => 'hive', table => 'default.T', options => 'file.format=parquet', options_map => map('k1','v1'), parallelism => 6)remove_orphan_filesTo remove the orphan data files and metadata files. Arguments:table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.older_than: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.dry_run: when true, view only orphan files, don't actually remove files. Default is false.parallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.mode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.CALL sys.remove_orphan_files(table => 'default.T', older_than => '2023-10-31 12:00:00')CALL sys.remove_orphan_files(table => 'default.*', older_than => '2023-10-31 12:00:00')CALL sys.remove_orphan_files(table => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => true)CALL sys.remove_orphan_files(table => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => true, parallelism => '5')CALL sys.remove_orphan_files(table => 'default.T', older_than => '2023-10-31 12:00:00', dry_run => true, parallelism => '5', mode => 'local')remove_unexisting_filesProcedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments:table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.dry_run (optional): only check what files will be removed, but not really remove them. Default is false.parallelism (optional): number of parallelisms to check files in the manifests.Note that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.-- remove unexisting data files in the table \`mydb.myt\`CALL sys.remove_unexisting_files(table => 'mydb.myt')-- only check what files will be removed, but not really remove them (dry run)CALL sys.remove_unexisting_files(table => 'mydb.myt', dry_run = true)repairSynchronize information from the file system to Metastore. Argument:database_or_table: empty or the target database name or the target table identifier, if you specify multiple tags, delimiter is ','CALL sys.repair('test_db.T')CALL sys.repair('test_db.T,test_db01,test_db.T2')create_branchTo merge a branch to main branch. Arguments:table: the target table identifier or branch identifier. Cannot be empty.branch: name of the branch to be merged.tag: name of the new tag. Cannot be empty.CALL sys.create_branch(table => 'test_db.T', branch => 'test_branch')CALL sys.create_branch(table => 'test_db.T', branch => 'test_branch', tag => 'my_tag')CALL sys.create_branch(table => 'test_db.T$branch_existBranchName', branch => 'test_branch', tag => 'my_tag')delete_branchTo merge a branch to main branch. Arguments:table: the target table identifier. Cannot be empty.branch: name of the branch to be merged. If you specify multiple branches, delimiter is ','.CALL sys.delete_branch(table => 'test_db.T', branch => 'test_branch')fast_forwardTo fast_forward a branch to main branch. Arguments:table: the target table identifier. Cannot be empty.branch: name of the branch to be merged.CALL sys.fast_forward(table => 'test_db.T', branch => 'test_branch')reset_consumerTo reset or delete consumer. Arguments:table: the target table identifier. Cannot be empty.consumerId: consumer to be reset or deleted.nextSnapshotId (Long): the new next snapshot id of the consumer.-- reset the new next snapshot id in the consumerCALL sys.reset_consumer(table => 'default.T', consumerId => 'myid', nextSnapshotId => 10)-- delete consumerCALL sys.reset_consumer(table => 'default.T', consumerId => 'myid')clear_consumersTo clear consumers. Arguments:table: the target table identifier. Cannot be empty.includingConsumers: consumers to be cleared.excludingConsumers: consumers which not to be cleared.-- clear all consumers in the tableCALL sys.clear_consumers(table => 'default.T')-- clear some consumers in the table (accept regular expression)CALL sys.clear_consumers(table => 'default.T', includingConsumers => 'myid.*')-- clear all consumers except excludingConsumers in the table (accept regular expression)CALL sys.clear_consumers(table => 'default.T', includingConsumers => '', excludingConsumers => 'myid1.*')-- clear all consumers with includingConsumers and excludingConsumers (accept regular expression)CALL sys.clear_consumers(table => 'default.T', includingConsumers => 'myid.*', excludingConsumers => 'myid1.*')mark_partition_doneTo mark partition to be done. Arguments:table: the target table identifier. Cannot be empty.partitions: partitions need to be mark done, If you specify multiple partitions, delimiter is ';'.-- mark single partition doneCALL sys.mark_partition_done(table => 'default.T', parititions => 'day=2024-07-01')-- mark multiple partitions doneCALL sys.mark_partition_done(table => 'default.T', parititions => 'day=2024-07-01;day=2024-07-02')refresh_object_tableTo refresh_object_table a object table. Arguments:table: the target table identifier. Cannot be empty.CALL sys.refresh_object_table('default.T')compact_manifestTo compact_manifest the manifests. Arguments:table: the target table identifier. Cannot be empty.options: the additional dynamic options of the table. It prioritizes higher than original \`tableProp\` and lower than \`procedureArg\`.CALL sys.compact_manifest(\`table\` => 'default.T')alter_view_dialectTo alter view dialect. Arguments:view: the target view identifier. Cannot be empty.action: define change action like: add, update, drop. Cannot be empty.engine: when engine which is not spark need define it.query: query for the dialect when action is add and update it couldn't be empty.-- add dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'add', 'spark', 'query')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'add', \`query\` => 'query')-- update dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'update', 'spark', 'query')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'update', \`query\` => 'query')-- drop dialect in the viewCALL sys.alter_view_dialect('view_identifier', 'drop', 'spark')CALL sys.alter_view_dialect(\`view\` => 'view_identifier', \`action\` => 'drop')create_functionCALL sys.create_function('function_identifier','[{"id": 0, "name":"length", "type":"INT"}, {"id": 1, "name":"width", "type":"INT"}]','[{"id": 0, "name":"area", "type":"BIGINT"}]',true, 'comment', 'k1=v1,k2=v2')To create a function. Arguments:function: the target function identifier. Cannot be empty.inputParams: inputParams of the function.returnParams: returnParams of the function.deterministic: Whether the function is deterministic.comment: The comment for the function.options: the additional dynamic options of the function.CALL sys.create_function(\`function\` => 'function_identifier',\`inputParams\` => '[{"id": 0, "name":"length", "type":"INT"}, {"id": 1, "name":"width", "type":"INT"}]',\`returnParams\` => '[{"id": 0, "name":"area", "type":"BIGINT"}]',\`deterministic\` => true,\`comment\` => 'comment',\`options\` => 'k1=v1,k2=v2')alter_functionCALL sys.alter_function('function_identifier','{"action" : "addDefinition", "name" : "spark", "definition" : {"type" : "lambda", "definition" : "(Integer length, Integer width) -> { return (long) length * width; }", "language": "JAVA" } }')To alter a function. Arguments:function: the target function identifier. Cannot be empty.change: change of the function.CALL sys.alter_function(\`function\` => 'function_identifier',\`change\` => '{"action" : "addDefinition", "name" : "spark", "definition" : {"type" : "lambda", "definition" : "(Integer length, Integer width) -> { return (long) length * width; }", "language": "JAVA" } }')drop_functionCALL [catalog.]sys.drop_function('function_identifier')To drop a function. Arguments:function: the target function identifier. Cannot be empty.CALL sys.drop_function(\`function\` => 'function_identifier')`}),e.add({id:109,href:"/project/",title:"Project",section:"Apache Paimon",content:``}),e.add({id:110,href:"/flink/savepoint/",title:"Savepoint",section:"Engine Flink",content:`Savepoint#Paimon has its own snapshot management, this may conflict with Flink&rsquo;s checkpoint management, causing exceptions when restoring from savepoint (don&rsquo;t worry, it will not cause the storage to be damaged).
It is recommended that you use the following methods to savepoint:
Use Flink Stop with savepoint. Use Paimon Tag with Flink Savepoint, and rollback-to-tag before restoring from savepoint. Stop with savepoint#This feature of Flink ensures that the last checkpoint is fully processed, which means there will be no more uncommitted metadata left. This is very safe, so we recommend using this feature to stop and start job.
Tag with Savepoint#In Flink, we may consume from Kafka and then write to Paimon. Since Flink&rsquo;s checkpoint only retains a limited number, we will trigger a savepoint at certain time (such as code upgrades, data updates, etc.) to ensure that the state can be retained for a longer time, so that the job can be restored incrementally.
Paimon&rsquo;s snapshot is similar to Flink&rsquo;s checkpoint, and both will automatically expire, but the tag feature of Paimon allows snapshots to be retained for a long time. Therefore, we can combine the two features of Paimon&rsquo;s tag and Flink&rsquo;s savepoint to achieve incremental recovery of job from the specified savepoint.
Starting from Flink 1.15 intermediate savepoints (savepoints other than created with stop-with-savepoint) are not used for recovery and do not commit any side effects.
For savepoint created with stop-with-savepoint, tags will be created automatically. For other savepoints, tags will be created after the next checkpoint succeeds.
Step 1: Enable automatically create tags for savepoint.
You can set sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for savepoint.
Step 2: Trigger savepoint.
You can refer to Flink savepoint to learn how to configure and trigger savepoint.
Step 3: Choose the tag corresponding to the savepoint.
The tag corresponding to the savepoint will be named in the form of savepoint-\${savepointID}. You can refer to Tags Table to query.
Step 4: Rollback the paimon table.
Rollback the Paimon table to the specified tag.
Step 5: Restart from the savepoint.
You can refer to here to learn how to restart from a specified savepoint.
`}),e.add({id:111,href:"/maintenance/configurations/",title:"Configurations",section:"Maintenance",content:`Configuration#CoreOptions#Core options for paimon.
KeyDefaultTypeDescriptionaggregation.remove-record-on-deletefalseBooleanWhether to remove the whole row in aggregation engine when -D records are received.alter-column-null-to-not-null.disabledtrueBooleanIf true, it disables altering column type from null to not null. Default is true. Users can disable this option to explicitly convert null column type to not null.async-file-writetrueBooleanWhether to enable asynchronous IO writing when writing files.auto-createfalseBooleanWhether to create underlying storage when reading and writing the table.bucket-1IntegerBucket number for file store.
It should either be equal to -1 (dynamic bucket mode), -2 (postpone bucket mode), or it must be greater than 0 (fixed bucket mode).bucket-key(none)StringSpecify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.
If you specify multiple fields, delimiter is ','.
If not specified, the primary key will be used; if there is no primary key, the full row will be used.cache-page-size64 kbMemorySizeMemory page size for caching.changelog-file.compression(none)StringChangelog file compression.changelog-file.format(none)StringSpecify the message format of changelog files, currently parquet, avro and orc are supported.changelog-file.prefix"changelog-"StringSpecify the file name prefix of changelog files.changelog-file.stats-mode(none)StringChangelog file metadata stats collection. none, counts, truncate(16), full is available.changelog-producernoneEnum
Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys. Possible values:"none": No changelog file."input": Double write to a changelog file when flushing memory table, the changelog is from input."full-compaction": Generate changelog files with each full compaction."lookup": Generate changelog files through 'lookup' before committing the data writing.changelog-producer.row-deduplicatefalseBooleanWhether to generate -U, +U changelog for the same record. This configuration is only valid for the changelog-producer is lookup or full-compaction.changelog-producer.row-deduplicate-ignore-fields(none)StringFields that are ignored for comparison while generating -U, +U changelog for the same record. This configuration is only valid for the changelog-producer.row-deduplicate is true.changelog.num-retained.max(none)IntegerThe maximum number of completed changelog to retain. Should be greater than or equal to the minimum number.changelog.num-retained.min(none)IntegerThe minimum number of completed changelog to retain. Should be greater than or equal to 1.changelog.time-retained(none)DurationThe maximum time of completed changelog to retain.commit.callback.#.param(none)StringParameter string for the constructor of class #. Callback class should parse the parameter by itself.commit.callbacks(none)StringA list of commit callback classes to be called after a successful commit. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).commit.force-compactfalseBooleanWhether to force a compaction before commit.commit.force-create-snapshotfalseBooleanIn streaming job, whether to force creating snapshot when there is no data in this write-commit phase.commit.max-retries10IntegerMaximum number of retries when commit failed.commit.strict-mode.last-safe-snapshot(none)LongIf set, committer will check if there are other commit user's COMPACT / OVERWRITE snapshot, starting from the snapshot after this one. If found, commit will be aborted. If the value of this option is -1, committer will not check for its first commit.commit.timeout(none)DurationTimeout duration of retry when commit failed.commit.user-prefix(none)StringSpecifies the commit user prefix.compaction.delete-ratio-threshold0.2DoubleRatio of the deleted rows in a data file to be forced compacted for append-only table.compaction.force-rewrite-all-filesfalseBooleanWhether to force pick all files for a full compaction. Usually seen in a compaction task to external paths.compaction.force-up-level-0falseBooleanIf set to true, compaction strategy will always include all level 0 files in candidates.compaction.max-size-amplification-percent200IntegerThe size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.compaction.min.file-num5IntegerFor file set [f_0,...,f_N], the minimum file number to trigger a compaction for append-only table.compaction.optimization-interval(none)DurationImplying how often to perform an optimization compaction, this configuration is used to ensure the query timeliness of the read-optimized system table.compaction.size-ratio1IntegerPercentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.consumer-id(none)StringConsumer id for recording the offset of consumption in the storage.consumer.expiration-time(none)DurationThe expiration interval of consumer files. A consumer file will be expired if it's lifetime after last modification is over this value.consumer.ignore-progressfalseBooleanWhether to ignore consumer progress for the newly started job.consumer.modeexactly-onceEnum
Specify the consumer consistency mode for table.
Possible values:"exactly-once": Readers consume data at snapshot granularity, and strictly ensure that the snapshot-id recorded in the consumer is the snapshot-id + 1 that all readers have exactly consumed."at-least-once": Each reader consumes snapshots at a different rate, and the snapshot with the slowest consumption progress among all readers will be recorded in the consumer.continuous.discovery-interval10 sDurationThe discovery interval of continuous reading.cross-partition-upsert.bootstrap-parallelism10IntegerThe parallelism for bootstrap in a single task for cross partition upsert.cross-partition-upsert.index-ttl(none)DurationThe TTL in rocksdb index for cross partition upsert (primary keys not contain all partition fields), this can avoid maintaining too many indexes and lead to worse and worse performance, but please note that this may also cause data duplication.data-file.external-paths(none)StringThe external paths where the data of this table will be written, multiple elements separated by commas.data-file.external-paths.specific-fs(none)StringThe specific file system of the external path when data-file.external-paths.strategy is set to specific-fs, should be the prefix scheme of the external path, now supported are s3 and oss.data-file.external-paths.strategynoneEnum
The strategy of selecting an external path when writing data.
Possible values:"none": Do not choose any external storage, data will still be written to the default warehouse path."specific-fs": Select a specific file system as the external path. Currently supported are S3 and OSS."round-robin": When writing a new file, a path is chosen from data-file.external-paths in turn.data-file.path-directory(none)StringSpecify the path directory of data files.data-file.prefix"data-"StringSpecify the file name prefix of data files.data-file.thin-modefalseBooleanEnable data file thin mode to avoid duplicate columns storage.delete-file.thread-num(none)IntegerThe maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.delete.force-produce-changelogfalseBooleanForce produce changelog in delete sql, or you can use 'streaming-read-overwrite' to read changelog from overwrite commit.deletion-vector.index-file.target-size2 mbMemorySizeThe target size of deletion vector index file.deletion-vectors.bitmap64falseBooleanEnable 64 bit bitmap implementation. Note that only 64 bit bitmap implementation is compatible with Iceberg.deletion-vectors.enabledfalseBooleanWhether to enable deletion vectors mode. In this mode, index files containing deletion vectors are generated when data is written, which marks the data for deletion. During read operations, by applying these index files, merging can be avoided.disable-explicit-type-castingfalseBooleanIf true, it disables explicit type casting. For ex: it disables converting LONG type to INT type. Users can enable this option to disable explicit type castingdynamic-bucket.assigner-parallelism(none)IntegerParallelism of assigner operator for dynamic bucket mode, it is related to the number of initialized bucket, too small will lead to insufficient processing speed of assigner.dynamic-bucket.initial-buckets(none)IntegerInitial buckets for a partition in assigner operator for dynamic bucket mode.dynamic-bucket.max-buckets-1IntegerMax buckets for a partition in dynamic bucket mode, It should either be equal to -1 (unlimited), or it must be greater than 0 (fixed upper bound).dynamic-bucket.target-row-num2000000LongIf the bucket is -1, for primary key table, is dynamic bucket mode, this option controls the target row number for one bucket.dynamic-partition-overwritetrueBooleanWhether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.end-input.check-partition-expirefalseBooleanOptional endInput check partition expire used in case of batch mode or bounded stream.fields.default-aggregate-function(none)StringDefault aggregate function of all fields for partial-update and aggregate merge function.file-index.in-manifest-threshold500 bytesMemorySizeThe threshold to store file index bytes in manifest.file-index.read.enabledtrueBooleanWhether enabled read file index.file-reader-async-threshold10 mbMemorySizeThe threshold for read file async.file.block-size(none)MemorySizeFile block size of format, default value of orc stripe is 64 MB, and parquet row group is 128 MB.file.compression"zstd"StringDefault file compression. For faster read and write, it is recommended to use zstd.file.compression.per.levelMapDefine different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zstd'.file.compression.zstd-level1IntegerDefault file compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.file.format"parquet"StringSpecify the message format of data files, currently orc, parquet and avro are supported.file.format.per.levelMapDefine different file format for different level, you can add the conf like this: 'file.format.per.level' = '0:avro,3:parquet', if the file format for level is not provided, the default format which set by \`file.format\` will be used.file.suffix.include.compressionfalseBooleanWhether to add file compression type in the file name of data file and changelog file.force-lookupfalseBooleanWhether to force the use of lookup for compaction.full-compaction.delta-commits(none)IntegerFull compaction will be constantly triggered after delta commits.ignore-deletefalseBooleanWhether to ignore delete records.incremental-between(none)StringRead incremental changes between start snapshot (exclusive) and end snapshot (inclusive), for example, '5,10' means changes between snapshot 5 and snapshot 10.incremental-between-scan-modeautoEnum
Scan kind when Read incremental changes between start snapshot (exclusive) and end snapshot (inclusive). Possible values:"auto": Scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files."delta": Scan newly changed files between snapshots."changelog": Scan changelog files between snapshots."diff": Get diff by comparing data of end snapshot with data of start snapshot.incremental-between-timestamp(none)StringRead incremental changes between start timestamp (exclusive) and end timestamp (inclusive), for example, 't1,t2' means changes between timestamp t1 and timestamp t2.incremental-to-auto-tag(none)StringUsed to specify the end tag (inclusive), and Paimon will find an earlier tag and return changes between them. If the tag doesn't exist or the earlier tag doesn't exist, return empty. This option requires 'tag.creation-period' and 'tag.period-formatter' configured.local-merge-buffer-size(none)MemorySizeLocal merge will buffer and merge input records before they're shuffled by bucket and written into sink. The buffer will be flushed when it is full.Mainly to resolve data skew on primary keys. We recommend starting with 64 mb when trying out this feature.local-sort.max-num-file-handles128IntegerThe maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.lookup-compactRADICALEnum
Lookup compact mode used for lookup compaction.
Possible values:"RADICAL""GENTLE"lookup-compact.max-interval(none)IntegerThe max interval for a gentle mode lookup compaction to be triggered. For every interval, a forced lookup compaction will be performed to flush L0 files to higher level. This option is only valid when lookup-compact mode is gentle.lookup-waittrueBooleanWhen need to lookup, commit will wait for compaction by lookup.lookup.cache-file-retention1 hDurationThe cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.lookup.cache-max-disk-sizeinfiniteMemorySizeMax disk size for lookup cache, you can use this option to limit the use of local disks.lookup.cache-max-memory-size256 mbMemorySizeMax memory size for lookup cache.lookup.cache-spill-compression"zstd"StringSpill compression for lookup cache, currently zstd, none, lz4 and lzo are supported.lookup.cache.bloom.filter.enabledtrueBooleanWhether to enable the bloom filter for lookup cache.lookup.cache.bloom.filter.fpp0.05DoubleDefine the default false positive probability for lookup cache bloom filters.lookup.cache.high-priority-pool-ratio0.25DoubleThe fraction of cache memory that is reserved for high-priority data like index, filter.lookup.hash-load-factor0.75FloatThe index load factor for lookup.lookup.local-file-typesortEnum
The local file type for lookup.
Possible values:"sort": Construct a sorted file for lookup."hash": Construct a hash file for lookup.manifest.compression"zstd"StringDefault file compression for manifest.manifest.delete-file-drop-statsfalseBooleanFor DELETE manifest entry in manifest file, drop stats to reduce memory and storage. Default value is false only for compatibility of old reader.manifest.format"avro"StringSpecify the message format of manifest files.manifest.full-compaction-threshold-size16 mbMemorySizeThe size threshold for triggering full compaction of manifest.manifest.merge-min-count30IntegerTo avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.manifest.target-file-size8 mbMemorySizeSuggested file size of a manifest file.merge-enginededuplicateEnum
Specify the merge engine for table with primary key.
Possible values:"deduplicate": De-duplicate and keep the last row."partial-update": Partial update non-null fields."aggregation": Aggregate fields with same primary key."first-row": De-duplicate and keep the first row.metadata.stats-dense-storetrueBooleanWhether to store statistic densely in metadata (manifest files), which will significantly reduce the storage size of metadata when the none statistic mode is set.
Note, when this mode is enabled with 'metadata.stats-mode:none', the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.metadata.stats-mode"truncate(16)"StringThe mode of metadata stats collection. none, counts, truncate(16), full is available.
"none": means disable the metadata stats collection."counts" means only collect the null count."full": means collect the null count, min/max value."truncate(16)": means collect the null count, min/max value with truncated length of 16.Field level stats mode can be specified by fields.{field_name}.stats-modemetadata.stats-mode.per.levelMapDefine different 'metadata.stats-mode' for different level, you can add the conf like this: 'metadata.stats-mode.per.level' = '0:none', if the metadata.stats-mode for level is not provided, the default mode which set by \`metadata.stats-mode\` will be used.metastore.partitioned-tablefalseBooleanWhether to create this table as a partitioned table in metastore.For example, if you want to list all partitions of a Paimon table in Hive, you need to create this table as a partitioned table in Hive metastore.This config option does not affect the default filesystem metastore.metastore.tag-to-partition(none)StringWhether to create this table as a partitioned table for mapping non-partitioned table tags in metastore. This allows the Hive engine to view this table in a partitioned table view and use partitioning field to read specific partitions (specific tags).metastore.tag-to-partition.previewnoneEnum
Whether to preview tag of generated snapshots in metastore. This allows the Hive engine to query specific tag before creation.
Possible values:"none": No automatically created tags."process-time": Based on the time of the machine, create TAG once the processing time passes period time plus delay."watermark": Based on the watermark of the input, create TAG once the watermark passes period time plus delay."batch": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.num-levels(none)IntegerTotal level number, for example, there are 3 levels, including 0,1,2 levels.num-sorted-run.compaction-trigger5IntegerThe sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).num-sorted-run.stop-trigger(none)IntegerThe number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.object-location(none)StringThe object location for object table.page-size64 kbMemorySizeMemory page size.parquet.enable.dictionary(none)IntegerTurn off the dictionary encoding for all fields in parquet.partial-update.remove-record-on-deletefalseBooleanWhether to remove the whole row in partial-update engine when -D records are received.partial-update.remove-record-on-sequence-group(none)StringWhen -D records of the given sequence groups are received, remove the whole row.partition(none)StringDefine partition by table options, cannot define partition on DDL and table options at the same time.partition.default-name"__DEFAULT_PARTITION__"StringThe default partition name in case the dynamic partition column value is null/empty string.partition.end-input-to-donefalseBooleanWhether mark the done status to indicate that the data is ready when end input.partition.expiration-check-interval1 hDurationThe check interval of partition expiration.partition.expiration-max-num100IntegerThe default deleted num of partition expiration.partition.expiration-strategyvalues-timeEnum
The strategy determines how to extract the partition time and compare it with the current time.
Possible values:"values-time": This strategy compares the time extracted from the partition value with the current time."update-time": This strategy compares the last update time of the partition with the current time."custom": This strategy use custom class to expire partitions.partition.expiration-time(none)DurationThe expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.partition.idle-time-to-report-statistic0 msDurationSet a time duration when a partition has no new data after this time duration, start to report the partition statistics to hms.partition.legacy-nametrueBooleanThe legacy partition name is using \`toString\` fpr all types. If false, using cast to string for all types.partition.mark-done-action"success-file"StringAction to mark a partition done is to notify the downstream application that the partition has finished writing, the partition is ready to be read.
1. 'success-file': add '_success' file to directory.
2. 'done-partition': add 'xxx.done' partition to metastore.
3. 'mark-event': mark partition event to metastore.
4. 'http-report': report partition mark done to remote http server.
5. 'custom': use policy class to create a mark-partition policy.
Both can be configured at the same time: 'done-partition,success-file,mark-event,custom'.partition.mark-done-action.custom.class(none)StringThe partition mark done class for implement PartitionMarkDoneAction interface. Only work in custom mark-done-action.partition.mark-done-action.http.params(none)StringHttp client request parameters will be written to the request body, this can only be used by http-report partition mark done action.partition.mark-done-action.http.url(none)StringMark done action will reports the partition to the remote http server, this can only be used by http-report partition mark done action.partition.sink-strategyNONEEnum
This is only for partitioned append table or postpone pk table, and the purpose is to reduce small files and improve write performance. Through this repartitioning strategy to reduce the number of partitions written by each task to as few as possible.none: Rebalanced or Forward partitioning, this is the default behavior, this strategy is suitable for the number of partitions you write in a batch is much smaller than write parallelism.hash: Hash the partitions value, this strategy is suitable for the number of partitions you write in a batch is greater equals than write parallelism.
Possible values:"NONE""HASH"partition.timestamp-formatter(none)StringThe formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.partition.timestamp-pattern(none)StringYou can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.primary-key(none)StringDefine primary key by table options, cannot define primary key on DDL and table options at the same time.query-auth.enabledfalseBooleanEnable query auth to give Catalog the opportunity to perform column level and row level permission validation on queries.read.batch-size1024IntegerRead batch size for any file format if it supports.record-level.expire-time(none)DurationRecord level expire time for primary key table, expiration happens in compaction, there is no strong guarantee to expire records in time. You must specific 'record-level.time-field' too.record-level.time-field(none)StringTime field for record level expire. It supports the following types: \`timestamps in seconds with INT\`,\`timestamps in seconds with BIGINT\`, \`timestamps in milliseconds with BIGINT\` or \`timestamp\`.rowkind.field(none)StringThe field that generates the row kind for primary key table, the row kind determines which data is '+I', '-U', '+U' or '-D'.scan.bounded.watermark(none)LongEnd condition "watermark" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.scan.creation-time-millis(none)LongOptional timestamp used in case of "from-creation-timestamp" scan mode.scan.fallback-branch(none)StringWhen a batch job queries from a table, if a partition does not exist in the current branch, the reader will try to get this partition from this fallback branch.scan.file-creation-time-millis(none)LongAfter configuring this time, only the data files created after this time will be read. It is independent of snapshots, but it is imprecise filtering (depending on whether or not compaction occurs).scan.manifest.parallelism(none)IntegerThe parallelism of scanning manifest files, default value is the size of cpu processor. Note: Scale-up this parameter will increase memory usage while scanning manifest files. We can consider downsize it when we encounter an out of memory exception while scanningscan.max-splits-per-task10IntegerMax split size should be cached for one task while scanning. If splits size cached in enumerator are greater than tasks size multiply by this value, scanner will pause scanning.scan.modedefaultEnum
Specify the scanning behavior of the source.
Possible values:"default": Determines actual startup mode according to other table properties. If "scan.timestamp-millis" is set the actual startup mode will be "from-timestamp", and if "scan.snapshot-id" or "scan.tag-name" is set the actual startup mode will be "from-snapshot". Otherwise the actual startup mode will be "latest-full"."latest-full": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes."full": Deprecated. Same as "latest-full"."latest": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the "latest-full" startup mode."compacted-full": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled."from-timestamp": For streaming sources, continuously reads changes starting from timestamp specified by "scan.timestamp-millis", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by "scan.timestamp-millis" but does not read new changes."from-creation-timestamp": For streaming sources and batch sources, If timestamp specified by "scan.creation-time-millis" is during in the range of earliest snapshot and latest snapshot: mode is from-snapshot which snapshot is equal or later the timestamp. If timestamp is earlier than earliest snapshot or later than latest snapshot, mode is from-file-creation-time."from-file-creation-time": For streaming and batch sources, consumes a snapshot and filters the data files by creation time. For streaming sources, upon first startup, and continue to read the latest changes."from-snapshot": For streaming sources, continuously reads changes starting from snapshot specified by "scan.snapshot-id", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by "scan.snapshot-id" or "scan.tag-name" but does not read new changes."from-snapshot-full": For streaming sources, produces from snapshot specified by "scan.snapshot-id" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by "scan.snapshot-id" but does not read new changes."incremental": Read incremental changes between start and end snapshot or timestamp.scan.plan-sort-partitionfalseBooleanWhether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.
It is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.scan.snapshot-id(none)LongOptional snapshot id used in case of "from-snapshot" or "from-snapshot-full" scan modescan.tag-name(none)StringOptional tag name used in case of "from-snapshot" scan mode.scan.timestamp(none)StringOptional timestamp used in case of "from-timestamp" scan mode, it will be automatically converted to timestamp in unix milliseconds, use local time zonescan.timestamp-millis(none)LongOptional timestamp used in case of "from-timestamp" scan mode. If there is no snapshot earlier than this time, the earliest snapshot will be chosen.scan.watermark(none)LongOptional watermark used in case of "from-snapshot" scan mode. If there is no snapshot later than this watermark, will throw an exceptions.sequence.field(none)StringThe field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.sequence.field.sort-orderascendingEnum
Specify the order of sequence.field.
Possible values:"ascending": specifies sequence.field sort order is ascending."descending": specifies sequence.field sort order is descending.sink.watermark-time-zone"UTC"StringThe time zone to parse the long watermark value to TIMESTAMP value. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the value should be the user configured local time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.snapshot.clean-empty-directoriesfalseBooleanWhether to try to clean empty directories when expiring snapshots, if enabled, please note:hdfs: may print exceptions in NameNode.oss/s3: may cause performance issue.snapshot.expire.execution-modesyncEnum
Specifies the execution mode of expire.
Possible values:"sync": Execute expire synchronously. If there are too many files, it may take a long time and block stream processing."async": Execute expire asynchronously. If the generation of snapshots is greater than the deletion, there will be a backlog of files.snapshot.expire.limit50IntegerThe maximum number of snapshots allowed to expire at a time.snapshot.num-retained.maxinfiniteIntegerThe maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.snapshot.num-retained.min10IntegerThe minimum number of completed snapshots to retain. Should be greater than or equal to 1.snapshot.time-retained1 hDurationThe maximum time of completed snapshots to retain.snapshot.watermark-idle-timeout(none)DurationIn watermarking, if a source remains idle beyond the specified timeout duration, it triggers snapshot advancement and facilitates tag creation.sort-compaction.local-sample.magnification1000IntegerThe magnification of local sample for sort-compaction.The size of local sample is sink parallelism * magnification.sort-compaction.range-strategyQUANTITYEnum
The range strategy of sort compaction, the default value is quantity.If the data size allocated for the sorting task is uneven,which may lead to performance bottlenecks, the config can be set to size.
Possible values:"SIZE""QUANTITY"sort-engineloser-treeEnum
Specify the sort engine for table with primary key.
Possible values:"min-heap": Use min-heap for multiway sorting."loser-tree": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.sort-spill-buffer-size64 mbMemorySizeAmount of data to spill records to disk in spilled sort.sort-spill-threshold(none)IntegerIf the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.source.split.open-file-cost4 mbMemorySizeOpen file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.source.split.target-size128 mbMemorySizeTarget size of a source split when scanning a bucket.spill-compression"zstd"StringCompression for spill, currently zstd, lzo and zstd are supported.spill-compression.zstd-level1IntegerDefault spill compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.streaming-read-append-overwritefalseBooleanWhether to read the delta from append table's overwrite commit in streaming mode.streaming-read-mode(none)Enum
The mode of streaming read that specifies to read the data of table file or log.
Possible values:"log": Read from the data of table log store."file": Read from the data of table file store.streaming-read-overwritefalseBooleanWhether to read the changes from overwrite in streaming mode. Cannot be set to true when changelog producer is full-compaction or lookup because it will read duplicated changes.streaming.read.snapshot.delay(none)DurationThe delay duration of stream read when scan incremental snapshots.tag.automatic-completionfalseBooleanWhether to automatically complete missing tags.tag.automatic-creationnoneEnum
Whether to create tag automatically. And how to generate tags.
Possible values:"none": No automatically created tags."process-time": Based on the time of the machine, create TAG once the processing time passes period time plus delay."watermark": Based on the watermark of the input, create TAG once the watermark passes period time plus delay."batch": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.tag.batch.customized-name(none)StringUse customized name when creating tags in Batch mode.tag.callback.#.param(none)StringParameter string for the constructor of class #. Callback class should parse the parameter by itself.tag.callbacks(none)StringA list of commit callback classes to be called after a successful tag. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).tag.create-success-filefalseBooleanWhether to create tag success file for new created tags.tag.creation-delay0 msDurationHow long is the delay after the period ends before creating a tag. This can allow some late data to enter the Tag.tag.creation-perioddailyEnum
What frequency is used to generate tags.
Possible values:"daily": Generate a tag every day."hourly": Generate a tag every hour."two-hours": Generate a tag every two hours.tag.creation-period-duration(none)DurationThe period duration for tag auto create periods.If user set it, tag.creation-period would be invalid.tag.default-time-retained(none)DurationThe default maximum time retained for newly created tags. It affects both auto-created tags and manually created (by procedure) tags.tag.num-retained-max(none)IntegerThe maximum number of tags to retain. It only affects auto-created tags.tag.period-formatterwith_dashesEnum
The date format for tag periods.
Possible values:"with_dashes": Dates and hours with dashes, e.g., 'yyyy-MM-dd HH'"without_dashes": Dates and hours without dashes, e.g., 'yyyyMMdd HH'"without_dashes_and_spaces": Dates and hours without dashes and spaces, e.g., 'yyyyMMddHH'target-file-size(none)MemorySizeTarget size of a file.primary key table: the default value is 128 MB.append table: the default value is 256 MB.typetableEnum
Type of the table.
Possible values:"table": Normal Paimon table."format-table": A file format table refers to a directory that contains multiple files of the same format."materialized-table": A materialized table combines normal Paimon table and materialized SQL."object-table": An object table combines normal Paimon table and object location.write-buffer-for-appendfalseBooleanThis option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.write-buffer-size256 mbMemorySizeAmount of data to build up in memory before converting to a sorted on-disk file.write-buffer-spill.max-disk-sizeinfiniteMemorySizeThe max disk to use for write buffer spill. This only work when the write buffer spill is enabledwrite-buffer-spillable(none)BooleanWhether the write buffer can be spillable. Enabled by default when using object storage or when 'target-file-size' is greater than 'write-buffer-size'.write-manifest-cache0 bytesMemorySizeCache size for reading manifest files for write initialization.write-max-writers-to-spill10IntegerWhen in batch append inserting, if the writer number is greater than this option, we open the buffer cache and spill function to avoid out-of-memory. write-onlyfalseBooleanIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.write.batch-size1024IntegerWrite batch size for any file format if it supports.zorder.var-length-contribution8IntegerThe bytes of types (CHAR, VARCHAR, BINARY, VARBINARY) devote to the zorder sort.CatalogOptions#Options for paimon catalog.
KeyDefaultTypeDescriptioncache-enabledtrueBooleanControls whether the catalog will cache databases, tables, manifests and partitions.cache.expire-after-access10 minDurationCache expiration policy: marks cache entries to expire after a specified duration has passed since their last access.cache.expire-after-write30 minDurationCache expiration policy: marks cache entries to expire after a specified duration has passed since their last refresh.cache.manifest.max-memory(none)MemorySizeControls the maximum memory to cache manifest content.cache.manifest.small-file-memory128 mbMemorySizeControls the cache memory to cache small manifest files.cache.manifest.small-file-threshold1 mbMemorySizeControls the threshold of small manifest file.cache.partition.max-num0LongControls the max number for which partitions in the catalog are cached.cache.snapshot.max-num-per-table20IntegerControls the max number for snapshots per table in the catalog are cached.case-sensitive(none)BooleanIndicates whether this catalog is case-sensitive.client-pool-size2IntegerConfigure the size of the connection pool.file-io.allow-cachetrueBooleanWhether to allow static cache in file io implementation. If not allowed, this means that there may be a large number of FileIO instances generated, enabling caching can lead to resource leakage.file-io.populate-metafalseBooleanWhether to populate file metadata while listing or getting file status.format-table.enabledtrueBooleanWhether to support format tables, format table corresponds to a regular csv, parquet or orc table, allowing read and write operations. However, during these processes, it does not connect to the metastore; hence, newly added partitions will not be reflected in the metastore and need to be manually added as separate partition operations.lock-acquire-timeout8 minDurationThe maximum time to wait for acquiring the lock.lock-check-max-sleep8 sDurationThe maximum sleep time when retrying to check the lock.lock.enabled(none)BooleanEnable Catalog Lock.lock.type(none)StringThe Lock Type for Catalog, such as 'hive', 'zookeeper'.metastore"filesystem"StringMetastore of paimon catalog, supports filesystem, hive and jdbc.resolving-file-io.enabledfalseBooleanWhether to enable resolving fileio, when this option is enabled, in conjunction with the table's property data-file.external-paths, Paimon can read and write to external storage paths, such as OSS or S3. In order to access these external paths correctly, you also need to configure the corresponding access key and secret key.sync-all-propertiestrueBooleanSync all table properties to hive metastoretable.typemanagedEnum
Type of table.
Possible values:"managed": Paimon owned table where the entire lifecycle of the table data is managed."external": The table where Paimon has loose coupling with the data stored in external locations.uri(none)StringUri of metastore server.warehouse(none)StringThe warehouse root path of catalog.HiveCatalogOptions#Options for Hive catalog.
KeyDefaultTypeDescriptionclient-pool-cache.eviction-interval-ms300000LongSetting the client's pool cache eviction interval(ms).client-pool-cache.keys(none)StringSpecify client cache key, multiple elements separated by commas.
"ugi": the Hadoop UserGroupInformation instance that represents the current user using the cache."user_name" similar to UGI but only includes the user's name determined by UserGroupInformation#getUserName."conf": name of an arbitrary configuration. The value of the configuration will be extracted from catalog properties and added to the cache key. A conf element should start with a "conf:" prefix which is followed by the configuration name. E.g. specifying "conf:a.b.c" will add "a.b.c" to the key, and so that configurations with different default catalog wouldn't share the same client pool. Multiple conf elements can be specified.hadoop-conf-dir(none)StringFile directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported.If not configured, try to load from 'HADOOP_CONF_DIR' or 'HADOOP_HOME' system environment.Configure Priority: 1.from 'hadoop-conf-dir' 2.from HADOOP_CONF_DIR 3.from HADOOP_HOME/conf 4.HADOOP_HOME/etc/hadoop.hive-conf-dir(none)StringFile directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on.If not configured, try to load from 'HIVE_CONF_DIR' env.location-in-propertiesfalseBooleanSetting the location in properties of hive table/database.If you don't want to access the location by the filesystem of hive when using a object storage such as s3,ossyou can set this option to true.metastore.client.class"org.apache.hadoop.hive.metastore.HiveMetaStoreClient"StringClass name of Hive metastore client.NOTE: This class must directly implements org.apache.hadoop.hive.metastore.IMetaStoreClient.JdbcCatalogOptions#Options for Jdbc catalog.
KeyDefaultTypeDescriptioncatalog-key"jdbc"StringCustom jdbc catalog store key.lock-key-max-length255IntegerSet the maximum length of the lock key. The 'lock-key' is composed of concatenating three fields : 'catalog-key', 'database', and 'table'.FlinkCatalogOptions#Flink catalog options for paimon.
KeyDefaultTypeDescriptiondefault-database"default"Stringdisable-create-table-in-default-dbfalseBooleanIf true, creating table in default database is not allowed. Default is false.FlinkConnectorOptions#Flink connector options for paimon.
KeyDefaultTypeDescriptionchangelog.precommit-compact.thread-num(none)IntegerMaximum number of threads to copy bytes from small changelog files. By default is the number of processors available to the Java virtual machine.end-input.watermark(none)LongOptional endInput watermark used in case of batch mode or bounded stream.lookup.asyncfalseBooleanWhether to enable async lookup join.lookup.async-thread-number16IntegerThe thread number for lookup async.lookup.bootstrap-parallelism4IntegerThe parallelism for bootstrap in a single task for lookup join.lookup.cacheAUTOEnum
The cache mode of lookup join.
Possible values:"AUTO""FULL""MEMORY"lookup.dynamic-partition.refresh-interval1 hDurationSpecific dynamic partition refresh interval for lookup, scan all partitions and obtain corresponding partition.lookup.refresh.asyncfalseBooleanWhether to refresh lookup table in an async thread.lookup.refresh.async.pending-snapshot-count5IntegerIf the pending snapshot count exceeds the threshold, lookup operator will refresh the table in sync.lookup.refresh.time-periods-blacklist(none)StringThe blacklist contains several time periods. During these time periods, the lookup table's cache refreshing is forbidden. Blacklist format is start1-&gt;end1,start2-&gt;end2,... , and the time format is yyyy-MM-dd HH:mm. Only used when lookup table is FULL cache mode.partition.idle-time-to-done(none)DurationSet a time duration when a partition has no new data after this time duration, mark the done status to indicate that the data is ready.partition.mark-done-action.modeprocess-timeEnum
How to trigger partition mark done action.
Possible values:"process-time": Based on the time of the machine, mark the partition done once the processing time passes period time plus delay."watermark": Based on the watermark of the input, mark the partition done once the watermark passes period time plus delay.partition.mark-done.recover-from-statetrueBooleanWhether trigger partition mark done when recover from state.partition.time-interval(none)DurationYou can specify time interval for partition, for example, daily partition is '1 d', hourly partition is '1 h'.postpone.default-bucket-num1IntegerBucket number for the partitions compacted for the first time in postpone bucket tables.precommit-compactfalseBooleanIf true, it will add a compact coordinator and worker operator after the writer operator,in order to compact several changelog files (for primary key tables) or newly created data files (for unaware bucket tables) from the same partition into large ones, which can decrease the number of small files.read.shuffle-bucket-with-partitiontrueBooleanWhether shuffle by partition and bucket when read.scan.bounded(none)BooleanBounded mode for Paimon consumer. By default, Paimon automatically selects bounded mode based on the mode of the Flink job.scan.dedicated-split-generationfalseBooleanIf true, the split generation process would be performed during runtime on a Flink task, instead of on the JobManager during initialization phase.scan.infer-parallelismtrueBooleanIf it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).scan.infer-parallelism.max1024IntegerIf scan.infer-parallelism is true, limit the parallelism of source through this option.scan.max-snapshot.count-1IntegerThe max snapshot count to scan per checkpoint. Not limited when it's negative.scan.parallelism(none)IntegerDefine a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.scan.partitions(none)StringSpecify the partitions to scan. Partitions should be given in the form of key1=value1,key2=value2. Partition keys not specified will be filled with the value of partition.default-name. Multiple partitions should be separated by semicolon (;). This option can support normal source tables and lookup join tables. For lookup joins, two special values max_pt() and max_two_pt() are also supported, specifying the (two) partition(s) with the largest partition value.scan.remove-normalizefalseBooleanWhether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.scan.split-enumerator.batch-size10IntegerHow many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed \`akka.framesize\` limit.scan.split-enumerator.modefairEnum
The mode used by StaticFileStoreSplitEnumerator to assign splits.
Possible values:"fair": Distribute splits evenly when batch reading to prevent a few tasks from reading all."preemptive": Distribute splits preemptively according to the consumption speed of the task.scan.watermark.alignment.group(none)StringA group of sources to align watermarks.scan.watermark.alignment.max-drift(none)DurationMaximal drift to align watermarks, before we pause consuming from the source/task/partition.scan.watermark.alignment.update-interval1 sDurationHow often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.scan.watermark.emit.strategyon-eventEnum
Emit strategy for watermark generation.
Possible values:"on-periodic": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'."on-event": Emit watermark per record.scan.watermark.idle-timeout(none)DurationIf no records flow in a partition of a stream for that amount of time, then that partition is considered "idle" and will not hold back the progress of watermarks in downstream operators.sink.clustering.by-columns(none)StringSpecifies the column name(s) used for comparison during range partitioning, in the format 'columnName1,columnName2'. If not set or set to an empty string, it indicates that the range partitioning feature is not enabled. This option will be effective only for bucket unaware table without primary keys and batch execution mode.sink.clustering.sample-factor100IntegerSpecifies the sample factor. Let S represent the total number of samples, F represent the sample factor, and P represent the sink parallelism, then S=F×P. The minimum allowed sample factor is 20.sink.clustering.sort-in-clustertrueBooleanIndicates whether to further sort data belonged to each sink task after range partitioning.sink.clustering.strategy"auto"StringSpecifies the comparison algorithm used for range partitioning, including 'zorder', 'hilbert', and 'order', corresponding to the z-order curve algorithm, hilbert curve algorithm, and basic type comparison algorithm, respectively. When not configured, it will automatically determine the algorithm based on the number of columns in 'sink.clustering.by-columns'. 'order' is used for 1 column, 'zorder' for less than 5 columns, and 'hilbert' for 5 or more columns.sink.committer-cpu1.0DoubleSink committer cpu to control cpu cores of global committer.sink.committer-memory(none)MemorySizeSink committer memory to control heap memory of global committer.sink.committer-operator-chainingtrueBooleanAllow sink committer and writer operator to be chained togethersink.cross-partition.managed-memory256 mbMemorySizeWeight of managed memory for RocksDB in cross-partition update, Flink will compute the memory size according to the weight, the actual memory used depends on the running environment.sink.managed.writer-buffer-memory256 mbMemorySizeWeight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.sink.operator-uid.suffix(none)StringSet the uid suffix for the writer, dynamic bucket assigner and committer operators. The uid format is \${UID_PREFIX}_\${TABLE_NAME}_\${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.sink.parallelism(none)IntegerDefines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.sink.savepoint.auto-tagfalseBooleanIf true, a tag will be automatically created for the snapshot created by flink savepoint.sink.use-managed-memory-allocatorfalseBooleanIf true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.sink.writer-cpu1.0DoubleSink writer cpu to control cpu cores of writer.sink.writer-memory(none)MemorySizeSink writer memory to control heap memory of writer.source.checkpoint-align.enabledfalseBooleanWhether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.source.checkpoint-align.timeout30 sDurationIf the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.source.operator-uid.suffix(none)StringSet the uid suffix for the source operators. After setting, the uid format is \${UID_PREFIX}_\${TABLE_NAME}_\${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.unaware-bucket.compaction.parallelism(none)IntegerDefines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.SparkCatalogOptions#Spark catalog options for paimon.
KeyDefaultTypeDescriptioncatalog.create-underlying-session-catalogfalseBooleanIf true, create and use an underlying session catalog instead of default session catalog when use SparkGenericCatalog.defaultDatabase"default"StringThe default database name.SparkConnectorOptions#Spark connector options for paimon.
KeyDefaultTypeDescriptionread.changelogfalseBooleanWhether to read row in the form of changelog (add rowkind column in row to represent its change type).read.stream.maxBytesPerTrigger(none)LongThe maximum number of bytes returned in a single batch.read.stream.maxFilesPerTrigger(none)IntegerThe maximum number of files returned in a single batch.read.stream.maxRowsPerTrigger(none)LongThe maximum number of rows returned in a single batch.read.stream.maxTriggerDelayMs(none)LongThe maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.read.stream.minRowsPerTrigger(none)LongThe minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.requiredSparkConfsCheck.enabledtrueBooleanWhether to verify SparkSession is initialized with required configurations.write.merge-schemafalseBooleanIf true, merge the data schema and the table schema automatically before write data.write.merge-schema.explicit-castfalseBooleanIf true, allow to merge data types if the two types meet the rules for explicit casting.write.use-v2-writefalseBooleanIf true, v2 write will be used. Currently, only HASH_FIXED and BUCKET_UNAWARE bucket modes are supported. Will fall back to v1 write for other bucket modes. Currently, Spark V2 write does not support TableCapability.STREAMING_WRITE and TableCapability.ACCEPT_ANY_SCHEMA.ORC Options#KeyDefaultTypeDescriptionorc.column.encoding.direct(none)IntegerComma-separated list of fields for which dictionary encoding is to be skipped in orc.orc.dictionary.key.threshold0.8DoubleIf the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 0 to always disable dictionary encoding. Use 1 to always use dictionary encoding.orc.timestamp-ltz.legacy.typetrueBooleanThis option is used to be compatible with the paimon-orc‘s old behavior for the \`timestamp_ltz\` data type.RocksDB Options#The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.
KeyDefaultTypeDescriptionlookup.cache-rows10000LongThe maximum number of rows to store in the cache.lookup.continuous.discovery-interval(none)DurationThe discovery interval of lookup continuous reading. This is used as an SQL hint. If it's not configured, the lookup function will fallback to 'continuous.discovery-interval'.rocksdb.block.blocksize4 kbMemorySizeThe approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.rocksdb.block.cache-size128 mbMemorySizeThe amount of the cache for data blocks in RocksDB.rocksdb.block.metadata-blocksize4 kbMemorySizeApproximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.rocksdb.bloom-filter.bits-per-key10.0DoubleBits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.rocksdb.bloom-filter.block-based-modefalseBooleanIf true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.rocksdb.compaction.level.max-size-level-base256 mbMemorySizeThe upper-bound of the total size of level base files in bytes. The default value is '256MB'.rocksdb.compaction.level.target-file-size-base64 mbMemorySizeThe target file size for compaction, which determines a level-1 file size. The default value is '64MB'.rocksdb.compaction.level.use-dynamic-sizefalseBooleanIf true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.rocksdb.compaction.styleLEVELEnum
The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.
Possible values:"LEVEL""UNIVERSAL""FIFO""NONE"rocksdb.compression.typeLZ4_COMPRESSIONEnum
The compression type.
Possible values:"NO_COMPRESSION""SNAPPY_COMPRESSION""ZLIB_COMPRESSION""BZLIB2_COMPRESSION""LZ4_COMPRESSION""LZ4HC_COMPRESSION""XPRESS_COMPRESSION""ZSTD_COMPRESSION""DISABLE_COMPRESSION_OPTION"rocksdb.files.open-1IntegerThe maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.rocksdb.thread.num2IntegerThe maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.rocksdb.use-bloom-filterfalseBooleanIf true, every newly created SST file will contain a Bloom filter. It is disabled by default.rocksdb.writebuffer.count2IntegerThe maximum number of write buffers that are built up in memory. The default value is '2'.rocksdb.writebuffer.number-to-merge1IntegerThe minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.rocksdb.writebuffer.size64 mbMemorySizeThe amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.`}),e.add({id:112,href:"/learn-paimon/",title:"Learn Paimon",section:"Apache Paimon",content:``}),e.add({id:113,href:"/concepts/rest/rest-api/",title:"REST API",section:"RESTCatalog",content:``}),e.add({id:114,href:"/versions/",title:"Versions",section:"Apache Paimon",content:`Versions#An appendix of hosted documentation for all versions of Apache Paimon.
masterstable1.21.11.0`})})()
[{"id":0,"href":"/concepts/","title":"Concepts","section":"Apache Paimon","content":"\r"},{"id":1,"href":"/maintenance/filesystems/","title":"Filesystems","section":"Maintenance","content":"\rFilesystems\r#\rApache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems\r#\rFileSystem URI Scheme Pluggable Description Local File System file:// N Built-in Support HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment Aliyun OSS oss:// Y S3 s3:// Y Tencent Cloud Object Storage cosn:// Y Huawei OBS obs:// Y Dependency\r#\rWe recommend you to download the jar directly: Download Link.\nYou can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-${fs}/target/paimon-${fs}-1.1.1.jar.\nHDFS\r#\rYou don\u0026rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.\nHDFS Configuration\r#\rFor HDFS, the most important thing is to be able to read your HDFS configuration.\nFlink\rYou may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:\nSet environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog. The first approach is recommended.\nIf you do not want to include the value of the environment variable, you can configure hadoop-conf-loader to option.\nHive/Spark\rHDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details.\rHadoop-compatible file systems (HCFS)\r#\rAll Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.\nThis way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).\nHDFS Alluxio (see configuration specifics below) XtreemFS … The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.\nFor Alluxio support add the following entry into the core-site.xml file:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Kerberos\r#\rFlink\rIt is recommended to use Flink Kerberos Keytab.\rSpark\rIt is recommended to use Spark Kerberos Keytab.\rHive\rAn intuitive approach is to configure Hive\u0026rsquo;s kerberos authentication.\rTrino/JavaAPI\rConfigure the following three options in your catalog configuration:\nsecurity.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache. For JavaAPI:\nSecurityContext.install(catalogOptions); HDFS HA\r#\rEnsure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.\nHDFS ViewFS\r#\rEnsure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.\nOSS\r#\rDownload paimon-oss-1.1.1.jar.\rFlink\rIf you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-oss-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark\rIf you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.\nPlace paimon-oss-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\ --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\ --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\ --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy Hive\rIf you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.\nNOTE: You need to ensure that Hive metastore can access oss.\nPlace paimon-oss-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino\rFrom version 0.8, paimon-trino uses trino filesystem as basic file read and write system. We strongly recommend you to use jindo-sdk in trino.\nYou can find How to config jindo sdk on trino here. Please note that:\nUse paimon to replace hive-hadoop2 when you decompress the plugin jar and find location to put in. You can specify the core-site.xml in paimon.properties on configuration hive.config.resources. Presto and Jindo use the same configuration method. If you environment has jindo sdk dependencies, you can use Jindo Fs to connect OSS. Jindo has better read and write efficiency.\nDownload paimon-jindo-1.1.1.jar.\rS3\r#\rDownload paimon-s3-1.1.1.jar.\rFlink\rIf you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-s3-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark\rIf you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.\nPlace paimon-s3-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\ --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\ --conf spark.sql.catalog.paimon.s3.access-key=xxx \\ --conf spark.sql.catalog.paimon.s3.secret-key=yyy Hive\rIf you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.\nNOTE: You need to ensure that Hive metastore can access s3.\nPlace paimon-s3-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino\rPaimon use shared trino filesystem as basic read and write system.\nPlease refer to Trino S3 to config s3 filesystem in trino.\nS3 Compliant Object Stores\r#\rThe S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent\u0026rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.\ns3.endpoint: your-endpoint-hostname Configure Path Style Access\r#\rSome S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.\ns3.path.style.access: true S3A Performance\r#\rTune Performance for S3AFileSystem.\nIf you encounter the following exception:\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.\nGoogle Cloud Storage\r#\rDownload paimon-gs-1.1.1.jar.\rFlink\rIf you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-gs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.gs.auth.type\u0026#39; = \u0026#39;SERVICE_ACCOUNT_JSON_KEYFILE\u0026#39;, \u0026#39;fs.gs.auth.service.account.json.keyfile\u0026#39; = \u0026#39;/path/to/service-account-.json\u0026#39; ); Microsoft Azure Storage\r#\rDownload paimon-azure-1.1.1.jar.\rFlink\rIf you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-gs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.gs.auth.type\u0026#39; = \u0026#39;SERVICE_ACCOUNT_JSON_KEYFILE\u0026#39;, \u0026#39;fs.gs.auth.service.account.json.keyfile\u0026#39; = \u0026#39;/path/to/service-account-.json\u0026#39; ); Microsoft Azure Storage\r#\rDownload paimon-azure-1.1.1.jar.\rFlink\rIf you have already configured azure access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-azure-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;wasb://,\u0026lt;container\u0026gt;@\u0026lt;account\u0026gt;.blob.core.windows.net/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.azure.account.key.Account.blob.core.windows.net\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark\rIf you have already configured azure access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.\nPlace paimon-azure-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=wasb://,\u0026lt;container\u0026gt;@\u0026lt;account\u0026gt;.blob.core.windows.net/\u0026lt;path\u0026gt; \\ --conf fs.azure.account.key.Account.blob.core.windows.net=yyy \\ OBS\r#\rDownload paimon-obs-1.1.1.jar.\rFlink\rIf you have already configured obs access through Flink (Via Flink FileSystem), here you can skip the following configuration.\nPut paimon-obs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;obs://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.obs.endpoint\u0026#39; = \u0026#39;obs-endpoint-hostname\u0026#39;, \u0026#39;fs.obs.access.key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.obs.secret.key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark\rIf you have already configured obs access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.\nPlace paimon-obs-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=obs://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\ --conf spark.sql.catalog.paimon.fs.obs.endpoint=obs-endpoint-hostname \\ --conf spark.sql.catalog.paimon.fs.obs.access.key=xxx \\ --conf spark.sql.catalog.paimon.fs.obs.secret.key=yyy Hive\rIf you have already configured obs access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.\nNOTE: You need to ensure that Hive metastore can access obs.\nPlace paimon-obs-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.obs.endpoint=obs-endpoint-hostname; SET paimon.fs.obs.access.key=xxx; SET paimon.fs.obs.secret.key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; "},{"id":2,"href":"/program-api/flink-api/","title":"Flink API","section":"Program API","content":"\rFlink API\r#\rIf possible, recommend using Flink SQL or Spark SQL, or simply use SQL APIs in programs.\rDependency\r#\rMaven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-1.20\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.20.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Flink.\rPlease choose your Flink version.\nPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nNot only DataStream API, you can also read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.\nWrite to Table\r#\rimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.sink.FlinkSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.types.DataType; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() throws Exception { // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval // env.enableCheckpointing(60_000); // create a changelog DataStream DataStream\u0026lt;Row\u0026gt; input = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}, Types.STRING, Types.INT)); // get table from catalog Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); DataType inputType = DataTypes.ROW( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT())); FlinkSinkBuilder builder = new FlinkSinkBuilder(table).forRow(input, inputType); // set sink parallelism // builder.parallelism(_your_parallelism) // set overwrite mode // builder.overwrite(...) builder.build(); env.execute(); } } Read from Table\r#\rimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.source.FlinkSourceBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get table from catalog Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); // table = table.copy(Collections.singletonMap(\u0026#34;scan.file-creation-time-millis\u0026#34;, \u0026#34;...\u0026#34;)); FlinkSourceBuilder builder = new FlinkSourceBuilder(table).env(env); // builder.sourceBounded(true); // builder.projection(...); // builder.predicate(...); // builder.limit(...); // builder.sourceParallelism(...); DataStream\u0026lt;Row\u0026gt; dataStream = builder.buildForRow(); // use this datastream dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] } } Cdc ingestion Table\r#\rPaimon supports ingest data into Paimon tables with schema evolution.\nYou can use Java API to write cdc records into Paimon Tables. You can write records to Paimon\u0026rsquo;s partial-update table with adding columns dynamically. Here is an example to use RichCdcSinkBuilder API:\nimport org.apache.paimon.catalog.CatalogLoader; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.sink.cdc.RichCdcRecord; import org.apache.paimon.flink.sink.cdc.RichCdcSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataTypes; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import static org.apache.paimon.types.RowKind.INSERT; public class WriteCdcToTable { public static void writeTo() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval // env.enableCheckpointing(60_000); DataStream\u0026lt;RichCdcRecord\u0026gt; dataStream = env.fromElements( RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;123\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;62.2\u0026#34;) .build(), // dt field will be added with schema evolution RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;245\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;82.1\u0026#34;) .field(\u0026#34;dt\u0026#34;, DataTypes.TIMESTAMP(), \u0026#34;2023-06-12 20:21:12\u0026#34;) .build()); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;); Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); CatalogLoader catalogLoader = () -\u0026gt; FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalogLoader.load().getTable(identifier); new RichCdcSinkBuilder(table) .forRichCdcRecord(dataStream) .identifier(identifier) .catalogLoader(catalogLoader) .build(); env.execute(); } } "},{"id":3,"href":"/migration/migration-from-hive/","title":"Migration From Hive","section":"Migration","content":"\rHive Table Migration\r#\rApache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table.\nNow, we can use paimon hive catalog with Migrate Table Procedure to totally migrate a table from hive to paimon. At the same time, you can use paimon hive catalog with Migrate Database Procedure to fully synchronize all tables in the database to paimon.\nMigrate Table Procedure: Paimon table does not exist, use the procedure upgrade hive table to paimon table. Hive table will disappear after action done. Migrate Database Procedure: Paimon table does not exist, use the procedure upgrade all hive tables in database to paimon table. All hive tables will disappear after action done. These three actions now support file format of hive \u0026ldquo;orc\u0026rdquo; and \u0026ldquo;parquet\u0026rdquo; and \u0026ldquo;avro\u0026rdquo;.\nWe highly recommend to back up hive table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. Migrate Hive Table\r#\rFlink SQL\rCREATE CATALOG PAIMON WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_table( connector =\u0026gt; \u0026#39;hive\u0026#39;, source_table =\u0026gt; \u0026#39;default.hivetable\u0026#39;, -- You can specify the target table, and if the target table already exists -- the file will be migrated directly to it -- target_table =\u0026gt; \u0026#39;default.paimontarget\u0026#39;, -- You can specify delete_origin is false, this won\u0026#39;t delete hivetable -- delete_origin =\u0026gt; false, options =\u0026gt; \u0026#39;file.format=orc\u0026#39;); Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-1.1.1.jar \\ migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --table default.hive_or_paimon After invoke, \u0026ldquo;hivetable\u0026rdquo; will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail.\nMigrate Hive Database\r#\rFlink SQL\rCREATE CATALOG PAIMON WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_database( connector =\u0026gt; \u0026#39;hive\u0026#39;, source_database =\u0026gt; \u0026#39;default\u0026#39;, options =\u0026gt; \u0026#39;file.format=orc\u0026#39;); Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_databse \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --source_type hive \\ --database \u0026lt;database\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-1.1.1.jar migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --database default After invoke, all tables in \u0026ldquo;default\u0026rdquo; database will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail.\n"},{"id":4,"href":"/migration/migration-from-iceberg/","title":"Migration From Iceberg","section":"Migration","content":"\rIceberg Migration\r#\rApache Iceberg data with parquet file format could be migrated to Apache Paimon. When migrating an iceberg table to a paimon table, the origin iceberg table will permanently disappear. So please back up your data if you still need the original table. The migrated paimon table will be an append table.\nWe highly recommend to back up iceberg table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. Migrate Iceberg Table\r#\rCurrently, we can use paimon catalog with MigrateIcebergTableProcedure or MigrateIcebergTableAction to migrate the data used by latest iceberg snapshot in an iceberg table to a paimon table.\nIceberg tables managed by hadoop-catalog or hive-catalog are supported to be migrated to paimon. As for the type of paimon catalog, it needs to have access to the file system where the iceberg metadata and data files are located. This means we could migrate an iceberg table managed by hadoop-catalog to a paimon table in hive catalog if their warehouses are in the same file system.\nWhen migrating, the iceberg data files which were marked by DELETED will be ignored. Only the data files referenced by manifest entries with \u0026lsquo;EXISTING\u0026rsquo; and \u0026lsquo;ADDED\u0026rsquo; content will be migrated to paimon. Notably, now we don\u0026rsquo;t support migrating iceberg tables with delete files(deletion vectors, position delete files, equality delete files etc.)\nNow only parquet format is supported in iceberg migration.\nMigrateIcebergTableProcedure\r#\rYou can run the following command to migrate an iceberg table to a paimon table.\n-- Use named argument CALL sys.migrate_iceberg_table(source_table =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, iceberg_options =\u0026gt; \u0026#39;iceberg_options\u0026#39;, options =\u0026gt; \u0026#39;paimon_options\u0026#39;, parallelism =\u0026gt; parallelism); -- Use indexed argument CALL sys.migrate_iceberg_table(\u0026#39;source_table\u0026#39;,\u0026#39;iceberg_options\u0026#39;, \u0026#39;options\u0026#39;, \u0026#39;parallelism\u0026#39;); source_table, string type, is used to specify the source iceberg table to migrate, it\u0026rsquo;s required. iceberg_options, string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it\u0026rsquo;s required. options, string type, is used to specify the additional options for the target paimon table, it\u0026rsquo;s optional. parallelism, integer type, is used to specify the parallelism of the migration job, it\u0026rsquo;s optional. hadoop-catalog\r#\rTo migrate iceberg table managed by hadoop-catalog, you need set metadata.iceberg.storage=hadoop-catalog and iceberg_warehouse. Example:\nCREATE CATALOG paimon_catalog WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39;); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse\u0026#39; ); If you want the metadata of the migrated paimon table to be managed by hive, you can also create a hive catalog of paimon for migration. Example:\nCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse\u0026#39; ); hive-catalog\r#\rTo migrate iceberg table managed by hive-catalog, you need set metadata.iceberg.storage=hive-catalog and provide information about Hive Metastore used by the iceberg table in iceberg_options.\nOption\rDefault\rType\rDescription\rmetadata.iceberg.uri\rnone\rString\rHive metastore uri for Iceberg Hive catalog.\rmetadata.iceberg.hive-conf-dir\rnone\rString\rhive-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.hadoop-conf-dir\rnone\rString\rhadoop-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.hive-client-class\rorg.apache.hadoop.hive.metastore.HiveMetaStoreClient\rString\rHive client class name for Iceberg Hive Catalog.\rExample:\nCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hive-catalog,metadata.iceberg.uri=thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); MigrateIcebergTableAction\r#\rYou can also use flink action for migration:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_iceberg_table \\ --table \u0026lt;icebergDatabase.icebergTable\u0026gt; \\ --iceberg_options \u0026lt;iceberg-conf [,iceberg-conf ...]\u0026gt; \\ [--parallelism \u0026lt;parallelism\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_iceberg_table \\ --table iceberg_db.iceberg_tbl \\ --iceberg_options metadata.iceberg.storage=hive-catalog, metadata.iceberg.uri=thrift://localhost:9083 \\ --parallelism 6 \\ --catalog_conf warehouse=/path/to/paimon/warehouse \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://localhost:9083 "},{"id":5,"href":"/append-table/overview/","title":"Overview","section":"Table w/o PK","content":"\rOverview\r#\rIf a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through upsert. It can only receive incoming data from append data.\nFlink\rCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( -- \u0026#39;target-file-size\u0026#39; = \u0026#39;256 MB\u0026#39;, -- \u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;, -- \u0026#39;file.compression\u0026#39; = \u0026#39;zstd\u0026#39;, -- \u0026#39;file.compression.zstd-level\u0026#39; = \u0026#39;3\u0026#39; ); Batch write and batch read in typical application scenarios, similar to a regular Hive partition table, but compared to the Hive table, it can bring:\nObject storage (S3, OSS) friendly Time Travel and Rollback DELETE / UPDATE with low cost Automatic small file merging in streaming sink Streaming read \u0026amp; write like a queue High performance query with order and index "},{"id":6,"href":"/cdc-ingestion/overview/","title":"Overview","section":"CDC Ingestion","content":"\rOverview\r#\rPaimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.\nWe currently support the following sync ways:\nMySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database. Program API Sync: synchronize your custom DataStream input into one Paimon table. Kafka Synchronizing Table: synchronize one Kafka topic\u0026rsquo;s table into one Paimon table. Kafka Synchronizing Database: synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. MongoDB Synchronizing Collection: synchronize one Collection from MongoDB into one Paimon table. MongoDB Synchronizing Database: synchronize the whole MongoDB database into one Paimon database. Pulsar Synchronizing Table: synchronize one Pulsar topic\u0026rsquo;s table into one Paimon table. Pulsar Synchronizing Database: synchronize one Pulsar topic containing multiple tables or multiple topics containing one table each into one Paimon database. What is Schema Evolution\r#\rSuppose we have a MySQL table named tableA, it has three fields: field_1, field_2, field_3. When we want to load this MySQL table to Paimon, we can do this in Flink SQL, or use MySqlSyncTableAction.\nFlink SQL:\nIn Flink SQL, if we change the table schema of the MySQL table after the ingestion, the table schema change will not be synchronized to Paimon.\nMySqlSyncTableAction:\nIn MySqlSyncTableAction, if we change the table schema of the MySQL table after the ingestion, the table schema change will be synchronized to Paimon, and the data of field_4 which is newly added will be synchronized to Paimon too.\nSchema Change Evolution\r#\rCdc Ingestion supports a limited number of schema changes. Currently, the framework can not rename table, drop columns, so the behaviors of RENAME TABLE and DROP COLUMN will be ignored, RENAME COLUMN will add a new column. Currently supported schema changes includes:\nAdding columns.\nAltering column types. More specifically,\naltering from a string type (char, varchar, text) to another string type with longer length, altering from a non-string type to string type (char, varchar, text), altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range, are supported.\nComputed Functions\r#\r--computed_column are the definitions of computed columns. The argument field is from source table field name.\nTemporal Functions\r#\rTemporal functions can convert date and epoch time to another form. A common use case is to generate partition values.\nFunction\rDescription\ryear(temporal-column [, precision])\rExtract year from the input. Output is an INT value represent the year.\rmonth(temporal-column [, precision])\rExtract month of year from the input. Output is an INT value represent the month of year.\rday(temporal-column [, precision])\rExtract day of month from the input. Output is an INT value represent the day of month.\rhour(temporal-column [, precision])\rExtract hour from the input. Output is an INT value represent the hour.\rminute(temporal-column [, precision])\rExtract minute from the input. Output is an INT value represent the minute.\rsecond(temporal-column [, precision])\rExtract second from the input. Output is an INT value represent the second.\rdate_format(temporal-column, format-string [, precision])\rConvert the input to desired formatted string. Output type is STRING.\rnow()\rGet the timestamp when ingesting the record. Output type is TIMESTAMP_LTZ(3).\rThe data type of the temporal-column can be one of the following cases:\nDATE, DATETIME or TIMESTAMP. Any integer numeric type (such as INT and BIGINT). In this case, the data will be considered as epoch time of 1970-01-01 00:00:00. You should set precision of the value (default is 0). STRING. In this case, if you didn\u0026rsquo;t set the time unit, the data will be considered as formatted string of DATE, DATETIME or TIMESTAMP value. Otherwise, the data will be considered as string value of epoch time. So you must set time unit in the latter case. The precision represents the unit of the epoch time. Currently, There are four valid precisions: 0 (for epoch seconds), 3 (for epoch milliseconds), 6(for epoch microseconds) and 9 (for epoch nanoseconds). Take the time point 1970-01-01 00:00:00.123456789 as an example, the epoch seconds are 0, the epoch milliseconds are 123, the epoch microseconds are 123456, and the epoch nanoseconds are 123456789. The precision should match the input values. You can set precision in this way: date_format(epoch_col, yyyy-MM-dd, 0).\ndate_format is a flexible function which is able to convert the temporal value to various formats with different format strings. A most common format string is yyyy-MM-dd HH:mm:ss.SSS. Another example is yyyy-ww which can extract the year and the week-of-the-year from the input. Note that the output is affected by the locale. For example, in some regions the first day of a week is Monday while in others is Sunday, so if you use date_format(date_col, yyyy-ww) and the input of date_col is 2024-01-07 (Sunday), the output maybe 2024-01 (if the first day of a week is Monday) or 2024-02 (if the first day of a week is Sunday).\nOther Functions\r#\rFunction\rDescription\rsubstring(column,beginInclusive)\rGet column.substring(beginInclusive). Output is a STRING.\rsubstring(column,beginInclusive,endExclusive)\rGet column.substring(beginInclusive,endExclusive). Output is a STRING.\rtruncate(column,width)\rtruncate column by width. Output type is same with column.If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely `value.substring(0, width)`.\rIf the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm `v - (((v % W) + W) % W)`. The `redundant` compute part is to keep the result always positive.\rIf the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let `scaled_W = decimal(W, scale(v))`, then return `v - (v % scaled_W)`.\rcast(value,dataType)\rGet a constant value. The output is an atomic type, such as STRING, INT, BOOLEAN, etc.\rSpecial Data Type Mapping\r#\rMySQL TINYINT(1) type will be mapped to Boolean by default. If you want to store number (-128~127) in it like MySQL, you can specify type mapping option tinyint1-not-bool (Use --type_mapping), then the column will be mapped to TINYINT in Paimon table. You can use type mapping option to-nullable (Use --type_mapping) to ignore all NOT NULL constraints (except primary keys). You can use type mapping option to-string (Use --type_mapping) to map all MySQL data type to STRING. You can use type mapping option char-to-string (Use --type_mapping) to map MySQL CHAR(length)/VARCHAR(length) types to STRING. You can use type mapping option longtext-to-bytes (Use --type_mapping) to map MySQL LONGTEXT types to BYTES. MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL will be mapped to DECIMAL(20, 0) by default. You can use type mapping option bigint-unsigned-to-bigint (Use --type_mapping) to map these types to Paimon BIGINT, but there is potential data overflow because BIGINT UNSIGNED can store up to 20 digits integer value but Paimon BIGINT can only store up to 19 digits integer value. So you should ensure the overflow won\u0026rsquo;t occur when using this option. MySQL BIT(1) type will be mapped to Boolean. When using Hive catalog, MySQL TIME type will be mapped to STRING. MySQL BINARY will be mapped to Paimon VARBINARY. This is because the binary value is passed as bytes in binlog, so it should be mapped to byte type (BYTES or VARBINARY). We choose VARBINARY because it can retain the length information. Custom Job Settings\r#\rCheckpointing\r#\rUse -Dexecution.checkpointing.interval=\u0026lt;interval\u0026gt; to enable checkpointing and set interval. For 0.7 and later versions, if you haven\u0026rsquo;t enabled checkpointing, Paimon will enable checkpointing by default and set checkpoint interval to 180 seconds.\nJob Name\r#\rUse -Dpipeline.name=\u0026lt;job-name\u0026gt; to set custom synchronization job name.\ntable configuration\r#\rYou can use --table_conf to set table properties and some flink job properties (like sink.parallelism). If the table is created by the cdc job, the table\u0026rsquo;s properties will be equal to the given properties. Otherwise, the job will use the given properties to alter table\u0026rsquo;s properties. But note that immutable options (like merge-engine) and bucket number won\u0026rsquo;t be altered.\n"},{"id":7,"href":"/concepts/overview/","title":"Overview","section":"Concepts","content":"\rOverview\r#\rApache Paimon\u0026rsquo;s Architecture:\nAs shown in the architecture above:\nRead/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.\nFor reads, it supports consuming data from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way. For writes, it supports streaming synchronization from the changelog of databases (CDC) batch insert/overwrite from offline data. Ecosystem: In addition to Apache Flink, Paimon also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal:\nUnder the hood, Paimon stores the columnar files on the filesystem/object-store The metadata of the file is saved in the manifest file, providing large-scale storage and data skipping. For primary key table, uses the LSM tree structure to support a large volume of data updates and high-performance queries. Unified Storage\r#\rFor streaming engines like Apache Flink, there are typically three types of connectors:\nMessage queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as ClickHouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE. Paimon provides table abstraction. It is used in a way that does not differ from the traditional database:\nIn batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires. "},{"id":8,"href":"/concepts/rest/overview/","title":"Overview","section":"RESTCatalog","content":"\rRESTCatalog\r#\rOverview\r#\rPaimon REST Catalog provides a lightweight implementation to access the catalog service. Paimon could access the catalog service through a catalog server which implements REST API. You can see all APIs in REST API.\nKey Features\r#\rUser Defined Technology-Specific Logic Implementation All technology-specific logic within the catalog server. This ensures that the user can define logic that could be owned by the user. Decoupled Architecture The REST Catalog interacts with the catalog server through a well-defined REST API. This decoupling allows for independent evolution and scaling of the catalog server and clients. Language Agnostic Developers can implement the catalog server in any programming language, provided that it adheres to the specified REST API. This flexibility enables teams to utilize their existing tech stacks and expertise. Support for Any Catalog Backend REST Catalog is designed to work with any catalog backend. As long as they implement the relevant APIs, they can seamlessly integrate with REST Catalog. Conclusion\r#\rREST Catalog offers adaptable solution for accessing the catalog service. According to REST API is decoupled from the catalog service.\nTechnology-specific Logic is encapsulated on the catalog server. At the same time, the catalog server supports any backend and languages.\nToken Provider\r#\rRESTCatalog supports multiple access authentication methods, including the following:\nBear Token. DLF Token. "},{"id":9,"href":"/concepts/spec/overview/","title":"Overview","section":"Specification","content":"\rSpec Overview\r#\rThis is the specification for the Paimon table format, this document standardizes the underlying file structure and design of Paimon.\nTerms\r#\rSchema: fields, primary keys definition, partition keys definition and options. Snapshot: the entrance to all data committed at some specific time point. Manifest list: includes several manifest files. Manifest: includes several data files or changelog files. Data File: contains incremental records. Changelog File: contains records produced by changelog-producer. Global Index: index for a bucket or partition. Data File Index: index for a data file. Run Flink SQL with Paimon:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/your/path\u0026#39; ); USE CATALOG my_catalog; CREATE TABLE my_table ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, f1 STRING ); INSERT INTO my_table VALUES (1, 11, \u0026#39;111\u0026#39;); Take a look to the disk:\nwarehouse └── default.db └── my_table ├── bucket-0 │ └── data-59f60cb9-44af-48cc-b5ad-59e85c663c8f-0.orc ├── index │ └── index-5625e6d9-dd44-403b-a738-2b6ea92e20f1-0 ├── manifest │ ├── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 │ ├── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 │ ├── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-0 │ └── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 "},{"id":10,"href":"/ecosystem/overview/","title":"Overview","section":"Ecosystem","content":"\rOverview\r#\rCompatibility Matrix\r#\rEngine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite DELETE \u0026amp; UPDATE MERGE INTO Time Travel Flink 1.15 - 1.20 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅ ✅(1.17+) ❌ ✅ Spark 3.2 - 3.5 ✅ ✅ ✅ ✅ ✅(3.3+) ✅(3.3+) ✅ ✅ ✅ ✅(3.3+) Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Trino 420 - 440 ✅ ✅(427+) ✅(427+) ✅(427+) ❌ ❌ ❌ ❌ ❌ ✅ Presto 0.236 - 0.280 ✅ ❌ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ StarRocks 3.1+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Doris 2.0.6+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅ Streaming Engines\r#\rFlink Streaming\r#\rFlink is the most comprehensive streaming computing engine that is widely used for data CDC ingestion and the construction of streaming pipelines.\nRecommended version is Flink 1.17.2.\nSpark Streaming\r#\rYou can also use Spark Streaming to build a streaming pipeline. Spark\u0026rsquo;s schema evolution capability will be better implemented, but you must accept the mechanism of mini-batch.\nBatch Engines\r#\rSpark Batch\r#\rSpark Batch is the most widely used batch computing engine.\nRecommended version is Spark 3.4.3.\nFlink Batch\r#\rFlink Batch is also available, which can make your pipeline more integrated with streaming and batch unified.\nOLAP Engines\r#\rStarRocks\r#\rStarRocks is the most recommended OLAP engine with the most advanced integration.\nRecommended version is StarRocks 3.2.6.\nOther OLAP\r#\rYou can also use Doris and Trino and Presto, or, you can just use Spark, Flink and Hive to query Paimon tables.\nDownload\r#\rDownload Link\n"},{"id":11,"href":"/primary-key-table/merge-engine/overview/","title":"Overview","section":"Merge Engine","content":"\rOverview\r#\rWhen Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nAlways set `table.exec.sink.upsert-materialize` to `NONE` in Flink SQL TableConfig, sink upsert-materialize may\rresult in strange behavior. When the input is out of order, we recommend that you use\r[Sequence Field](https://example.org/primary-key-table/sequence-rowkind/#sequence-field) to correct disorder.\rDeduplicate\r#\rThe deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted. You can config ignore-delete to ignore it.\n"},{"id":12,"href":"/primary-key-table/overview/","title":"Overview","section":"Table with PK","content":"\rOverview\r#\rIf you define a table with primary key, you can insert, update or delete records in the table.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key. See CREATE TABLE.\nBucket\r#\rUnpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nEach bucket directory contains an LSM tree and its changelog files.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 200MB - 1GB.\nAlso, see rescale bucket if you want to adjust the number of buckets after a table is created.\nLSM Trees\r#\rPaimon adopts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs\r#\rLSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapped primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\n"},{"id":13,"href":"/flink/quick-start/","title":"Quick Start","section":"Engine Flink","content":"\rQuick Start\r#\rThis documentation is a guide for using Paimon in Flink.\nJars\r#\rPaimon currently supports Flink 1.20, 1.19, 1.18, 1.17, 1.16, 1.15. We recommend the latest Flink version for a better experience.\nDownload the jar file with corresponding version.\nCurrently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction, Version Type Jar Flink 2.0 Bundled Jar paimon-flink-2.0-1.1.1.jar Flink 1.20 Bundled Jar paimon-flink-1.20-1.1.1.jar Flink 1.19 Bundled Jar paimon-flink-1.19-1.1.1.jar Flink 1.18 Bundled Jar paimon-flink-1.18-1.1.1.jar Flink 1.17 Bundled Jar paimon-flink-1.17-1.1.1.jar Flink 1.16 Bundled Jar paimon-flink-1.16-1.1.1.jar Flink 1.15 Bundled Jar paimon-flink-1.15-1.1.1.jar Flink Action Action Jar paimon-flink-action-1.1.1.jar You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find the bundled jar in ./paimon-flink/paimon-flink-\u0026lt;flink-version\u0026gt;/target/paimon-flink-\u0026lt;flink-version\u0026gt;-1.1.1.jar, and the action jar in ./paimon-flink/paimon-flink-action/target/paimon-flink-action-1.1.1.jar.\nStart\r#\rStep 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar\nCopy paimon bundled jar to the lib directory of your Flink home.\ncp paimon-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nIf the machine is in a hadoop environment, please ensure the value of the environment variable `HADOOP_CLASSPATH` include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.\rDownload Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml(Flink version \u0026lt; 1.19) or \u0026lt;FLINK_HOME\u0026gt;/conf/config.yaml(Flink version \u0026gt;= 1.19).\ntaskmanager.numberOfTaskSlots: 2 To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\nCatalog\r-- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Generic-Catalog\rUsing FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!\nIn this mode, you should use \u0026lsquo;connector\u0026rsquo; option for creating tables.\nPaimon will use hive.metastore.warehouse.dir in your hive-site.xml, please use path with scheme. For example, hdfs://.... Otherwise, Paimon will use the local path.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon-generic\u0026#39;, \u0026#39;hive-conf-dir\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;hadoop-conf-dir\u0026#39;=\u0026#39;...\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;paimon\u0026#39; ); Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count; -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Use Flink Managed Memory\r#\rPaimon tasks can create memory pools based on executor memory which will be managed by Flink executor, such as managed memory in Flink task manager. It will improve the stability and performance of sinks by managing writer buffers for multiple tasks through executor.\nThe following properties can be set if using Flink managed memory:\nOption Default Description sink.use-managed-memory-allocator false If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator, which means each task allocates and manages its own memory pool (heap memory), if there are too many tasks in one Executor, it may cause performance issues and even OOM. sink.managed.writer-buffer-memory 256M Weight of writer buffer in managed memory, Flink will compute the memory size, for writer according to the weight, the actual memory used depends on the running environment. Now the memory size defined in this property are equals to the exact memory allocated to write buffer in runtime. Use In SQL Users can set memory weight in SQL for Flink Managed Memory, then Flink sink operator will get the memory pool size and create allocator for Paimon writer.\nINSERT INTO paimon_table /*+ OPTIONS(\u0026#39;sink.use-managed-memory-allocator\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;sink.managed.writer-buffer-memory\u0026#39;=\u0026#39;256M\u0026#39;) */ SELECT * FROM ....; Setting dynamic options\r#\rWhen interacting with the Paimon table, table options can be tuned without changing the options in the catalog. Paimon will extract job-level dynamic options and take effect in the current session. The dynamic table option\u0026rsquo;s key format is paimon.${catalogName}.${dbName}.${tableName}.${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. The dynamic global option\u0026rsquo;s key format is ${config_key}. Global options will take effect for all the tables. Table options will override global options if there are conflicts.\nFor example:\n-- set scan.timestamp-millis=1697018249001 for all tables SET \u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1697018249001\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T SET \u0026#39;paimon.mycatalog.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table default.T in any catalog SET \u0026#39;paimon.*.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T1 -- set scan.timestamp-millis=1697018249001 for others tables SET \u0026#39;paimon.mycatalog.default.T1.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SET \u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1697018249001\u0026#39;; SELECT * FROM T1 JOIN T2 ON xxxx; "},{"id":14,"href":"/spark/quick-start/","title":"Quick Start","section":"Engine Spark","content":"\rQuick Start\r#\rPreparation\r#\rPaimon currently supports Spark 3.5, 3.4, 3.3, and 3.2. We recommend the latest Spark version for a better experience.\nDownload the jar file with corresponding version.\nVersion Jar Spark 3.5 paimon-spark-3.5-1.1.1.jar Spark 3.4 paimon-spark-3.4-1.1.1.jar Spark 3.3 paimon-spark-3.3-1.1.1.jar Spark 3.2 paimon-spark-3.2-1.1.1.jar You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-1.1.1.jar.\nSetup\r#\rIf you are using HDFS, make sure that the environment variable `HADOOP_HOME` or `HADOOP_CONF_DIR` is set.\rStep 1: Specify Paimon Jar File\nAppend path to paimon jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/paimon-spark-3.3-1.1.1.jar OR use the --packages option.\nspark-sql ... --packages org.apache.paimon:paimon-spark-3.3:1.1.1 Alternatively, you can copy paimon-spark-3.3-1.1.1.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Paimon Catalog\nCatalog\rWhen starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.\nspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon \\ --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Catalogs are configured using properties under spark.sql.catalog.(catalog_name). In above case, \u0026lsquo;paimon\u0026rsquo; is the catalog name, you can change it to your own favorite catalog name.\nAfter spark-sql command line has started, run the following SQL to create and switch to database default.\nUSE paimon; USE default; After switching to the catalog ('USE paimon'), Spark\u0026rsquo;s existing tables will not be directly accessible, you can use the spark_catalog.${database_name}.${table_name} to access Spark tables.\nGeneric Catalog\rWhen starting spark-sql, use the following command to register Paimon’s Spark Generic catalog to replace Spark default catalog spark_catalog. (default warehouse is Spark spark.sql.warehouse.dir)\nCurrently, it is only recommended to use SparkGenericCatalog in the case of Hive metastore, Paimon will infer Hive conf from Spark session, you just need to configure Spark\u0026rsquo;s Hive conf.\nspark-sql ... \\ --conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog \\ --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Using SparkGenericCatalog, you can use Paimon tables in this Catalog or non-Paimon tables such as Spark\u0026rsquo;s csv, parquet, Hive tables, etc.\nCreate Table\r#\rCatalog\rcreate table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); Generic Catalog\rcreate table my_table ( k int, v string ) USING paimon tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); Insert Table\r#\rSQL\rINSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); DataFrame\r-- you can use Seq((1, \u0026#34;Hi\u0026#34;), (2, \u0026#34;Hello\u0026#34;)).toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) .write.format(\u0026#34;paimon\u0026#34;).mode(\u0026#34;append\u0026#34;).saveAsTable(\u0026#34;my_table\u0026#34;) -- or Seq((1, \u0026#34;Hi\u0026#34;), (2, \u0026#34;Hello\u0026#34;)).toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) .write.format(\u0026#34;paimon\u0026#34;).mode(\u0026#34;append\u0026#34;).save(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;) Query Table\r#\rSQL\rSELECT * FROM my_table; /* 1\tHi 2\tHello */ DataFrame\r-- you can use spark.read.format(\u0026#34;paimon\u0026#34;).table(\u0026#34;my_table\u0026#34;).show() -- or spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;).show() /* +---+------+ | k | v| +---+------+ | 1| Hi| | 2| Hello| +---+------+ */ Spark Type Conversion\r#\rThis section lists all supported type conversion between Spark and Paimon. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\nSpark Data Type\rPaimon Data Type\rAtomic Type\rStructType\rRowType\rfalse\rMapType\rMapType\rfalse\rArrayType\rArrayType\rfalse\rBooleanType\rBooleanType\rtrue\rByteType\rTinyIntType\rtrue\rShortType\rSmallIntType\rtrue\rIntegerType\rIntType\rtrue\rLongType\rBigIntType\rtrue\rFloatType\rFloatType\rtrue\rDoubleType\rDoubleType\rtrue\rStringType\rVarCharType(Integer.MAX_VALUE)\rtrue\rVarCharType(length)\rVarCharType(length)\rtrue\rCharType(length)\rCharType(length)\rtrue\rDateType\rDateType\rtrue\rTimestampType\rLocalZonedTimestamp\rtrue\rTimestampNTZType(Spark3.4+)\rTimestampType\rtrue\rDecimalType(precision, scale)\rDecimalType(precision, scale)\rtrue\rBinaryType\rVarBinaryType, BinaryType\rtrue\rDue to the previous design, in Spark3.3 and below, Paimon will map both Paimon's TimestampType and LocalZonedTimestamp to Spark's TimestampType, and only correctly handle with TimestampType.\rTherefore, when using Spark3.3 and below, reads Paimon table with LocalZonedTimestamp type written by other engines, such as Flink, the query result of LocalZonedTimestamp type will have time zone offset, which needs to be adjusted manually.\rWhen using Spark3.4 and above, all timestamp types can be parsed correctly.\r"},{"id":15,"href":"/project/roadmap/","title":"Roadmap","section":"Project","content":"\rRoadmap\r#\rFlink Lookup Join\r#\rSupport Flink Custom Data Distribution Lookup Join to reach large-scale data lookup join.\nProduce Iceberg snapshots\r#\rIntroduce a mode to produce Iceberg snapshots.\nVariant Type\r#\rSupport Variant Type with Spark 4.0 and Flink 2.0. Unlocking support for semi-structured data.\nFile Index\r#\rAdd more index:\nInverse Vector Compaction\r#\rSupport Vector Compaction for super Wide Table.\nFunction support\r#\rPaimon Catalog supports functions.\nFiles Schema Evolution Ingestion\r#\rIntroduce a files Ingestion with Schema Evolution.\n"},{"id":16,"href":"/learn-paimon/understand-files/","title":"Understand Files","section":"Learn Paimon","content":"\rUnderstand Files\r#\rThis article is specifically designed to clarify the impact that various file operations have on files.\nThis page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.\nPrerequisite\r#\rBefore delving further into this page, please ensure that you have read through the following sections:\nBasic Concepts, Primary Key Table and Append Table How to use Paimon in Flink. Understand File Operations\r#\rCreate Catalog\r#\rStart Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.\nCREATE CATALOG paimon WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;file:///tmp/paimon\u0026#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.\nCreate Table\r#\rExecute the following create table statement will create a Paimon table with 3 fields:\nCREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT \u0026#39;timestamp string in format yyyyMMdd\u0026#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0\nInsert Records Into Table\r#\rRun the following insert statement in Flink SQL:\nINSERT INTO T VALUES (1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:\nThe content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 1, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684155393354, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 1, \u0026#34;deltaRecordCount\u0026#34; : 1, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):\n./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\tmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.\nNow let\u0026rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:\nINSERT INTO T VALUES (2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;), (3, 10003, \u0026#39;varchar00003\u0026#39;, \u0026#39;20230503\u0026#39;), (4, 10004, \u0026#39;varchar00004\u0026#39;, \u0026#39;20230504\u0026#39;), (5, 10005, \u0026#39;varchar00005\u0026#39;, \u0026#39;20230505\u0026#39;), (6, 10006, \u0026#39;varchar00006\u0026#39;, \u0026#39;20230506\u0026#39;), (7, 10007, \u0026#39;varchar00007\u0026#39;, \u0026#39;20230507\u0026#39;), (8, 10008, \u0026#39;varchar00008\u0026#39;, \u0026#39;20230508\u0026#39;), (9, 10009, \u0026#39;varchar00009\u0026#39;, \u0026#39;20230509\u0026#39;), (10, 10010, \u0026#39;varchar00010\u0026#39;, \u0026#39;20230510\u0026#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:\n% ls -1tR . ./T: dt=20230501 dt=20230502\tdt=20230503\tdt=20230504\tdt=20230505\tdt=20230506\tdt=20230507\tdt=20230508\tdt=20230509\tdt=20230510\tsnapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1 # delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2\tmanifest-f1267033-e246-4470-a54c-5c27fdbdd074-0\t# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 # delta manifest list for snapshot-1 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table\r#\rNow let\u0026rsquo;s delete records that meet the condition dt\u0026gt;=20230503. In Flink SQL, execute the following statement:\nBatch\rDELETE FROM T WHERE dt \u0026gt;= \u0026#39;20230503\u0026#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:\n./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:\n+I[1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;]\r+I[2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.\nCompact Table\r#\rAs you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.\nLet\u0026rsquo;s trigger the full-compaction now, and run a dedicated compaction job through flink run:\nBatch\rFlink SQL\rCALL sys.compact( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, partitions =\u0026gt; \u0026#39;partition_name\u0026#39;, order_strategy =\u0026gt; \u0026#39;order_strategy\u0026#39;, order_by =\u0026gt; \u0026#39;order_by\u0026#39;, options =\u0026gt; \u0026#39;paimon_table_dynamic_conf\u0026#39; ); Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition \u0026lt;partition-name\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] an example would be (suppose you\u0026rsquo;re already in Flink home)\nFlink SQL\rCALL sys.compact(\u0026#39;T\u0026#39;); Flink Action\r./bin/flink run \\ ./lib/paimon-flink-action-1.1.1.jar \\ compact \\ --path file:///tmp/paimon/default.db/T All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 4, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;COMPACT\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684163217960, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 2, \u0026#34;deltaRecordCount\u0026#34; : -16, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)\nFor partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file. This is because there has been an upgrade of the file from level 0 to the highest level. Please rest assured that this is only a change in metadata, and the file is still the same. Alter Table\r#\rExecute the following statement to configure full-compaction:\nALTER TABLE T SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.\nExpire Snapshots\r#\rRemind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.\nDuring the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.\nLet\u0026rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:\nIt first deletes all marked data files, and records any changed buckets.\nIt then deletes any changelog files and associated manifests.\nFinally, it deletes the snapshots themselves and writes the earliest hint file.\nIf any directories are left empty after the deletion process, they will be deleted as well.\nLet\u0026rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are\nto be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:\nAs a result, partition 20230503 to 20230510 are physically deleted.\nFlink Stream Write\r#\rFinally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.\nTo begin, let\u0026rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.\nMySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots. Next, we will go over end-to-end data flow.\nMySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.\nPaimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.\nDuring checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot\ncontains information about all data files in the table.\nAt later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.\nUnderstand Small Files\r#\rMany users are concerned about small files, which can lead to:\nStability issue: Too many small files in HDFS, NameNode will be overstressed. Cost issue: A small file in HDFS will temporarily use the size of a minimum of one Block, for example 128 MB. Query efficiency: The efficiency of querying too many small files will be affected. Understand Checkpoints\r#\rAssuming you are using Flink Writer, each checkpoint generates 1-2 snapshots, and the checkpoint forces the files to be generated on DFS, so the smaller the checkpoint interval the more small files will be generated.\nSo first thing is increase checkpoint interval. By default, not only checkpoint will cause the file to be generated, but writer\u0026rsquo;s memory (write-buffer-size) exhaustion will also flush data to DFS and generate the corresponding file. You can enable write-buffer-spillable to generate spilled files in writer to generate bigger files in DFS.\nSo second thing is increase write-buffer-size or enable write-buffer-spillable. Understand Snapshots\r#\rPaimon maintains multiple versions of files, compaction and deletion of files are logical and do not actually delete files. Files are only really deleted when Snapshot is expired, so the first way to reduce files is to reduce the time it takes for snapshot to be expired. Flink writer will automatically expire snapshots.\nSee Expire Snapshots.\nUnderstand Partitions and Buckets\r#\rPaimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nFor example, the following table:\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;10\u0026#39; ); The table data will be physically sliced into different partitions, and different buckets inside, so if the overall data volume is too small, there is at least one file in a single bucket, I suggest you configure a smaller number of buckets, otherwise there will be quite a few small files as well.\nUnderstand LSM for Primary Table\r#\rLSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nBy default, sorted runs number depends on num-sorted-run.compaction-trigger, see Compaction for Primary Key Table, this means that there are at least 5 files in a bucket. If you want to reduce this number, you can keep fewer files, but write performance may suffer.\nUnderstand Files for Bucketed Append Table\r#\rBy default, Append also does automatic compaction to reduce the number of small files.\nHowever, for Bucketed Append table, it will only compact the files within the Bucket for sequential purposes, which may keep more small files. See Bucketed Append.\nUnderstand Full-Compaction\r#\rMaybe you think the 5 files for the primary key table are actually okay, but the Append table (bucket) may have 50 small files in a single bucket, which is very difficult to accept. Worse still, partitions that are no longer active also keep so many small files.\nConfigure ‘full-compaction.delta-commits’ perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\n"},{"id":17,"href":"/migration/upsert-to-partitioned/","title":"Upsert To Partitioned","section":"Migration","content":"\rUpsert To Partitioned\r#\r__Note:__ Only Hive Engine can be used to query these upsert-to-partitioned tables.\rThe Tag Management will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nWhen using primary key tables, a non-partitioned approach is often used to maintain updates, in order to mirror and synchronize tables from upstream database tables. This allows users to query the latest data. The tradition of Hive data warehouses is not like this. Offline data warehouses require an immutable view every day to ensure the idempotence of calculations. So we created a Tag mechanism to output these views.\nHowever, the traditional use of Hive data warehouses is more accustomed to using partitions to specify the query\u0026rsquo;s Tag, and is more accustomed to using Hive computing engines.\nSo, we introduce 'metastore.tag-to-partition' and 'metastore.tag-to-partition.preview' to mapping a non-partitioned primary key table to the partition table in Hive metastore, and mapping the partition field to the name of the Tag to be fully compatible with Hive.\nExample for Tag to Partition\r#\rStep 1: Create table and tag in Flink SQL\nFlink\rCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1); Step 2: Query table in Hive with Partition Pruning\nHive\rSHOW PARTITIONS t; /* OK dt=2023-10-16 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-16\u0026#39;; /* OK 1 10 100 2023-10-16 2 20 200 2023-10-16 */ Example for Tag Preview\r#\rThe above example can only query tags that have already been created, but Paimon is a real-time data lake, and you also need to query the latest data. Therefore, Paimon provides a preview feature:\nStep 1: Create table and tag in Flink SQL\nFlink\rCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39;, -- preview tag creation mode process-time -- paimon will create partitions early based on process-time \u0026#39;metastore.tag-to-partition.preview\u0026#39; = \u0026#39;process-time\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1); -- new data in \u0026#39;2023-10-17\u0026#39; INSERT INTO t VALUES (3, \u0026#39;30\u0026#39;, \u0026#39;300\u0026#39;), (4, \u0026#39;40\u0026#39;, \u0026#39;400\u0026#39;); -- haven\u0026#39;t finished writing the data for \u0026#39;2023-10-17\u0026#39; yet, so there\u0026#39;s no need to create a tag for now -- but the data is already visible for Hive Step 2: Query table in Hive with Partition Pruning\nHive\rSHOW PARTITIONS t; /* OK dt=2023-10-16 dt=2023-10-17 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-17\u0026#39;; -- preview tag \u0026#39;2023-10-17\u0026#39; /* OK 1 10 100 2023-10-17 2 20 200 2023-10-17 3 30 300 2023-10-17 4 40 400 2023-10-17 */ "},{"id":18,"href":"/concepts/basic-concepts/","title":"Basic Concepts","section":"Concepts","content":"\rBasic Concepts\r#\rFile Layouts\r#\rAll files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nSnapshot\r#\rAll snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\nthe schema file in use the manifest list containing all changes of this snapshot A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nManifest Files\r#\rAll manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files\r#\rData files are grouped by partitions. Currently, Paimon supports using parquet (default), orc and avro as data file\u0026rsquo;s format.\nPartition\r#\rPaimon adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table.\nConsistency Guarantees\r#\rPaimon writers use two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time. It depends on the incremental write and compaction strategy. If only incremental writes are performed without triggering a compaction operation, only an incremental snapshot will be created. If a compaction operation is triggered, an incremental snapshot and a compacted snapshot will be created.\nFor any two writers modifying a table at the same time, as long as they do not modify the same partition, their commits can occur in parallel. If they modify the same partition, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost. See dedicated compaction job for more info.\n"},{"id":19,"href":"/concepts/rest/bear/","title":"Bear Token","section":"RESTCatalog","content":"\rBear Token\r#\rA bearer token is an encrypted string, typically generated by the server based on a secret key. When the client sends a request to the server, it must include Authorization: Bearer \u0026lt;token\u0026gt; in the request header. After receiving the request, the server extracts the \u0026lt;token\u0026gt; and validates its legitimacy. If the validation passes, the authentication is successful.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;bear\u0026#39; \u0026#39;token\u0026#39; = \u0026#39;\u0026lt;token\u0026gt;\u0026#39; ); "},{"id":20,"href":"/primary-key-table/data-distribution/","title":"Data Distribution","section":"Table with PK","content":"\rData Distribution\r#\rA bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.\nFixed Bucket\r#\rConfigure a bucket greater than 0, using Fixed Bucket mode, according to Math.abs(key_hashcode % numBuckets) to compute the bucket of record.\nRescaling buckets can only be done through offline processes, see Rescale Bucket. A too large number of buckets leads to too many small files, and a too small number of buckets leads to poor write performance.\nDynamic Bucket\r#\rDefault mode for primary key table, or configure 'bucket' = '-1'.\nThe keys that arrive first will fall into the old buckets, and the new keys will fall into the new buckets, the distribution of buckets and keys depends on the order in which the data arrives. Paimon maintains an index to determine which key corresponds to which bucket.\nPaimon will automatically expand the number of buckets.\nOption1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.initial-buckets': controls the number of initialized bucket. Option3: 'dynamic-bucket.max-buckets': controls the number of max buckets. Dynamic Bucket only support single write job. Please do not start multiple jobs to write to the same partition\r(this can lead to duplicate data). Even if you enable `'write-only'` and start a dedicated compaction job, it won't work.\rNormal Dynamic Bucket Mode\r#\rWhen your updates do not cross partitions (no partitions, or primary keys contain all partition fields), Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.\nPerformance:\nGenerally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance. Normal Dynamic Bucket Mode supports sort-compact to speed up queries. See Sort Compact.\nCross Partitions Upsert Dynamic Bucket Mode\r#\rWhen you need cross partition upsert (primary keys not contain all partition fields), Dynamic Bucket mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting stream write job. Different merge engines have different behaviors:\nDeduplicate: Delete data from the old partition and insert new data into the new partition. PartialUpdate \u0026amp; Aggregation: Insert new data into the old partition. FirstRow: Ignore new data if there is old value. Performance: For tables with a large amount of data, there will be a significant loss in performance. Moreover, initialization takes a long time.\nIf your upsert does not rely on too old data, you can consider configuring index TTL to reduce Index and initialization time:\n'cross-partition-upsert.index-ttl': The TTL in rocksdb index and initialization, this can avoid maintaining too many indexes and lead to worse and worse performance. But please note that this may also cause data duplication.\nPostpone Bucket\r#\rPostpone bucket mode is configured by 'bucket' = '-2'. This mode aims to solve the difficulty to determine a fixed number of buckets and support different buckets for different partitions.\nWhen writing records into the table, all records will first be stored in the bucket-postpone directory of each partition and are not available to readers.\nTo move the records into the correct bucket and make them readable, you need to run a compaction job. See compact procedure. The bucket number for the partitions compacted for the first time is configured by the option postpone.default-bucket-num, whose default value is 4.\nFinally, when you feel that the bucket number of some partition is too small, you can also run a rescale job. See rescale procedure.\nPick Partition Fields\r#\rThe following three types of fields may be defined as partition fields in the warehouse:\nCreation Time (Recommended): The creation time is generally immutable, so you can confidently treat it as a partition field and add it to the primary key. Event Time: Event time is a field in the original table. For CDC data, such as tables synchronized from MySQL CDC or Changelogs generated by Paimon, they are all complete CDC data, including UPDATE_BEFORE records, even if you declare the primary key containing partition field, you can achieve the unique effect (require 'changelog-producer'='input'). CDC op_ts: It cannot be defined as a partition field, unable to know previous record timestamp. So you need to use cross partition upsert, it will consume more resources. "},{"id":21,"href":"/project/download/","title":"Download","section":"Project","content":"\rDownload\r#\rThis documentation is a guide for downloading Paimon Jars.\nEngine Jars\r#\rVersion Jar Flink 2.0 paimon-flink-2.0-1.1.1.jar Flink 1.20 paimon-flink-1.20-1.1.1.jar Flink 1.19 paimon-flink-1.19-1.1.1.jar Flink 1.18 paimon-flink-1.18-1.1.1.jar Flink 1.17 paimon-flink-1.17-1.1.1.jar Flink 1.16 paimon-flink-1.16-1.1.1.jar Flink 1.15 paimon-flink-1.15-1.1.1.jar Flink Action paimon-flink-action-1.1.1.jar Spark 3.5 paimon-spark-3.5-1.1.1.jar Spark 3.4 paimon-spark-3.4-1.1.1.jar Spark 3.3 paimon-spark-3.3-1.1.1.jar Spark 3.2 paimon-spark-3.2-1.1.1.jar Hive 3.1 paimon-hive-connector-3.1-1.1.1.jar Hive 2.3 paimon-hive-connector-2.3-1.1.1.jar Hive 2.2 paimon-hive-connector-2.2-1.1.1.jar Hive 2.1 paimon-hive-connector-2.1-1.1.1.jar Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.1.1.jar Trino Download from master Filesystem Jars\r#\rVersion Jar paimon-oss paimon-oss-1.1.1.jar paimon-jindo paimon-jindo-1.1.1.jar paimon-s3 paimon-s3-1.1.1.jar API Jars\r#\rVersion Jar paimon-bundle paimon-bundle-1.1.1.jar "},{"id":22,"href":"/program-api/java-api/","title":"Java API","section":"Program API","content":"\rJava API\r#\rIf possible, recommend using computing engines such as Flink SQL or Spark SQL.\rDependency\r#\rMaven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-bundle\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Bundle.\rPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nCreate Catalog\r#\rBefore coming into contact with the Table, you need to create a Catalog.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static Catalog createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); return CatalogFactory.createCatalog(context); } public static Catalog createHiveCatalog() { // Paimon Hive catalog relies on Hive jars // You should add hive classpath or hive bundled jar. Options options = new Options(); options.set(\u0026#34;warehouse\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;metastore\u0026#34;, \u0026#34;hive\u0026#34;); options.set(\u0026#34;uri\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hive-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hadoop-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); CatalogContext context = CatalogContext.create(options); return CatalogFactory.createCatalog(context); } } Create Table\r#\rYou can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;f0\u0026#34;, \u0026#34;f1\u0026#34;); schemaBuilder.partitionKeys(\u0026#34;f1\u0026#34;); schemaBuilder.column(\u0026#34;f0\u0026#34;, DataTypes.STRING()); schemaBuilder.column(\u0026#34;f1\u0026#34;, DataTypes.INT()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } } } Get Table\r#\rThe Table interface provides access to the table metadata and tools to read and write table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.table.Table; public class GetTable { public static Table getTable() { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); return catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something throw new RuntimeException(\u0026#34;table not exist\u0026#34;); } } } Batch Read\r#\rFor relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.\nBut if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.\nThe reading is divided into two stages:\nScan Plan: Generate plan splits in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;). Read Split: Read split in distributed tasks. import org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class ReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`) // and projection (`withProjection`) if necessary Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;) List\u0026lt;Split\u0026gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks // 4. Read a split in task TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); } } Batch Write\r#\rThe writing is divided into two stages:\nWrite records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;, or named \u0026lsquo;Committer\u0026rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages. import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; import java.util.List; public class BatchWrite { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable) Table table = GetTable.getTable(); BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder().withOverwrite(); // 2. Write records in distributed tasks BatchTableWrite write = writeBuilder.newWrite(); GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector. // WriteSelector determines to which logical downstream writers a record should be written to. // If it returns empty, no data distribution is required. write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files // commit.abort(messages); } } Stream Read\r#\rThe difference of Stream Read is that StreamTableScan can continuously scan and generate splits.\nStreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.\nimport org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class StreamReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`) // and projection (`withProjection`) if necessary Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;) StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List\u0026lt;Split\u0026gt; splits = scan.plan().splits(); // Distribute these splits to different tasks Long state = scan.checkpoint(); // can be restored in scan.restore(state) after fail over // 3. Read a split in task TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); Thread.sleep(1000); } } } Stream Write\r#\rThe difference of Stream Write is that StreamTableCommit can continuously commit.\nKey points to achieve exactly-once consistency:\nCommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterAndCommit to exclude the committed messages by commitIdentifier. import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; import java.util.List; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable) Table table = GetTable.getTable(); StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId long commitIdentifier = 0; while (true) { GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector. // WriteSelector determines to which logical downstream writers a record should be written to. // If it returns empty, no data distribution is required. write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(false, commitIdentifier); commitIdentifier++; // 3. Collect all CommitMessages to a global node and commit StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failure occurs and you\u0026#39;re not sure if the commit process is successful, // you can use `filterAndCommit` to retry the commit process. // Succeeded commits will be automatically skipped. /* Map\u0026lt;Long, List\u0026lt;CommitMessage\u0026gt;\u0026gt; commitIdentifiersAndMessages = new HashMap\u0026lt;\u0026gt;(); commitIdentifiersAndMessages.put(commitIdentifier, messages); commit.filterAndCommit(commitIdentifiersAndMessages); */ Thread.sleep(1000); } } } Data Types\r#\rJava Paimon boolean boolean byte byte short short int int long long float float double double string org.apache.paimon.data.BinaryString decimal org.apache.paimon.data.Decimal timestamp org.apache.paimon.data.Timestamp byte[] byte[] array org.apache.paimon.data.InternalArray map org.apache.paimon.data.InternalMap InternalRow org.apache.paimon.data.InternalRow Predicate Types\r#\rSQL Predicate Paimon Predicate and org.apache.paimon.predicate.PredicateBuilder.And or org.apache.paimon.predicate.PredicateBuilder.Or is null org.apache.paimon.predicate.PredicateBuilder.IsNull is not null org.apache.paimon.predicate.PredicateBuilder.IsNotNull in org.apache.paimon.predicate.PredicateBuilder.In not in org.apache.paimon.predicate.PredicateBuilder.NotIn = org.apache.paimon.predicate.PredicateBuilder.Equal \u0026lt;\u0026gt; org.apache.paimon.predicate.PredicateBuilder.NotEqual \u0026lt; org.apache.paimon.predicate.PredicateBuilder.LessThan \u0026lt;= org.apache.paimon.predicate.PredicateBuilder.LessOrEqual \u0026gt; org.apache.paimon.predicate.PredicateBuilder.GreaterThan \u0026gt;= org.apache.paimon.predicate.PredicateBuilder.GreaterOrEqual "},{"id":23,"href":"/cdc-ingestion/mysql-cdc/","title":"Mysql CDC","section":"CDC Ingestion","content":"\rMySQL CDC\r#\rPaimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar\r#\rDownload CDC Bundled Jar and put them under \u0026lt;FLINK_HOME\u0026gt;/lib/.\nVersion Bundled Jar 3.1.x flink-sql-connector-mysql-cdc-3.1.x.jar mysql-connector-java-8.0.27.jar Only cdc 3.1+ is supported.\rYou can download the `flink-connector-mysql-cdc` jar package by clicking [here](https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-mysql-cdc/).\rSynchronizing Tables\r#\rBy using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_table \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\ [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\ [--metadata_column \u0026lt;metadata-column\u0026gt;] \\ [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table\rThe Paimon table name.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r--computed_column\rThe definitions of computed columns. The argument field is from MySQL table field name. See here for a complete list of configurations. --metadata_column\r--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.\r--mysql_conf\rThe configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rIf the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize tables into one Paimon table\r#\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=\u0026#39;source_db\u0026#39; \\ --mysql_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 As example shows, the mysql_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\r#\rYou can also set \u0026lsquo;database-name\u0026rsquo; with a regular expression to capture multiple databases. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into database \u0026lsquo;source_db1\u0026rsquo;, \u0026lsquo;source_db2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=\u0026#39;source_db.+\u0026#39; \\ --mysql_conf table-name=\u0026#39;source_table\u0026#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronizing Databases\r#\rBy using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ [--ignore_incompatible \u0026lt;true/false\u0026gt;] \\ [--merge_shards \u0026lt;true/false\u0026gt;] \\ [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\ [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\ [--including_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\ [--mode \u0026lt;sync-mode\u0026gt;] \\ [--metadata_column \u0026lt;metadata-column\u0026gt;] \\ [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--ignore_incompatible\rIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.\r--merge_shards\rIt is default true, in this case, if some tables in different databases have the same name, their schemas will be merged and their records will be synchronized into one Paimon table. Otherwise, each table's records will be synchronized to a corresponding Paimon table, and the Paimon table will be named to 'databaseName_tableName' to avoid potential name conflict.\r--table_prefix\rThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".\r--table_suffix\rThe suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".\r--including_tables\rIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.\r--excluding_tables\rIt is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.\r--mode\rIt is used to specify synchronization mode.\nPossible values:\"divided\" (the default mode if you haven't specified one): start a sink for each table, the synchronization of the new table requires restarting the job.\"combined\": start a single combined sink for all tables, the new table will be automatically synchronized.\r--metadata_column\r--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\rIf the keys are not in source table, the sink table won't set partition keys.\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\rIf the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.\rOtherwise, the sink table won't set primary keys.\r--mysql_conf\rThe configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rOnly tables with primary keys will be synchronized.\nFor each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize entire database\r#\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: synchronize newly added tables under database\r#\rLet\u0026rsquo;s say at first a Flink job is synchronizing tables [product, user, address] under database source_db. The command to submit the job looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 \\ --including_tables \u0026#39;product|user|address\u0026#39; At a later point we would like the job to also synchronize tables [order, custom], which contains history data. We can achieve this by recovering from the previous snapshot of the job and thus reusing existing state of the job. The recovered job will first snapshot newly added tables, and then continue reading changelog from previous position automatically.\nThe command to recover from previous snapshot and add new tables to synchronize looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables \u0026#39;product|user|address|order|custom\u0026#39; You can set `--mode combined` to enable synchronizing newly added tables without restarting job.\rExample 3: synchronize and merge multiple shards\r#\rLet\u0026rsquo;s say you have multiple database shards db1, db2, \u0026hellip; and each database has tables tbl1, tbl2, \u0026hellip;. You can synchronize all the db.+.tbl.+ into tables test_db.tbl1, test_db.tbl2 \u0026hellip; by following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mysql_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mysql_conf hostname=127.0.0.1 \\ --mysql_conf username=root \\ --mysql_conf password=123456 \\ --mysql_conf database-name=\u0026#39;db.+\u0026#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 \\ --including_tables \u0026#39;tbl.+\u0026#39; By setting database-name to a regular expression, the synchronization job will capture all tables under matched databases and merge tables of the same name into one table.\nYou can set `--merge_shards false` to prevent merging shards. The synchronized tables will be named to 'databaseName_tableName' to avoid potential name conflict.\rFAQ\r#\rChinese characters in records ingested from MySQL are garbled. Try to set env.java.opts: -Dfile.encoding=UTF-8 in flink-conf.yaml(Flink version \u0026lt; 1.19) or config.yaml(Flink version \u0026gt;= 1.19) (the option is changed to env.java.opts.all since Flink-1.17). Synchronize MySQL table and column comment. Synchronize MySQL create table comment to the paimon table, you need to configure --mysql_conf jdbc.properties.useInformationSchema=true. Synchronize MySQL alter table or column comment to the paimon table, you need to configure --mysql_conf debezium.include.schema.comments=true. "},{"id":24,"href":"/primary-key-table/merge-engine/partial-update/","title":"Partial Update","section":"Merge Engine","content":"\rPartial Update\r#\rBy specifying 'merge-engine' = 'partial-update', users have the ability to update columns of a record through multiple updates until the record is complete. This is achieved by updating the value fields one by one, using the latest data under the same primary key. However, null values are not overwritten in the process.\nFor example, suppose Paimon receives three records:\n\u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt; Assuming that the first column is the primary key, the final result would be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, `partial-update` merge engine must be used together with `lookup` or `full-compaction`\r[changelog producer](https://example.org/primary-key-table/changelog-producer/). ('input' changelog producer is also supported,\rbut only returns input records.)\rBy default, Partial update can not accept delete records, you can choose one of the following solutions:\r- Configure 'ignore-delete' to ignore delete records.\r- Configure 'partial-update.remove-record-on-delete' to remove the whole row when receiving delete records.\r- Configure 'sequence-group's to retract partial columns.\r* Configure 'partial-update.remove-record-on-sequence-group' to remove the whole row when receiving delete records of specified sequence group.\rSequence Group\r#\rA sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update.\nSo we introduce sequence group mechanism for partial-update tables. It can solve:\nDisorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update. See example:\nCREATE TABLE t ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, 1, 1, 1, 1); -- g_2 is null, c, d should not be updated INSERT INTO t VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT)); SELECT * FROM t; -- output 1, 2, 2, 2, 1, 1, 1 -- g_1 is smaller, a, b should not be updated INSERT INTO t VALUES (1, 3, 3, 1, 3, 3, 3); SELECT * FROM t; -- output 1, 2, 2, 2, 3, 3, 3 For fields.\u0026lt;field-name\u0026gt;.sequence-group, valid comparative data types include: DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ.\nYou can also configure multiple sorted fields in a sequence-group, like fields.\u0026lt;field-name1\u0026gt;,\u0026lt;field-name2\u0026gt;.sequence-group, multiple fields will be compared in order.\nSee example:\nCREATE TABLE SG ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2,g_3.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO SG VALUES (1, 1, 1, 1, 1, 1, 1, 1); -- g_2, g_3 should not be updated INSERT INTO SG VALUES (1, 2, 2, 2, 2, 2, 1, CAST(NULL AS INT)); SELECT * FROM SG; -- output 1, 2, 2, 2, 1, 1, 1, 1 -- g_1 should not be updated INSERT INTO SG VALUES (1, 3, 3, 1, 3, 3, 3, 1); SELECT * FROM SG; -- output 1, 2, 2, 2, 3, 3, 3, 1 Aggregation For Partial Update\r#\rYou can specify aggregation function for the input field, all the functions in the Aggregation are supported.\nSee example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.b.aggregate-function\u0026#39; = \u0026#39;first_value\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 1, 2, 3 You can also configure an aggregation function for a sequence-group within multiple sorted fields.\nSee example:\nCREATE TABLE AGG ( k INT, a INT, b INT, g_1 INT, c VARCHAR, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39;, \u0026#39;fields.g_1,g_3.sequence-group\u0026#39; = \u0026#39;a\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c\u0026#39;); -- a in sequence-group g_1, g_3 with sum agg -- b not in sequence-group -- c in sequence-group g_2 without agg INSERT INTO AGG VALUES (1, 1, 1, 1, \u0026#39;1\u0026#39;, 1, 1); -- g_2 should not be updated INSERT INTO AGG VALUES (1, 2, 2, 2, \u0026#39;2\u0026#39;, CAST(NULL AS INT), 2); SELECT * FROM AGG; -- output 1, 3, 2, 2, \u0026#34;1\u0026#34;, 1, 2 -- g_1, g_3 should not be updated INSERT INTO AGG VALUES (1, 3, 3, 2, \u0026#39;3\u0026#39;, 3, 1); SELECT * FROM AGG; -- output 1, 6, 3, 2, \u0026#34;3\u0026#34;, 3, 2 You can specify a default aggregation function for all the input fields with fields.default-aggregate-function, see example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.default-aggregate-function\u0026#39; = \u0026#39;last_non_null_value\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 2, 2, 3 "},{"id":25,"href":"/cdc-ingestion/postgres-cdc/","title":"Postgres CDC","section":"CDC Ingestion","content":"\rPostgres CDC\r#\rPaimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar\r#\rflink-connector-postgres-cdc-*.jar Synchronizing Tables\r#\rBy using PostgresSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from PostgreSQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ postgres_sync_table \\ --warehouse \u0026lt;warehouse_path\u0026gt; \\ --database \u0026lt;database_name\u0026gt; \\ --table \u0026lt;table_name\u0026gt; \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary_keys\u0026gt;] \\ [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\ [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\ [--metadata_column \u0026lt;metadata_column\u0026gt;] \\ [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table\rThe Paimon table name.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\r--type_mapping\rIt is used to specify how to map PostgreSQL data type to Paimon type.\nSupported options:\r\"to-string\": maps all PostgreSQL types to STRING.\r--computed_column\rThe definitions of computed columns. The argument field is from PostgreSQL table field name. See here for a complete list of configurations. --metadata_column\r--metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,schema_name,op_ts. See its document for a complete list of available metadata.\r--postgres_conf\rThe configuration for Flink CDC Postgres sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name, schema-name, table-name and slot.name are required configurations, others are optional. See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rIf the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified PostgreSQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified PostgreSQL tables.\nExample 1: synchronize tables into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ postgres_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --postgres_conf hostname=127.0.0.1 \\ --postgres_conf username=root \\ --postgres_conf password=123456 \\ --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\ --postgres_conf schema-name=\u0026#39;public\u0026#39; \\ --postgres_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\ --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 As example shows, the postgres_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\nYou can also set \u0026lsquo;schema-name\u0026rsquo; with a regular expression to capture multiple schemas. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into schema \u0026lsquo;source_schema1\u0026rsquo;, \u0026lsquo;source_schema2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ postgres_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --postgres_conf hostname=127.0.0.1 \\ --postgres_conf username=root \\ --postgres_conf password=123456 \\ --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\ --postgres_conf schema-name=\u0026#39;source_schema.+\u0026#39; \\ --postgres_conf table-name=\u0026#39;source_table\u0026#39; \\ --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 "},{"id":26,"href":"/concepts/spec/schema/","title":"Schema","section":"Specification","content":"\rSchema\r#\rThe version of the schema file starts from 0 and currently retains all versions of the schema. There may be old files that rely on the old schema version, so its deletion should be done with caution.\nSchema File is JSON, it includes:\nfields: data field list, data field contains id, name, type, field id is used to support schema evolution. partitionKeys: field name list, partition definition of the table, it cannot be modified. primaryKeys: field name list, primary key definition of the table, it cannot be modified. options: map\u0026lt;string, string\u0026gt;, no ordered, options of the table, including a lot of capabilities and optimizations. Example\r#\r{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ { \u0026#34;id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT NOT NULL\u0026#34; }, { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;order_name\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;STRING\u0026#34; }, { \u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;order_user_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; }, { \u0026#34;id\u0026#34; : 3, \u0026#34;name\u0026#34; : \u0026#34;order_shop_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; } ], \u0026#34;highestFieldId\u0026#34; : 3, \u0026#34;partitionKeys\u0026#34; : [ ], \u0026#34;primaryKeys\u0026#34; : [ \u0026#34;order_id\u0026#34; ], \u0026#34;options\u0026#34; : { \u0026#34;bucket\u0026#34; : \u0026#34;5\u0026#34; }, \u0026#34;comment\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1720496663041 } Compatibility\r#\rFor old versions:\nversion 1: should put bucket -\u0026gt; 1 to options if there is no bucket key. version 1 \u0026amp; 2: should put file.format -\u0026gt; orc to options if there is no file.format key. DataField\r#\rDataField represents a column of the table.\nid: int, column id, automatic increment, it is used for schema evolution. name: string, column name. type: data type, it is very similar to SQL type string. description: string. Update Schema\r#\rUpdating the schema should generate a new schema file.\nwarehouse └── default.db └── my_table ├── schema ├── schema-0 ├── schema-1 └── schema-2 There is a reference to schema in the snapshot. The schema file with the highest numerical value is usually the latest schema file.\nOld schema files cannot be directly deleted because there may be old data files that reference old schema files. When reading table, it is necessary to rely on them for schema evolution reading.\n"},{"id":27,"href":"/flink/sql-ddl/","title":"SQL DDL","section":"Engine Flink","content":"\rSQL DDL\r#\rCreate Catalog\r#\rPaimon catalogs currently support three types of metastores:\nfilesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog\r#\rThe following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.\nCreating Hive Catalog\r#\rBy using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nPaimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Hive connector bundled jar and add it to classpath.\nMetastore version Bundle Name SQL Client JAR 2.3.0 - 3.1.3 Flink Bundle Download 1.2.0 - x.x.x Presto Bundle Download The following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nIf your Hive requires security authentication such as Kerberos, LDAP, Ranger or you want the paimon table to be managed by Apache Atlas(Setting \u0026lsquo;hive.metastore.event.listeners\u0026rsquo; in hive-site.xml). You can specify the hive-conf-dir and hadoop-conf-dir parameter to the hive-site.xml file path.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.\nAlso, you can create FlinkGenericCatalog.\nWhen using hive catalog to change incompatible column types through alter table, you need to configure hive.metastore.disallow.incompatible.col.type.changes=false. see HIVE-17832.\nIf you are using Hive3, please disable Hive ACID:\nhive.strict.managed.tables=false hive.create.as.insert.only=false metastore.create.as.acid=false Synchronizing Partitions into Hive Metastore\r#\rBy default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nAdding Parameters to a Hive Table\r#\rUsing the table option facilitates the convenient definition of Hive table parameters. Parameters prefixed with hive. will be automatically defined in the TBLPROPERTIES of the Hive table. For instance, using the option hive.table.owner=Jon will automatically add the parameter table.owner=Jon to the table properties during the creation process.\nSetting Location in Properties\r#\rIf you are using an object storage , and you don\u0026rsquo;t want that the location of paimon table/database is accessed by the filesystem of hive, which may lead to the error such as \u0026ldquo;No FileSystem for scheme: s3a\u0026rdquo;. You can set location in the properties of table/database by the config of location-in-properties. See setting the location of table/database in properties Creating JDBC Catalog\r#\rBy using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Flink needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\ndatabase type Bundle Name SQL Client JAR mysql mysql-connector-java Download postgres postgresql Download CREATE CATALOG my_jdbc WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt;\u0026#39;, \u0026#39;jdbc.user\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;jdbc.password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;catalog-key\u0026#39;=\u0026#39;jdbc\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_jdbc; You can configure any connection parameters that have been declared by JDBC through \u0026ldquo;jdbc.\u0026rdquo;, the connection parameters may be different between different databases, please configure according to the actual situation.\nYou can also perform logical isolation for databases under multiple catalogs by specifying \u0026ldquo;catalog-key\u0026rdquo;.\nAdditionally, when creating a JdbcCatalog, you can specify the maximum length for the lock key by configuring \u0026ldquo;lock-key-max-length,\u0026rdquo; which defaults to 255. Since this value is a combination of {catalog-key}.{database-name}.{table-name}, please adjust accordingly.\nYou can define any default table options with the prefix table-default. for tables created in the catalog.\nCreate Table\r#\rAfter use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); If you need cross partition upsert (primary keys not contain all partition fields), see [Cross partition Upsert](https://example.org/primary-key-table/data-distribution/#cross-partitions-upsert-dynamic-bucket-mode) mode.\rBy configuring [partition.expiration-time](https://example.org/maintenance/manage-partitions/), expired partitions can be automatically deleted.\rSpecify Statistics Mode\r#\rPaimon will automatically collect the statistics of the data file for speeding up the query process. There are four modes supported:\nfull: collect the full metrics: null_count, min, max . truncate(length): length can be any positive number, the default mode is truncate(16), which means collect the null count, min/max value with truncated length of 16. This is mainly to avoid too big column which will enlarge the manifest file. counts: only collect the null count. none: disable the metadata stats collection. The statistics collector mode can be configured by 'metadata.stats-mode', by default is 'truncate(16)'. You can configure the field level by setting 'fields.{field_name}.stats-mode'.\nFor the stats mode of none, by default metadata.stats-dense-store is true, which will significantly reduce the storage size of the manifest. But the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.\nField Default Value\r#\rPaimon table currently supports setting default values for fields in table properties by 'fields.item_id.default-value', note that partition fields and primary key fields can not be specified.\nCreate Table As Select\r#\rTable can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\n/* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table */ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_partition; /* change options */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_pk_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_all_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_all; Create Table Like\r#\rTo create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_like LIKE my_table (EXCLUDING OPTIONS); Work with Flink Temporary Tables\r#\rFlink Temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs:///path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k; "},{"id":28,"href":"/spark/sql-ddl/","title":"SQL DDL","section":"Engine Spark","content":"\rSQL DDL\r#\rCatalog\r#\rCreate Catalog\r#\rPaimon catalogs currently support three types of metastores:\nfilesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog\r#\rThe following Spark SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nThe following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse.\nspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Creating Hive Catalog\r#\rBy using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nYour Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.\nThe following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\ --conf spark.sql.catalog.paimon.metastore=hive \\ --conf spark.sql.catalog.paimon.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Also, you can create SparkGenericCatalog.\nSynchronizing Partitions into Hive Metastore\nBy default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nCreating JDBC Catalog\r#\rBy using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Spark needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\ndatabase type Bundle Name SQL Client JAR mysql mysql-connector-java Download postgres postgresql Download spark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\ --conf spark.sql.catalog.paimon.metastore=jdbc \\ --conf spark.sql.catalog.paimon.uri=jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt; \\ --conf spark.sql.catalog.paimon.jdbc.user=... \\ --conf spark.sql.catalog.paimon.jdbc.password=... USE paimon.default; Creating REST Catalog\r#\rBy using the Paimon REST catalog, changes to the catalog will be directly stored in remote server.\nbear token\r#\rspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\ --conf spark.sql.catalog.paimon.token.provider=bear \\ --conf spark.sql.catalog.paimon.token=\u0026lt;token\u0026gt; dlf ak\r#\rspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\ --conf spark.sql.catalog.paimon.token.provider=dlf \\ --conf spark.sql.catalog.paimon.dlf.access-key-id=\u0026lt;access-key-id\u0026gt; \\ --conf spark.sql.catalog.paimon.dlf.access-key-secret=\u0026lt;security-token\u0026gt; dlf sts token\r#\rspark-sql ... \\ --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon.metastore=rest \\ --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\ --conf spark.sql.catalog.paimon.token.provider=dlf \\ --conf spark.sql.catalog.paimon.dlf.access-key-id=\u0026lt;access-key-id\u0026gt; \\ --conf spark.sql.catalog.paimon.dlf.access-key-secret=\u0026lt;access-key-secret\u0026gt; \\ --conf spark.sql.catalog.paimon.dlf.security-token=\u0026lt;security-token\u0026gt; USE paimon.default; Table\r#\rCreate Table\r#\rAfter use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Create External Table\r#\rWhen the catalog\u0026rsquo;s metastore type is hive, if the location is specified when creating a table, that table will be considered an external table; otherwise, it will be a managed table.\nWhen you drop an external table, only the metadata in Hive will be removed, and the actual data files will not be deleted; whereas dropping a managed table will also delete the data.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ) LOCATION \u0026#39;/path/to/table\u0026#39;; Furthermore, if there is already data stored in the specified location, you can create the table without explicitly specifying the fields, partitions and props or other information. In this case, the new table will inherit them all from the existing table’s metadata.\nHowever, if you manually specify them, you need to ensure that they are consistent with those of the existing table (props can be a subset). Therefore, it is strongly recommended not to specify them.\nCREATE TABLE my_table LOCATION \u0026#39;/path/to/table\u0026#39;; Create Table As Select\r#\rTable can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table*/ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as PARTITIONED BY (dt) AS SELECT * FROM my_table_partition; /* change TBLPROPERTIES */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_pk_as TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_all_as PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_all; View\r#\rViews are based on the result-set of an SQL query, when using org.apache.paimon.spark.SparkCatalog, views are managed by paimon itself. And in this case, views are supported when the metastore type is hive, and temporary views are not supported yet.\nCreate Or Replace View\r#\rCREATE VIEW constructs a virtual table that has no physical data.\n-- create a view. CREATE VIEW v1 AS SELECT * FROM t1; -- create a view, if a view of same name already exists, it will be replaced. CREATE OR REPLACE VIEW v1 AS SELECT * FROM t1; Drop View\r#\rDROP VIEW removes the metadata associated with a specified view from the catalog.\n-- drop a view DROP VIEW v1; Tag\r#\rCreate Or Replace Tag\r#\rCreate or replace a tag syntax with the following options.\nCreate a tag with or without the snapshot id and time retention. Create an existed tag is not failed if using IF NOT EXISTS syntax. Update a tag using REPLACE TAG or CREATE OR REPLACE TAG syntax. -- create a tag based on the latest snapshot and no retention. ALTER TABLE T CREATE TAG `TAG-1`; -- create a tag based on the latest snapshot and no retention if it doesn\u0026#39;t exist. ALTER TABLE T CREATE TAG IF NOT EXISTS `TAG-1`; -- create a tag based on the latest snapshot and retain it for 7 day. ALTER TABLE T CREATE TAG `TAG-2` RETAIN 7 DAYS; -- create a tag based on snapshot-1 and no retention. ALTER TABLE T CREATE TAG `TAG-3` AS OF VERSION 1; -- create a tag based on snapshot-2 and retain it for 12 hour. ALTER TABLE T CREATE TAG `TAG-4` AS OF VERSION 2 RETAIN 12 HOURS; -- replace a existed tag with new snapshot id and new retention ALTER TABLE T REPLACE TAG `TAG-4` AS OF VERSION 2 RETAIN 24 HOURS; -- create or replace a tag, create tag if it not exist, replace tag if it exists. ALTER TABLE T CREATE OR REPLACE TAG `TAG-5` AS OF VERSION 2 RETAIN 24 HOURS; Delete Tag\r#\rDelete a tag or multiple tags of a table.\n-- delete a tag. ALTER TABLE T DELETE TAG `TAG-1`; -- delete a tag if it exists. ALTER TABLE T DELETE TAG IF EXISTS `TAG-1` -- delete multiple tags, delimiter is \u0026#39;,\u0026#39;. ALTER TABLE T DELETE TAG `TAG-1,TAG-2`; Rename Tag\r#\rRename an existing tag with a new tag name.\nALTER TABLE T RENAME TAG `TAG-1` TO `TAG-2`; Show Tags\r#\rList all tags of a table.\nSHOW TAGS T; "},{"id":29,"href":"/spark/sql-functions/","title":"SQL Functions","section":"Engine Spark","content":"\rSQL Functions\r#\rThis section introduce all available Paimon Spark functions.\nmax_pt\r#\rmax_pt($table_name)\nIt accepts a string type literal to specify the table name and return a max-valid-toplevel partition value.\nvalid: the partition which contains data files toplevel: only return the first partition value if the table has multi-partition columns It would throw exception when:\nthe table is not a partitioned table the partitioned table does not have partition all of the partitions do not contains data files Example\n\u0026gt; SELECT max_pt(\u0026#39;t\u0026#39;); 20250101 \u0026gt; SELECT * FROM t where pt = max_pt(\u0026#39;t\u0026#39;); a, 20250101 Since: 1.1.0\n"},{"id":30,"href":"/flink/sql-write/","title":"SQL Write","section":"Engine Flink","content":"\rSQL Write\r#\rSyntax\r#\rINSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:\nFlink INSERT Statement\nINSERT INTO\r#\rUse INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... INSERT INTO supports both batch and streaming mode. In Streaming mode, by default, it will also perform compaction, snapshot expiration, and even partition expiration in Flink Sink (if it is configured).\nFor multiple jobs to write the same table, you can refer to dedicated compaction job for more info.\nClustering\r#\rIn Paimon, clustering is a feature that allows you to cluster data in your Append Table based on the values of certain columns during the write process. This organization of data can significantly enhance the efficiency of downstream tasks when reading the data, as it enables faster and more targeted data retrieval. This feature is only supported for Append Table(bucket = -1) and batch execution mode.\nTo utilize clustering, you can specify the columns you want to cluster when creating or writing to a table. Here\u0026rsquo;s a simple example of how to enable clustering:\nCREATE TABLE my_table ( a STRING, b STRING, c STRING, ) WITH ( \u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;, ); You can also use SQL hints to dynamically set clustering options:\nINSERT INTO my_table /*+ OPTIONS(\u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;) */ SELECT * FROM source; The data is clustered using an automatically chosen strategy (such as ORDER, ZORDER, or HILBERT), but you can manually specify the clustering strategy by setting the sink.clustering.strategy. Clustering relies on sampling and sorting. If the clustering process takes too much time, you can decrease the total sample number by setting the sink.clustering.sample-factor or disable the sorting step by setting the sink.clustering.sort-in-cluster to false.\nYou can refer to FlinkConnectorOptions for more info about the configurations above.\nOverwriting the Whole Table\r#\rFor unpartitioned tables, Paimon supports overwriting the whole table. (or for partitioned table which disables dynamic-partition-overwrite option).\nUse INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE my_table SELECT ... Overwriting a Partition\r#\rFor partitioned tables, Paimon supports overwriting a partition.\nUse INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite\r#\rFlink\u0026rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.\n-- MyTable is a Partitioned Table -- Dynamic overwrite INSERT OVERWRITE my_table SELECT ... -- Static overwrite (Overwrite whole table) INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39; = \u0026#39;false\u0026#39;) */ SELECT ... Truncate tables\r#\rFlink 1.17-\rYou can use INSERT OVERWRITE to purge tables by inserting empty value.\nINSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ SELECT * FROM my_table WHERE false; Flink 1.18\u0026#43;\rTRUNCATE TABLE my_table; Purging Partitions\r#\rCurrently, Paimon supports two ways to purge partitions.\nLike purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\nMethod #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop_partition job through flink run.\n-- Syntax INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM my_table WHERE false; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0) SELECT k1, v FROM my_table WHERE false; -- or INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0, k1 = 0) SELECT v FROM my_table WHERE false; Updating tables\r#\rImportant table properties setting:\r1. Only [primary key table](https://example.org/primary-key-table/overview/) supports this feature.\r2. [MergeEngine](https://example.org/primary-key-table/merge-engine/) needs to be [deduplicate](https://example.org/primary-key-table/merge-engine/overview/#deduplicate)\ror [partial-update](https://example.org/primary-key-table/merge-engine/partial-update/) to support this feature.\r3. Do not support updating primary keys.\rCurrently, Paimon supports updating records by using UPDATE in Flink 1.17 and later versions. You can perform UPDATE in Flink\u0026rsquo;s batch mode.\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( a STRING, b INT, c INT, PRIMARY KEY (a) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE my_table SET b = 1, c = 2 WHERE a = \u0026#39;myTable\u0026#39;; Deleting from table\r#\rFlink 1.17\u0026#43;\rImportant table properties setting:\nOnly primary key tables support this feature. If the table has primary keys, the following MergeEngine support this feature: deduplicate. partial-update with option \u0026lsquo;partial-update.remove-record-on-delete\u0026rsquo; enabled. Do not support deleting from table in streaming mode. -- Syntax DELETE FROM table_identifier WHERE conditions; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( id BIGINT NOT NULL, currency STRING, rate BIGINT, dt String, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use DELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Partition Mark Done\r#\rFor partitioned tables, each partition may need to be scheduled to trigger downstream batch computation. Therefore, it is necessary to choose this timing to indicate that it is ready for scheduling and to minimize the amount of data drift during scheduling. We call this process: \u0026ldquo;Partition Mark Done\u0026rdquo;.\nExample to mark done:\nCREATE TABLE my_partitioned_table ( f0 INT, f1 INT, f2 INT, ... dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;partition.timestamp-formatter\u0026#39;=\u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39;=\u0026#39;$dt\u0026#39;, \u0026#39;partition.time-interval\u0026#39;=\u0026#39;1 d\u0026#39;, \u0026#39;partition.idle-time-to-done\u0026#39;=\u0026#39;15 m\u0026#39;, \u0026#39;partition.mark-done-action\u0026#39;=\u0026#39;done-partition\u0026#39; ); You can also customize a PartitionMarkDoneAction to mark the partition completed.\npartition.mark-done-action: custom partition.mark-done-action.custom.class: The partition mark done class for implement PartitionMarkDoneAction interface (e.g. org.apache.paimon.CustomPartitionMarkDoneAction). Define a class CustomPartitionMarkDoneAction to implement the PartitionMarkDoneAction interface.\npackage org.apache.paimon; public class CustomPartitionMarkDoneAction implements PartitionMarkDoneAction { @Override public void markDone(String partition) { // do something. } @Override public void close() {} } Paimon also support http-report partition mark done action, this action will report the partition to the remote http server.\npartition.mark-done-action: http-report partition.mark-done-action.http.url : Action will report the partition to the remote http server. partition.mark-done-action.http.timeout : Http client connection timeout and default is 5s. partition.mark-done-action.http.params : Http client request params in the request body json. Http Post request body :\n{ \u0026#34;table\u0026#34;: \u0026#34;table fullName\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;table location path\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;mark done partition\u0026#34;, \u0026#34;params\u0026#34; : \u0026#34;custom params\u0026#34; } Http Response body :\n{ \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; } Firstly, you need to define the time parser of the partition and the time interval between partitions in order to determine when the partition can be properly marked done. Secondly, you need to define idle-time, which determines how long it takes for the partition to have no new data, and then it will be marked as done. Thirdly, by default, partition mark done will create _SUCCESS file, the content of _SUCCESS file is a json, contains creationTime and modificationTime, they can help you understand if there is any delayed data. You can also configure other actions, like 'done-partition', for example, partition 'dt=20240501' with produce 'dt=20240501.done' done partition. "},{"id":31,"href":"/spark/sql-write/","title":"SQL Write","section":"Engine Spark","content":"\rSQL Write\r#\rInsert Table\r#\rThe INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.\nSyntax\nINSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; Parameters\ntable_identifier: Specifies a table name, which may be optionally qualified with a database name.\npart_spec: An optional parameter that specifies a comma-separated list of key and value pairs for partitions.\ncolumn_list: An optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table. Spark will reorder the columns of the input query to match the table schema according to the specified column list.\nNote: Since Spark 3.4, INSERT INTO commands with explicit column lists comprising fewer columns than the target table will automatically add the corresponding default values for the remaining columns (or NULL for any column lacking an explicitly-assigned default value). In Spark 3.3 or earlier, column_list\u0026rsquo;s size must be equal to the target table\u0026rsquo;s column size, otherwise these commands would have failed.\nvalue_expr ( { value | NULL } [ , … ] ) [ , ( … ) ]: Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.\nFor more information, please check the syntax document: Spark INSERT Statement\nInsert Into\r#\rUse INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... Insert Overwrite\r#\rUse INSERT OVERWRITE to overwrite the whole table.\nINSERT OVERWRITE my_table SELECT ... Insert Overwrite Partition\r#\rUse INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite Partition\r#\rSpark\u0026rsquo;s default overwrite mode is static partition overwrite. To enable dynamic overwritten you need to set the Spark session configuration spark.sql.sources.partitionOverwriteMode to dynamic\nFor example:\nCREATE TABLE my_table (id INT, pt STRING) PARTITIONED BY (pt); INSERT INTO my_table VALUES (1, \u0026#39;p1\u0026#39;), (2, \u0026#39;p2\u0026#39;); -- Static overwrite (Overwrite the whole table) INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); -- or INSERT OVERWRITE my_table PARTITION (pt) VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 3| p1| +---+---+ */ -- Static overwrite with specified partitions (Only overwrite pt=\u0026#39;p1\u0026#39;) INSERT OVERWRITE my_table PARTITION (pt=\u0026#39;p1\u0026#39;) VALUES (3); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ -- Dynamic overwrite (Only overwrite pt=\u0026#39;p1\u0026#39;) SET spark.sql.sources.partitionOverwriteMode=dynamic; INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ Truncate Table\r#\rThe TRUNCATE TABLE statement removes all the rows from a table or partition(s).\nTRUNCATE TABLE my_table; Update Table\r#\rUpdates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.\nNote:\nUpdate primary key columns is not supported when the target table is a primary key table.\rSpark supports update PrimitiveType and StructType, for example:\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; CREATE TABLE t ( id INT, s STRUCT\u0026lt;c1: INT, c2: STRING\u0026gt;, name STRING) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;id\u0026#39;, \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE t SET name = \u0026#39;a_new\u0026#39; WHERE id = 1; UPDATE t SET s.c2 = \u0026#39;a_new\u0026#39; WHERE s.c1 = 1; Delete From Table\r#\rDeletes the rows that match a predicate. When no predicate is provided, deletes all rows.\nDELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Merge Into Table\r#\rMerges a set of updates, insertions and deletions based on a source table into a target table.\nNote:\nIn update clause, to update primary key columns is not supported when the target table is a primary key table.\rExample: One\nThis is a simple demo that, if a row exists in the target table update it, else insert it.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key. MERGE INTO target USING source ON target.a = source.a WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT * Example: Two\nThis is a demo with multiple, conditional clauses.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key. MERGE INTO target USING source ON target.a = source.a WHEN MATCHED AND target.a = 5 THEN UPDATE SET b = source.b + target.b -- when matched and meet the condition 1, then update b; WHEN MATCHED AND source.c \u0026gt; \u0026#39;c2\u0026#39; THEN UPDATE SET * -- when matched and meet the condition 2, then update all the columns; WHEN MATCHED THEN DELETE -- when matched, delete this row in target table; WHEN NOT MATCHED AND c \u0026gt; \u0026#39;c9\u0026#39; THEN INSERT (a, b, c) VALUES (a, b * 1.1, c) -- when not matched but meet the condition 3, then transform and insert this row; WHEN NOT MATCHED THEN INSERT * -- when not matched, insert this row without any transformation; Streaming Write\r#\rPaimon Structured Streaming only supports the two `append` and `complete` modes.\r// Create a paimon table if not exists. spark.sql(s\u0026#34;\u0026#34;\u0026#34; |CREATE TABLE T (k INT, v STRING) |TBLPROPERTIES (\u0026#39;primary-key\u0026#39;=\u0026#39;k\u0026#39;, \u0026#39;bucket\u0026#39;=\u0026#39;3\u0026#39;) |\u0026#34;\u0026#34;\u0026#34;.stripMargin) // Here we use MemoryStream to fake a streaming source. val inputData = MemoryStream[(Int, String)] val df = inputData.toDS().toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) // Streaming Write to paimon table. val stream = df .writeStream .outputMode(\u0026#34;append\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .format(\u0026#34;paimon\u0026#34;) .start(\u0026#34;/path/to/paimon/sink/table\u0026#34;) Schema Evolution\r#\rSchema evolution is a feature that allows users to easily modify the current schema of a table to adapt to existing data, or new data that changes over time, while maintaining data integrity and consistency.\nPaimon supports automatic schema merging of source data and current table data while data is being written, and uses the merged schema as the latest schema of the table, and it only requires configuring write.merge-schema.\ndata.write .format(\u0026#34;paimon\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .save(location) When enable write.merge-schema, Paimon can allow users to perform the following actions on table schema by default:\nAdding columns Up-casting the type of column(e.g. Int -\u0026gt; Long) Paimon also supports explicit type conversions between certain types (e.g. String -\u0026gt; Date, Long -\u0026gt; Int), it requires an explicit configuration write.merge-schema.explicit-cast.\nSchema evolution can be used in streaming mode at the same time.\nval inputData = MemoryStream[(Int, String)] inputData .toDS() .toDF(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;) .writeStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;write.merge-schema.explicit-cast\u0026#34;, \u0026#34;true\u0026#34;) .start(location) Here list the configurations.\nScan Mode\rDescription\rwrite.merge-schema\rIf true, merge the data schema and the table schema automatically before write data.\rwrite.merge-schema.explicit-cast\rIf true, allow to merge data types if the two types meet the rules for explicit casting.\r"},{"id":32,"href":"/ecosystem/starrocks/","title":"StarRocks","section":"Ecosystem","content":"\rStarRocks\r#\rThis documentation is a guide for using Paimon in StarRocks.\nVersion\r#\rPaimon currently supports StarRocks 3.1 and above. Recommended version is StarRocks 3.2.6 or above.\nCreate Paimon Catalog\r#\rPaimon catalogs are registered by executing a CREATE EXTERNAL CATALOG SQL in StarRocks. For example, you can use the following SQL to create a Paimon catalog named paimon_catalog.\nCREATE EXTERNAL CATALOG paimon_catalog PROPERTIES( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;filesystem\u0026#34;, \u0026#34;paimon.catalog.warehouse\u0026#34; = \u0026#34;oss://\u0026lt;your_bucket\u0026gt;/user/warehouse/\u0026#34; ); More catalog types and configures can be seen in Paimon catalog.\nQuery\r#\rSuppose there already exists a database named test_db and a table named test_tbl in paimon_catalog, you can query this table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.test_tbl; Query System Tables\r#\rYou can access all kinds of Paimon system tables by StarRocks. For example, you can read the ro (read-optimized) system table to improve reading performance of primary-key tables.\nSELECT * FROM paimon_catalog.test_db.test_tbl$ro; For another example, you can query partition files of the table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.partition_tbl$partitions; /* +-----------+--------------+--------------------+------------+----------------------------+ | partition | record_count | file_size_in_bytes | file_count | last_update_time | +-----------+--------------+--------------------+------------+----------------------------+ | [1] | 1 | 645 | 1 | 2024-01-01 00:00:00.000000 | +-----------+--------------+--------------------+------------+----------------------------+ */ StarRocks to Paimon type mapping\r#\rThis section lists all supported type conversion between StarRocks and Paimon. All StarRocks’s data types can be found in this doc StarRocks Data type overview.\nStarRocks Data Type\rPaimon Data Type\rAtomic Type\rSTRUCT\rRowType\rfalse\rMAP\rMapType\rfalse\rARRAY\rArrayType\rfalse\rBOOLEAN\rBooleanType\rtrue\rTINYINT\rTinyIntType\rtrue\rSMALLINT\rSmallIntType\rtrue\rINT\rIntType\rtrue\rBIGINT\rBigIntType\rtrue\rFLOAT\rFloatType\rtrue\rDOUBLE\rDoubleType\rtrue\rCHAR(length)\rCharType(length)\rtrue\rVARCHAR(MAX_VARCHAR_LENGTH)\rVarCharType(VarCharType.MAX_LENGTH)\rtrue\rVARCHAR(length)\rVarCharType(length), length is less than VarCharType.MAX_LENGTH\rtrue\rDATE\rDateType\rtrue\rDATETIME\rTimestampType\rtrue\rDECIMAL(precision, scale)\rDecimalType(precision, scale)\rtrue\rVARBINARY(length)\rVarBinaryType(length)\rtrue\rDATETIME\rLocalZonedTimestampType\rtrue\r"},{"id":33,"href":"/append-table/streaming/","title":"Streaming","section":"Table w/o PK","content":"\rStreaming\r#\rYou can stream write to the Append table in a very flexible way through Flink, or read the Append table through Flink, using it like a queue. The only difference is that its latency is in minutes. Its advantages are very low cost and the ability to push down filters and projection.\nPre small files merging\r#\r\u0026ldquo;Pre\u0026rdquo; means that this compact occurs before committing files to the snapshot.\nIf Flink\u0026rsquo;s checkpoint interval is short (for example, 30 seconds), each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.\nIn order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.\nPost small files merging\r#\r\u0026ldquo;Post\u0026rdquo; means that this compact occurs after committing files to the snapshot.\nIn streaming write job, without bucket definition, there is no compaction in writer, instead, will use Compact Coordinator to scan the small files and pass compaction task to Compact Worker. In streaming mode, if you run insert sql in flink, the topology will be like this:\nDo not worry about backpressure, compaction never backpressure.\nIf you set write-only to true, the Compact Coordinator and Compact Worker will be removed in the topology.\nThe auto compaction is only supported in Flink engine streaming mode. You can also start a compaction job in Flink by Flink action in Paimon and disable all the other compactions by setting write-only.\nStreaming Query\r#\rYou can stream the Append table and use it like a Message Queue. As with primary key tables, there are two options for streaming reads:\nBy default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest incremental records. You can specify scan.mode, scan.snapshot-id, scan.timestamp-millis and/or scan.file-creation-time-millis to stream read incremental only. Similar to flink-kafka, order is not guaranteed by default, if your data has some sort of order requirement, you also need to consider defining a bucket-key, see Bucketed Append\n"},{"id":34,"href":"/primary-key-table/","title":"Table with PK","section":"Apache Paimon","content":"\r"},{"id":35,"href":"/primary-key-table/merge-engine/aggregation/","title":"Aggregation","section":"Merge Engine","content":"\rAggregation\r#\rNOTE: Always set `table.exec.sink.upsert-materialize` to `NONE` in Flink SQL TableConfig.\rSometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink\rCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nAggregation Functions\r#\rCurrent supported aggregate functions and data types are:\nsum\r#\rThe sum function aggregates the values across multiple rows. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\nproduct\r#\rThe product function can compute product values across multiple lines. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\ncount\r#\rIn scenarios where counting rows that match a specific condition is required, you can use the SUM function to achieve this. By expressing a condition as a Boolean value (TRUE or FALSE) and converting it into a numerical value, you can effectively count the rows. In this approach, TRUE is converted to 1, and FALSE is converted to 0.\nFor example, if you have a table orders and want to count the number of rows that meet a specific condition, you can use the following query:\nSELECT SUM(CASE WHEN condition THEN 1 ELSE 0 END) AS count FROM orders; max\r#\rThe max function identifies and retains the maximum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nmin\r#\rThe min function identifies and retains the minimum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nlast_value\r#\rThe last_value function replaces the previous value with the most recently imported value. It supports all data types.\nlast_non_null_value\r#\rThe last_non_null_value function replaces the previous value with the latest non-null value. It supports all data types.\nlistagg\r#\rThe listagg function concatenates multiple string values into a single string. It supports STRING data type. Each field not part of the primary keys can be given a list agg delimiter, specified by the fields..list-agg-delimiter table property, otherwise it will use \u0026ldquo;,\u0026rdquo; as default.\nbool_and\r#\rThe bool_and function evaluates whether all values in a boolean set are true. It supports BOOLEAN data type.\nbool_or\r#\rThe bool_or function checks if at least one value in a boolean set is true. It supports BOOLEAN data type.\nfirst_value\r#\rThe first_value function retrieves the first null value from a data set. It supports all data types.\nfirst_non_null_value\r#\rThe first_non_null_value function selects the first non-null value in a data set. It supports all data types.\nrbm32\r#\rThe rbm32 function aggregates multiple serialized 32-bit RoaringBitmap into a single RoaringBitmap. It supports VARBINARY data type.\nrbm64\r#\rThe rbm64 function aggregates multiple serialized 64-bit Roaring64Bitmap into a single Roaring64Bitmap. It supports VARBINARY data type.\nnested_update\r#\rThe nested_update function collects multiple rows into one array (so-called \u0026rsquo;nested table\u0026rsquo;). It supports ARRAY data types.\nUse fields.\u0026lt;field-name\u0026gt;.nested-key=pk0,pk1,... to specify the primary keys of the nested table. If no keys, row will be appended to array.\nAn example:\nFlink\r-- orders table CREATE TABLE orders ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING ); -- sub orders that have the same order_id -- belongs to the same order CREATE TABLE sub_orders ( order_id BIGINT, sub_order_id INT, product_name STRING, price BIGINT, PRIMARY KEY (order_id, sub_order_id) NOT ENFORCED ); -- wide table CREATE TABLE order_wide ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING, sub_orders ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt; ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.sub_orders.aggregate-function\u0026#39; = \u0026#39;nested_update\u0026#39;, \u0026#39;fields.sub_orders.nested-key\u0026#39; = \u0026#39;sub_order_id\u0026#39; ); -- widen INSERT INTO order_wide SELECT order_id, user_name, address, CAST (NULL AS ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt;) FROM orders UNION ALL SELECT order_id, CAST (NULL AS STRING), CAST (NULL AS STRING), ARRAY[ROW(sub_order_id, product_name, price)] FROM sub_orders; -- query using UNNEST SELECT order_id, user_name, address, sub_order_id, product_name, price FROM order_wide, UNNEST(sub_orders) AS so(sub_order_id, product_name, price) collect\r#\rThe collect function collects elements into an Array. You can set fields.\u0026lt;field-name\u0026gt;.distinct=true to deduplicate elements. It only supports ARRAY type.\nmerge_map\r#\rThe merge_map function merge input maps. It only supports MAP type.\nTypes of cardinality sketches\r#\rPaimon uses the Apache DataSketches library of stochastic streaming algorithms to implement sketch modules. The DataSketches library includes various types of sketches, each one designed to solve a different sort of problem. Paimon supports HyperLogLog (HLL) and Theta cardinality sketches.\nHyperLogLog\r#\rThe HyperLogLog (HLL) sketch aggregator is a very compact sketch algorithm for approximate distinct counting. You can also use the HLL aggregator to calculate a union of HLL sketches.\nTheta\r#\rThe Theta sketch is a sketch algorithm for approximate distinct counting with set operations. Theta sketches let you count the overlap between sets, so that you can compute the union, intersection, or set difference between sketch objects.\nChoosing a sketch type\r#\rHLL and Theta sketches both support approximate distinct counting; however, the HLL sketch produces more accurate results and consumes less storage space. Theta sketches are more flexible but require significantly more memory.\nWhen choosing an approximation algorithm for your use case, consider the following:\nIf your use case entails distinct counting and merging sketch objects, use the HLL sketch. If you need to evaluate union, intersection, or difference set operations, use the Theta sketch. You cannot merge HLL sketches with Theta sketches.\nhll_sketch\r#\rThe hll_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink\r-- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.uv.aggregate-function\u0026#39; = \u0026#39;hll_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH\u0026#34; -- for example: create TEMPORARY function HLL_SKETCH as \u0026#39;HllSketchFunction\u0026#39;; -- which is used to transform input to sketch bytes array: -- -- public static class HllSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- HllSketch hllSketch = new HllSketch(); -- hllSketch.update(user_id); -- return hllSketch.toCompactByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, HLL_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH_COUNT\u0026#34; -- for example: create TEMPORARY function HLL_SKETCH_COUNT as \u0026#39;HllSketchCountFunction\u0026#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class HllSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return HllSketch.heapify(sketchBytes).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, HLL_SKETCH_COUNT(UV) as uv FROM UV_AGG; theta_sketch\r#\rThe theta_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink\r-- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.uv.aggregate-function\u0026#39; = \u0026#39;theta_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH\u0026#34; -- for example: create TEMPORARY function THETA_SKETCH as \u0026#39;ThetaSketchFunction\u0026#39;; -- which is used to transform input to sketch bytes array: -- -- public static class ThetaSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- UpdateSketch updateSketch = UpdateSketch.builder().build(); -- updateSketch.update(user_id); -- return updateSketch.compact().toByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, THETA_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH_COUNT\u0026#34; -- for example: create TEMPORARY function THETA_SKETCH_COUNT as \u0026#39;ThetaSketchCountFunction\u0026#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class ThetaSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return Sketches.wrapCompactSketch(Memory.wrap(sketchBytes)).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, THETA_SKETCH_COUNT(UV) as uv FROM UV_AGG; For streaming queries, `aggregation` merge engine must be used together with `lookup` or `full-compaction`\r[changelog producer](https://example.org/primary-key-table/changelog-producer/). ('input' changelog producer is also supported, but only returns input records.)\rRetraction\r#\rOnly sum, product, collect, merge_map, nested_update, last_value and last_non_null_value supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nThe last_value and last_non_null_value just set field to null when accept retract messages.\nThe product will return null for retraction message when accumulator is null.\nThe collect and merge_map make a best-effort attempt to handle retraction messages, but the results are not guaranteed to be accurate. The following behaviors may occur when processing retraction messages:\nIt might fail to handle retraction messages if records are disordered. For example, the table uses collect, and the upstreams send +I['A', 'B'] and -U['A'] respectively. If the table receives -U['A'] first, it can do nothing; then it receives +I['A', 'B'], the merge result will be +I['A', 'B'] instead of +I['B'].\nThe retract message from one upstream will retract the result merged from multiple upstreams. For example, the table uses merge_map, and one upstream sends +I[1-\u0026gt;A], another upstream sends +I[1-\u0026gt;B], -D[1-\u0026gt;B] later. The table will merge two insert values to +I[1-\u0026gt;B] first, and then the -D[1-\u0026gt;B] will retract the whole result, so the final result is an empty map instead of +I[1-\u0026gt;A]\n"},{"id":36,"href":"/program-api/catalog-api/","title":"Catalog API","section":"Program API","content":"\rCatalog API\r#\rCreate Database\r#\rYou can use the catalog to create databases. The created databases are persistence in the file system.\nimport org.apache.paimon.catalog.Catalog; public class CreateDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something } } } Determine Whether Database Exists\r#\rYou can use the catalog to determine whether the database exists\nimport org.apache.paimon.catalog.Catalog; public class DatabaseExists { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.databaseExists(\u0026#34;my_db\u0026#34;); } } List Databases\r#\rYou can use the catalog to list databases.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListDatabases { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; databases = catalog.listDatabases(); } } Drop Database\r#\rYou can use the catalog to drop database.\nimport org.apache.paimon.catalog.Catalog; public class DropDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropDatabase(\u0026#34;my_db\u0026#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } } } Alter Database\r#\rYou can use the catalog to alter database\u0026rsquo;s properties.(ps: only support hive and jdbc catalog)\nimport java.util.ArrayList; import org.apache.paimon.catalog.Catalog; public class AlterDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createHiveCatalog(); List\u0026lt;DatabaseChange\u0026gt; changes = new ArrayList\u0026lt;\u0026gt;(); changes.add(DatabaseChange.setProperty(\u0026#34;k1\u0026#34;, \u0026#34;v1\u0026#34;)); changes.add(DatabaseChange.removeProperty(\u0026#34;k2\u0026#34;)); catalog.alterDatabase(\u0026#34;my_db\u0026#34;, changes, true); } catch (Catalog.DatabaseNotExistException e) { // do something } } } Determine Whether Table Exists\r#\rYou can use the catalog to determine whether the table exists\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.tableExists(identifier); } } List Tables\r#\rYou can use the catalog to list tables.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListTables { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; tables = catalog.listTables(\u0026#34;my_db\u0026#34;); } catch (Catalog.DatabaseNotExistException e) { // do something } } } Drop Table\r#\rYou can use the catalog to drop table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something } } } Rename Table\r#\rYou can use the catalog to rename a table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Identifier toTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;test_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.TableNotExistException e) { // do something } } } Alter Table\r#\rYou can use the catalog to alter a table, but you need to pay attention to the following points.\nColumn %s cannot specify NOT NULL in the %s table. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported. import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Map\u0026lt;String, String\u0026gt; options = new HashMap\u0026lt;\u0026gt;(); options.put(\u0026#34;bucket\u0026#34;, \u0026#34;4\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); try { catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, \u0026#34;col1\u0026#34;, DataTypes.STRING(), \u0026#34;field1\u0026#34;), new DataField(1, \u0026#34;col2\u0026#34;, DataTypes.STRING(), \u0026#34;field2\u0026#34;), new DataField(2, \u0026#34;col3\u0026#34;, DataTypes.STRING(), \u0026#34;field3\u0026#34;), new DataField(3, \u0026#34;col4\u0026#34;, DataTypes.BIGINT(), \u0026#34;field4\u0026#34;), new DataField( 4, \u0026#34;col5\u0026#34;, DataTypes.ROW( new DataField( 5, \u0026#34;f1\u0026#34;, DataTypes.STRING(), \u0026#34;f1\u0026#34;), new DataField( 6, \u0026#34;f2\u0026#34;, DataTypes.STRING(), \u0026#34;f2\u0026#34;), new DataField( 7, \u0026#34;f3\u0026#34;, DataTypes.STRING(), \u0026#34;f3\u0026#34;)), \u0026#34;field5\u0026#34;), new DataField(8, \u0026#34;col6\u0026#34;, DataTypes.STRING(), \u0026#34;field6\u0026#34;)), Lists.newArrayList(\u0026#34;col1\u0026#34;), // partition keys Lists.newArrayList(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;), // primary key options, \u0026#34;table comment\u0026#34;), false); } catch (Catalog.TableAlreadyExistException e) { // do something } catch (Catalog.DatabaseNotExistException e) { // do something } // add option SchemaChange addOption = SchemaChange.setOption(\u0026#34;snapshot.time-retained\u0026#34;, \u0026#34;2h\u0026#34;); // add column SchemaChange addColumn = SchemaChange.addColumn(\u0026#34;col1_after\u0026#34;, DataTypes.STRING()); // add a column after col1 SchemaChange.Move after = SchemaChange.Move.after(\u0026#34;col1_after\u0026#34;, \u0026#34;col1\u0026#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(\u0026#34;col7\u0026#34;, DataTypes.STRING(), \u0026#34;\u0026#34;, after); // rename column SchemaChange renameColumn = SchemaChange.renameColumn(\u0026#34;col3\u0026#34;, \u0026#34;col3_new_name\u0026#34;); // drop column SchemaChange dropColumn = SchemaChange.dropColumn(\u0026#34;col6\u0026#34;); // update column comment SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col4\u0026#34;}, \u0026#34;col4 field\u0026#34;); // update nested column comment SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f1\u0026#34;}, \u0026#34;col5 f1 field\u0026#34;); // update column type SchemaChange updateColumnType = SchemaChange.updateColumnType(\u0026#34;col4\u0026#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(\u0026#34;col4\u0026#34;)); // update column nullability SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col4\u0026#34;}, false); // update nested column nullability SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f2\u0026#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[] { addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability }; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something } catch (Catalog.ColumnAlreadyExistException e) { // do something } catch (Catalog.ColumnNotExistException e) { // do something } } } "},{"id":37,"href":"/concepts/concurrency-control/","title":"Concurrency Control","section":"Concepts","content":"\rConcurrency Control\r#\rPaimon supports optimistic concurrency for multiple concurrent write jobs.\nEach job writes data at its own pace and generates a new snapshot based on the current snapshot by applying incremental files (deleting or adding files) at the time of committing.\nThere may be two types of commit failures here:\nSnapshot conflict: the snapshot id has been preempted, the table has generated a new snapshot from another job. OK, let\u0026rsquo;s commit again. Files conflict: The file that this job wants to delete has been deleted by another jobs. At this point, the job can only fail. (For streaming jobs, it will fail and restart, intentionally failover once) Snapshot conflict\r#\rPaimon\u0026rsquo;s snapshot ID is unique, so as long as the job writes its snapshot file to the file system, it is considered successful.\nPaimon uses the file system\u0026rsquo;s renaming mechanism to commit snapshots, which is secure for HDFS as it ensures transactional and atomic renaming.\nBut for object storage such as OSS and S3, their 'RENAME' does not have atomic semantic. We need to configure Hive or jdbc metastore and enable 'lock.enabled' option for the catalog. Otherwise, there may be a chance of losing the snapshot.\nFiles conflict\r#\rWhen Paimon commits a file deletion (which is only a logical deletion), it checks for conflicts with the latest snapshot. If there are conflicts (which means the file has been logically deleted), it can no longer continue on this commit node, so it can only intentionally trigger a failover to restart, and the job will retrieve the latest status from the filesystem in the hope of resolving this conflict.\nPaimon will ensure that there is no data loss or duplication here, but if two streaming jobs are writing at the same time and there are conflicts, you will see that they are constantly restarting, which is not a good thing.\nThe essence of conflict lies in deleting files (logically), and deleting files is born from compaction, so as long as we close the compaction of the writing job (Set \u0026lsquo;write-only\u0026rsquo; to true) and start a separate job to do the compaction work, everything is very good.\nSee dedicated compaction job for more info.\n"},{"id":38,"href":"/project/contributing/","title":"Contributing","section":"Project","content":"\rContributing\r#\rApache Paimon is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.\nWhat do you want to do?\r#\rContributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:\nArea\rFurther information\rReport Bug\rTo report a problem with Paimon, open Paimon’s issues. Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.\rContribute Code\rRead the Code Contribution Guide\rCode Reviews\rRead the Code Review Guide\rRelease Version\rReleasing a new Paimon version.\rSupport Users\rReply to questions on the user mailing list,\rcheck the latest issues in Issues for tickets which are actually user questions.\rSpread the Word About Paimon\rOrganize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blog\rpost on the dev@paimon.apache.org mailing list.\rAny other question? Reach out to the\rdev@paimon.apache.org mailing list to get help!\rCode Contribution Guide\r#\rApache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.\nPlease feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.\n1Discuss\rCreate an Issue or mailing list discussion and reach consensus\nTo request an issue, please note that it is not just a \"please assign it to me\", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.\n2Implement\rCreate the Pull Request and the approach agreed upon in the issue.\n1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.\n3Review\rWork with the reviewer.\n1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.\n4Merge\rA committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.\nCode Review Guide\r#\rEvery review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.\n1. Is the Contribution Well-Described?\r#\rCheck whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.\nIf the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.\n2. Does the Contribution Need Attention from some Specific Committers?\r#\rSome changes require attention and approval from specific committers.\nIf the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.\n3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon?\r#\rDoes the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated? Code guidelines can be found in the Flink Java Code Style and Quality Guide.\n4. Are the documentation updated?\r#\rIf the pull request introduces a new feature, the feature should be documented.\nBecome a Committer\r#\rWhen you have made enough contributions, you can be nominated as Paimon\u0026rsquo;s Committer. See Committer.\n"},{"id":39,"href":"/concepts/rest/dlf/","title":"DLF Token","section":"RESTCatalog","content":"\rDLF Token\r#\rDLF (Data Lake Formation) building is a fully-managed platform for unified metadata and data storage and management, aiming to provide customers with functions such as metadata management, storage management, permission management, storage analysis, and storage optimization.\nDLF provides multiple authentication methods for different environments.\nThe `'warehouse'` is your catalog instance name on the server, not the path.\rUse the access key\r#\rCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.access-key-id\u0026#39;=\u0026#39;\u0026lt;access-key-id\u0026gt;\u0026#39;, \u0026#39;dlf.access-key-secret\u0026#39;=\u0026#39;\u0026lt;access-key-secret\u0026gt;\u0026#39;, ); You can grant specific permissions to a RAM user and use the RAM user\u0026rsquo;s access key for long-term access to your DLF resources. Compared to using the Alibaba Cloud account access key, accessing DLF resources with a RAM user access key is more secure.\nUse the STS temporary access token\r#\rThrough the STS service, you can generate temporary access tokens for users, allowing them to access DLF resources restricted by policies within the validity period.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.access-key-id\u0026#39;=\u0026#39;\u0026lt;access-key-id\u0026gt;\u0026#39;, \u0026#39;dlf.access-key-secret\u0026#39;=\u0026#39;\u0026lt;access-key-secret\u0026gt;\u0026#39;, \u0026#39;dlf.security-token\u0026#39;=\u0026#39;\u0026lt;security-token\u0026gt;\u0026#39; ); In some environments, temporary access token can be periodically refreshed by using a local file:\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.token-path\u0026#39; = \u0026#39;my_token_path_in_disk\u0026#39; ); Use the STS token from aliyun ecs role\r#\rAn instance RAM role refers to a RAM role granted to an ECS instance. This RAM role is a standard service role with the trusted entity being the cloud server. By using an instance RAM role, it is possible to obtain temporary access token (STS Token) within the ECS instance without configuring an AccessKey.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.token-loader\u0026#39; = \u0026#39;ecs\u0026#39; -- optional, loader can obtain it through ecs metadata service -- \u0026#39;dlf.token-ecs-role-name\u0026#39; = \u0026#39;my_ecs_role_name\u0026#39; ); "},{"id":40,"href":"/ecosystem/doris/","title":"Doris","section":"Ecosystem","content":"\rDoris\r#\rThis documentation is a guide for using Paimon in Doris.\nMore details can be found in Apache Doris Website\nVersion\r#\rPaimon currently supports Apache Doris 2.0.6 and above.\nCreate Paimon Catalog\r#\rUse CREATE CATALOG statement in Apache Doris to create Paimon Catalog.\nDoris support multi types of Paimon Catalogs. Here are some examples:\n-- HDFS based Paimon Catalog CREATE CATALOG `paimon_hdfs` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/paimon\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); -- Aliyun OSS based Paimon Catalog CREATE CATALOG `paimon_oss` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;oss://paimon-bucket/paimonoss\u0026#34;, \u0026#34;oss.endpoint\u0026#34; = \u0026#34;oss-cn-beijing.aliyuncs.com\u0026#34;, \u0026#34;oss.access_key\u0026#34; = \u0026#34;ak\u0026#34;, \u0026#34;oss.secret_key\u0026#34; = \u0026#34;sk\u0026#34; ); -- Hive Metastore based Paimon Catalog CREATE CATALOG `paimon_hms` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;hms\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/zhangdong/paimon2\u0026#34;, \u0026#34;hive.metastore.uris\u0026#34; = \u0026#34;thrift://172.21.0.44:7004\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); -- Integrate with Aliyun DLF CREATE CATALOG paimon_dlf PROPERTIES ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;paimon.catalog.type\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://paimon-bucket/paimonoss/\u0026#39;, \u0026#39;dlf.proxy.mode\u0026#39; = \u0026#39;DLF_ONLY\u0026#39;, \u0026#39;dlf.uid\u0026#39; = \u0026#39;xxxxx\u0026#39;, \u0026#39;dlf.region\u0026#39; = \u0026#39;cn-beijing\u0026#39;, \u0026#39;dlf.access_key\u0026#39; = \u0026#39;ak\u0026#39;, \u0026#39;dlf.secret_key\u0026#39; = \u0026#39;sk\u0026#39; ); See Apache Doris Website for more examples.\nAccess Paimon Catalog\r#\rQuery Paimon table with full qualified name\nSELECT * FROM paimon_hdfs.paimon_db.paimon_table; Switch to Paimon Catalog and query\nSWITCH paimon_hdfs; USE paimon_db; SELECT * FROM paimon_table; Query Optimization\r#\rRead optimized for Primary Key Table\nDoris can utilize the Read optimized feature for Primary Key Table(release in Paimon 0.6), by reading base data files using native Parquet/ORC reader and delta file using JNI.\nDeletion Vectors\nDoris(2.1.4+) natively supports Deletion Vectors(released in Paimon 0.8).\nDoris to Paimon type mapping\r#\rDoris Data Type\rPaimon Data Type\rAtomic Type\rBoolean\rBooleanType\rtrue\rTinyInt\rTinyIntType\rtrue\rSmallInt\rSmallIntType\rtrue\rInt\rIntType\rtrue\rBigInt\rBigIntType\rtrue\rFloat\rFloatType\rtrue\rDouble\rDoubleType\rtrue\rVarchar\rVarCharType\rtrue\rChar\rCharType\rtrue\rBinary\rVarBinaryType, BinaryType\rtrue\rDecimal(precision, scale)\rDecimalType(precision, scale)\rtrue\rDatetime\rTimestampType,LocalZonedTimestampType\rtrue\rDate\rDateType\rtrue\rArray\rArrayType\rfalse\rMap\rMapType\rfalse\rStruct\rRowType\rfalse\r"},{"id":41,"href":"/cdc-ingestion/kafka-cdc/","title":"Kafka CDC","section":"CDC Ingestion","content":"\rKafka CDC\r#\rPrepare Kafka Bundled Jar\r#\rflink-sql-connector-kafka-*.jar Supported Formats\r#\rFlink provides several Kafka CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\nFormats\rSupported\rCanal CDC\rTrue\rDebezium CDC\rTrue\rMaxwell CDC\rTrue\rOGG CDC\rTrue\rJSON\rTrue\raws-dms-json\rTrue\rdebezium-bson\rTrue\rThe JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don't contain field types; When you write JSON sources into Flink Kafka sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\r1. Usually, debezium-json contains 'schema' field, from which Paimon will retrieve data types. Make sure your debezium json has this field, or Paimon will use 'STRING' type.\r2. If missing field types, Paimon will use 'STRING' type as default. 3. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization.\r4. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.\rSynchronizing Tables\r#\rBy using KafkaSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Kafka\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--type_mapping to-string] \\ [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\ [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table\rThe Paimon table name.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r\"decimal-no-change\": Ignore decimal type change.\r--computed_column\rThe definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations. --kafka_conf\rThe configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rIf the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Kafka topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Kafka topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 If the kafka topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key create_time TIMESTAMP, -- the argument of computed column part part STRING, -- partition key PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\ ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=test_log \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf sink.parallelism=4 Synchronizing Databases\r#\rBy using KafkaSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ [--table_mapping \u0026lt;table-name\u0026gt;=\u0026lt;paimon-table-name1\u0026gt; [--table_mapping \u0026lt;table-name2\u0026gt;=\u0026lt;paimon-table-name2\u0026gt; ...]] \\ [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\ [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\ [--table_prefix_db \u0026lt;db-name1\u0026gt;=\u0026lt;table-prefix1\u0026gt; [--table_prefix_db \u0026lt;db-name2\u0026gt;=\u0026lt;table-prefix2\u0026gt; ...]] \\ [--table_suffix_db \u0026lt;db-name1\u0026gt;=\u0026lt;table-suffix1\u0026gt; [--table_suffix_db \u0026lt;db-name2\u0026gt;=\u0026lt;table-suffix2\u0026gt; ...]] \\ [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\ [--including_dbs \u0026lt;database-name|name-regular-expr\u0026gt;] \\ [--excluding_dbs \u0026lt;database-name|name-regular-expr\u0026gt;] \\ [--type_mapping to-string] \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--ignore_incompatible\rIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.\r--table_mapping\rThe table name mapping between source database and Paimon. For example, if you want to synchronize a source table named \"test\" to a Paimon table named \"paimon_test\", you can specify \"--table_mapping test=paimon_test\". Multiple mappings could be specified with multiple \"--table_mapping\" options. \"--table_mapping\" has higher priority than \"--table_prefix\" and \"--table_suffix\".\r--table_prefix\rThe prefix of all Paimon tables to be synchronized except those specified by \"--table_mapping\" or \"--table_prefix_db\". For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".\r--table_suffix\rThe suffix of all Paimon tables to be synchronized except those specified by \"--table_mapping\" or \"--table_suffix_db\". The usage is same as \"--table_prefix\".\r--table_prefix_db\rThe prefix of the Paimon tables to be synchronized from the specified db. For example, if you want to prefix the tables from db1 with \"ods_db1_\", you can specify \"--table_prefix_db db1=ods_db1_\". Multiple mappings could be specified multiple \"--table_prefix_db\" options. \"--table_prefix_db\" has higher priority than \"--table_prefix\".\r--table_suffix_db\rThe suffix of the Paimon tables to be synchronized from the specified db. The usage is same as \"--table_prefix_db\".\r--including_tables\rIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.\r--excluding_tables\rIt is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.\r--including_dbs\rIt is used to specify the databases within which the tables are to be synchronized. The usage is same as \"--including_tables\".\r--excluding_dbs\rIt is used to specify the databases within which the tables are not to be synchronized. The usage is same as \"--excluding_tables\". \"--excluding_dbs\" has higher priority than \"--including_dbs\" if you specified both.\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r\"decimal-no-change\": Ignore decimal type change.\r--computed_column\rThe definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations. --eager_init\rIt is default false. If true, all relevant tables commiter will be initialized eagerly, which means those tables could be forced to create snapshot.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\rIf the keys are not in source table, the sink table won't set partition keys.\r--multiple_table_partition_keys\rThe partition keys for each different Paimon table. If there are multiple partition keys, connect them with comma, for example\r--multiple_table_partition_keys tableName1=col1,col2.col3\r--multiple_table_partition_keys tableName2=col4,col5.col6\r--multiple_table_partition_keys tableName3=col7,col8.col9\rIf the keys are not in source table, the sink table won't set partition keys.\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\rIf the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.\rOtherwise, the sink table won't set primary keys.\r--kafka_conf\rThe configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rThis action will build a single combined sink for all tables. For each Kafka topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Kafka topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Kafka record, this action will try to preform schema evolution.\nExample\nSynchronization from one Kafka topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronization from multiple Kafka topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=order\\;logistic_order\\;user \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=canal-json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Additional kafka_config\r#\rThere are some useful options to build Flink Kafka Source, but they are not provided by flink-kafka-connector document. They are:\nKey\rDefault\rType\rDescription\rschema.registry.url\r(none)\rString\rWhen configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.\r"},{"id":42,"href":"/append-table/query-performance/","title":"Query Performance","section":"Table w/o PK","content":"\rQuery Performance\r#\rData Skipping By Order\r#\rPaimon by default records the maximum and minimum values of each field in the manifest file.\nIn the query, according to the WHERE condition of the query, together with the statistics in the manifest we can perform file filtering. If the filtering effect is good, the query that would have cost minutes will be accelerated to milliseconds to complete the execution.\nOften the data distribution is not always ideal for filtering, so can we sort the data by the field in WHERE condition? You can take a look at Flink COMPACT Action, Flink COMPACT Procedure or Spark COMPACT Procedure.\nData Skipping By File Index\r#\rYou can use file index too, it filters files by indexing on the reading side.\nCREATE TABLE \u0026lt;PAIMON_TABLE\u0026gt; (\u0026lt;COLUMN\u0026gt; \u0026lt;COLUMN_TYPE\u0026gt; , ...) WITH ( \u0026#39;file-index.bloom-filter.columns\u0026#39; = \u0026#39;c1,c2\u0026#39;, \u0026#39;file-index.bloom-filter.c1.items\u0026#39; = \u0026#39;200\u0026#39; ); Define file-index.bloom-filter.columns, Data file index is an external index file and Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, otherwise in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nDifferent file indexes may be efficient in different scenarios. For example bloom filter may speed up query in point lookup scenario. Using a bitmap may consume more space but can result in greater accuracy.\nBloom Filter:\nfile-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.fpp to config false positive probability. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.items to config the expected distinct items in one data file. Bitmap:\nfile-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap. Bit-Slice Index Bitmap\nfile-index.bsi.columns: specify the columns that need bsi index. More filter types will be supported\u0026hellip;\nIf you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.\u0026lt;filter-type\u0026gt;.columns to the table.\nHow to invoke: see flink procedures\n"},{"id":43,"href":"/concepts/spec/snapshot/","title":"Snapshot","section":"Specification","content":"\rSnapshot\r#\rEach commit generates a snapshot file, and the version of the snapshot file starts from 1 and must be continuous. EARLIEST and LATEST are hint files at the beginning and end of the snapshot list, and they can be inaccurate. When hint files are inaccurate, the read will scan all snapshot files to determine the beginning and end.\nwarehouse └── default.db └── my_table ├── snapshot ├── EARLIEST ├── LATEST ├── snapshot-1 ├── snapshot-2 └── snapshot-3 Writing commit will preempt the next snapshot id, and once the snapshot file is successfully written, this commit will be visible.\nSnapshot File is JSON, it includes:\nversion: Snapshot file version, current is 3. id: snapshot id, same to file name. schemaId: the corresponding schema version for this commit. baseManifestList: a manifest list recording all changes from the previous snapshots. deltaManifestList: a manifest list recording all new changes occurred in this snapshot. changelogManifestList: a manifest list recording all changelog produced in this snapshot, null if no changelog is produced. indexManifest: a manifest recording all index files of this table, null if no table index file. commitUser: usually generated by UUID, it is used for recovery of streaming writes, one stream write job with one user. commitIdentifier: transaction id corresponding to streaming write, each transaction may result in multiple commits for different commitKinds. commitKind: type of changes in this snapshot, including append, compact, overwrite and analyze. timeMillis: commit time millis. logOffsets: commit log offsets. totalRecordCount: record count of all changes occurred in this snapshot. deltaRecordCount: record count of all new changes occurred in this snapshot. changelogRecordCount: record count of all changelog produced in this snapshot. watermark: watermark for input records, from Flink watermark mechanism, Long.MIN_VALUE if there is no watermark. statistics: stats file name for statistics of this table. "},{"id":44,"href":"/primary-key-table/table-mode/","title":"Table Mode","section":"Table with PK","content":"\rTable Mode\r#\rThe file structure of the primary key table is roughly shown in the above figure. The table or partition contains multiple buckets, and each bucket is a separate LSM tree structure that contains multiple files.\nThe writing process of LSM is roughly as follows: Flink checkpoint flush L0 files, and trigger a compaction as needed to merge the data. According to the different processing ways during writing, there are three modes:\nMOR (Merge On Read): Default mode, only minor compactions are performed, and merging are required for reading. COW (Copy On Write): Using 'full-compaction.delta-commits' = '1', full compaction will be synchronized, which means the merge is completed on write. MOW (Merge On Write): Using 'deletion-vectors.enabled' = 'true', in writing phase, LSM will be queried to generate the deletion vector file for the data file, which directly filters out unnecessary lines during reading. The Merge On Write mode is recommended for general primary key tables (merge-engine is default deduplicate).\nMerge On Read\r#\rMOR is the default mode of primary key table.\nWhen the mode is MOR, it is necessary to merge all files for reading, as all files are ordered and undergo multi way merging, which includes a comparison calculation of the primary key.\nThere is an obvious issue here, where a single LSM tree can only have a single thread to read, so the read parallelism is limited. If the amount of data in the bucket is too large, it can lead to poor read performance. So in order to read performance, it is recommended to analyze the query requirements table and set the data volume in the bucket to be between 200MB and 1GB. But if the bucket is too small, there will be a lot of small file reads and writes, causing pressure on the file system.\nIn addition, due to the merging process, Filter based data skipping cannot be performed on non primary key columns, otherwise new data will be filtered out, resulting in incorrect old data.\nWrite performance: very good. Read performance: not so good. Copy On Write\r#\rALTER TABLE orders SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); Set full-compaction.delta-commits to 1, which means that every write will be fully merged, and all data will be merged to the highest level. When reading, merging is not necessary at this time, and the reading performance is the highest. But every write requires full merging, and write amplification is very severe.\nWrite performance: very bad. Read performance: very good. Merge On Write\r#\rALTER TABLE orders SET (\u0026#39;deletion-vectors.enabled\u0026#39; = \u0026#39;true\u0026#39;); Thanks to Paimon\u0026rsquo;s LSM structure, it has the ability to be queried by primary key. We can generate deletion vectors files when writing, representing which data in the file has been deleted. This directly filters out unnecessary rows during reading, which is equivalent to merging and does not affect reading performance.\nA simple example just like:\nUpdates data by deleting old record first and then adding new one.\nWrite performance: good. Read performance: good. Visibility guarantee: Tables in deletion vectors mode, the files with level 0 will only be visible after compaction.\rSo by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.\rMOR Read Optimized\r#\rIf you don\u0026rsquo;t want to use Deletion Vectors mode, you want to query fast enough in MOR mode, but can only find older data, you can also:\nConfigure \u0026lsquo;compaction.optimization-interval\u0026rsquo; when writing data. Query from read-optimized system table. Reading from results of optimized files avoids merging records with the same key, thus improving reading performance. You can flexibly balance query performance and data latency when reading.\n"},{"id":45,"href":"/append-table/","title":"Table w/o PK","section":"Apache Paimon","content":"\r"},{"id":46,"href":"/maintenance/write-performance/","title":"Write Performance","section":"Maintenance","content":"\rWrite Performance\r#\rPaimon\u0026rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:\nFlink Configuration ('flink-conf.yaml'/'config.yaml' or SET in SQL): Increase the checkpoint interval ('execution.checkpointing.interval'), increase max concurrent checkpoints to 3 ('execution.checkpointing.max-concurrent-checkpoints'), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode. Option 'changelog-producer' = 'lookup' or 'full-compaction', and option 'full-compaction.delta-commits' have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.\nIf you find that the input of the job shows a jagged pattern in the case of backpressure, it may be imbalanced work nodes. You can consider turning on Asynchronous Compaction to observe if the throughput is increased.\nParallelism\r#\rIt is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\nOption\rRequired\rDefault\rType\rDescription\rsink.parallelism\rNo\r(none)\rInteger\rDefines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.\rLocal Merging\r#\rIf your job suffers from primary key data skew (for example, you want to count the number of views for each page in a website, and some particular pages are very popular among the users), you can set 'local-merge-buffer-size' so that input records will be buffered and merged before they\u0026rsquo;re shuffled by bucket and written into sink. This is particularly useful when the same primary key is updated frequently between snapshots.\nThe buffer will be flushed when it is full. We recommend starting with 64 mb when you are faced with data skew but don\u0026rsquo;t know where to start adjusting buffer size.\n(Currently, Local merging not works for CDC ingestion)\nFile Format\r#\rIf you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.\nThe advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase. This a tradeoff.\nEnable row storage through the following options:\nfile.format = avro metadata.stats-mode = none The collection of statistical information for row storage is a bit expensive, so I suggest turning off statistical information as well.\nIf you don\u0026rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.\nFile Compression\r#\rBy default, Paimon uses zstd with level 1, you can modify the compression algorithm:\n'file.compression.zstd-level': Default zstd level is 1. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.\nStability\r#\rIf there are too few buckets or resources, full-compaction may cause the checkpoint timeout, Flink\u0026rsquo;s default checkpoint timeout is 10 minutes.\nIf you expect stability even in this case, you can turn up the checkpoint timeout, for example:\nexecution.checkpointing.timeout = 60 min Write Initialize\r#\rIn the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.\nWrite Memory\r#\rThere are three main places in Paimon writer that takes up memory:\nWriter\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar ORC file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format. If files are automatically compaction in the write task, dictionaries for certain large columns can significantly consume memory during compaction. To disable dictionary encoding for all fields in Parquet format, set 'parquet.enable.dictionary'= 'false'. To disable dictionary encoding for all fields in ORC format, set orc.dictionary.key.threshold='0'. Additionally,set orc.column.encoding.direct='field1,field2' to disable dictionary encoding for specific columns. If your Flink job does not rely on state, please avoid using managed memory, which you can control with the following Flink parameter:\ntaskmanager.memory.managed.size=1m Or you can use Flink managed memory for your write buffer to avoid OOM, set table property:\nsink.use-managed-memory-allocator=true Commit Memory\r#\rCommitter node may use a large memory if the amount of data written to the table is particularly large, OOM may occur if the memory is too small. In this case, you need to increase the Committer heap memory, but you may not want to increase the memory of Flink\u0026rsquo;s TaskManager uniformly, which may lead to a waste of memory.\nYou can use fine-grained-resource-management of Flink to increase committer heap memory only:\nConfigure Flink Configuration cluster.fine-grained-resource-management.enabled: true. (This is default after Flink 1.18) Configure Paimon Table Options: sink.committer-memory, for example 300 MB, depends on your TaskManager. (sink.committer-cpu is also supported) If you use Flink batch job write data into Paimon or run dedicated compaction, Configure Flink Configuration fine-grained.shuffle-mode.all-blocking: true. "},{"id":47,"href":"/concepts/catalog/","title":"Catalog","section":"Concepts","content":"\rCatalog\r#\rPaimon provides a Catalog abstraction to manage the table of contents and metadata. The Catalog abstraction provides a series of ways to help you better integrate with computing engines. We always recommend that you use Catalog to access the Paimon table.\nCatalogs\r#\rPaimon catalogs currently support four types of metastores:\nfilesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. rest metastore, which is designed to provide a lightweight way to access any catalog backend from a single client. Filesystem Catalog\r#\rMetadata and table files are stored under hdfs:///path/to/warehouse.\n-- Flink SQL CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); Hive Catalog\r#\rBy using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, schema is also stored in Hive metastore.\n-- Flink SQL CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table option metastore.partitioned-table to true.\nJDBC Catalog\r#\rBy using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\n-- Flink SQL CREATE CATALOG my_jdbc WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt;\u0026#39;, \u0026#39;jdbc.user\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;jdbc.password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;catalog-key\u0026#39;=\u0026#39;jdbc\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); REST Catalog\r#\rBy using the Paimon REST catalog, changes to the catalog will be directly stored in a remote catalog server which exposed through REST API. See Paimon REST Catalog.\n"},{"id":48,"href":"/project/committer/","title":"Committer","section":"Project","content":"\rCommitter\r#\rBecome a Committer\r#\rHow to become a committer\r#\rThere is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.\nIf you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.\nCommunity contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The \u0026ldquo;Apache Way\u0026rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.\nCode/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.\nIdentify promising candidates\r#\rWhile the prior points give ways to identify promising candidates, the following are \u0026ldquo;must haves\u0026rdquo; for any committer candidate:\nBeing community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.\nWe trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don\u0026rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.\nThey have shown to be respectful towards other community members and constructive in discussions.\nCommitter Rights\r#\rJetBrains provides a free license to Apache Committers, allowing them to access all JetBrains IDEs, such as IntelliJ IDEA, PyCharm, and other desktop tools.\nPlease use your @apache.org email address to All Products Packs for Apache committers.\n"},{"id":49,"href":"/maintenance/dedicated-compaction/","title":"Dedicated Compaction","section":"Maintenance","content":"\rDedicated Compaction\r#\rPaimon\u0026rsquo;s snapshot management supports writing with multiple writers.\nFor S3-like object store, its `'RENAME'` does not have atomic semantic. We need to configure Hive metastore and\renable `'lock.enabled'` option for the catalog.\rBy default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon\u0026rsquo;s latest partition, Simultaneously batch job (overwrite) writes records to the historical partition.\nSo far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated. For example, you don\u0026rsquo;t want to use UNION ALL, you have multiple streaming jobs to write records to a 'partial-update' table. Please refer to the 'Dedicated Compaction Job' below.\nDedicated Compaction Job\r#\rBy default, Paimon writers will perform compaction as needed during writing records. This is sufficient for most use cases.\nCompaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file, a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.\nTo avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\nOption\rRequired\rDefault\rType\rDescription\rwrite-only\rNo\rfalse\rBoolean\rIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.\rTo run a dedicated job for compaction, follow these instructions.\nFlink SQL\rRun the following sql:\nCALL sys.compact( `table` =\u0026gt; \u0026#39;default.T\u0026#39;, partitions =\u0026gt; \u0026#39;p=0\u0026#39;, options =\u0026gt; \u0026#39;sink.parallelism=4\u0026#39;, `where` =\u0026gt; \u0026#39;dt\u0026gt;10 and h\u0026lt;20\u0026#39; ); Flink Action Jar\rRun the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition \u0026lt;partition-name\u0026gt;] \\ [--compact_strategy \u0026lt;minor / full\u0026gt;] \\ [--table_conf \u0026lt;table_conf\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Example: compact table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --warehouse s3:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition dt=20221126,hh=08 \\ --partition dt=20221127,hh=09 \\ --table_conf sink.parallelism=10 \\ --compact_strategy minor \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** --compact_strategy Determines how to pick files to be merged, the default is determined by the runtime execution mode, streaming-mode use minor strategy and batch-mode use full strategy. full : Only supports batch mode. All files will be selected for merging. minor : Pick the set of files that need to be merged based on specified conditions. You can use -D execution.runtime-mode=batch or -yD execution.runtime-mode=batch (for the ON-YARN scenario) to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nFor more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact --help Similarly, the default is synchronous compaction, which may cause checkpoint timeouts.\rYou can configure `table_conf` to use [Asynchronous Compaction](https://example.org/primary-key-table/compaction/#asynchronous-compaction).\rDatabase Compaction Job\r#\rYou can run the following command to submit a compaction job for multiple database.\nFlink SQL\rRun the following sql:\nCALL sys.compact_database( including_databases =\u0026gt; \u0026#39;includingDatabases\u0026#39;, mode =\u0026gt; \u0026#39;mode\u0026#39;, including_tables =\u0026gt; \u0026#39;includingTables\u0026#39;, excluding_tables =\u0026gt; \u0026#39;excludingTables\u0026#39;, table_options =\u0026gt; \u0026#39;tableOptions\u0026#39; ) -- example CALL sys.compact_database( including_databases =\u0026gt; \u0026#39;db1|db2\u0026#39;, mode =\u0026gt; \u0026#39;combined\u0026#39;, including_tables =\u0026gt; \u0026#39;table_.*\u0026#39;, excluding_tables =\u0026gt; \u0026#39;ignore\u0026#39;, table_options =\u0026gt; \u0026#39;sink.parallelism=4\u0026#39; ) Flink Action Jar\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\ [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\ [--mode \u0026lt;compact-mode\u0026gt;] \\ [--compact_strategy \u0026lt;minor / full\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]] --including_databases is used to specify which database is to be compacted. In compact mode, you need to specify a database name, in compact_database mode, you could specify multiple database, regular expression is supported. --including_tables is used to specify which source tables are to be compacted, you must use \u0026lsquo;|\u0026rsquo; to separate multiple tables, the format is databaseName.tableName, regular expression is supported. For example, specifying \u0026ldquo;\u0026ndash;including_tables db1.t1|db2.+\u0026rdquo; means to compact table \u0026lsquo;db1.t1\u0026rsquo; and all tables in the db2 database. --excluding_tables is used to specify which source tables are not to be compacted. The usage is same as \u0026ldquo;\u0026ndash;including_tables\u0026rdquo;. \u0026ldquo;\u0026ndash;excluding_tables\u0026rdquo; has higher priority than \u0026ldquo;\u0026ndash;including_tables\u0026rdquo; if you specified both. --mode is used to specify compaction mode. Possible values: \u0026ldquo;divided\u0026rdquo; (the default mode if you haven\u0026rsquo;t specified one): start a sink for each table, the compaction of the new table requires restarting the job. \u0026ldquo;combined\u0026rdquo;: start a single combined sink for all tables, the new table will be automatically compacted. --catalog_conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations. --table_conf is the configuration for compaction. Each configuration should be specified in the format key=value. Pivotal configuration is listed below: Key Default Type Description continuous.discovery-interval 10 s Duration The discovery interval of continuous reading. sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. You can use -D execution.runtime-mode=batch to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.\nYou can set --mode combined to enable compacting newly added tables without restarting job.\nExample1: compact database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** Example2: compact database in combined mode\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --mode combined \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** \\ --table_conf continuous.discovery-interval=***** For more usage of the compact_database action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database --help Sort Compact\r#\rIf your table is configured with dynamic bucket primary key table or append table , you can trigger a compact with specified column sort to speed up queries.\nFlink SQL\rRun the following sql:\n-- sort compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, order_strategy =\u0026gt; \u0026#39;zorder\u0026#39;, order_by =\u0026gt; \u0026#39;a,b\u0026#39;) Flink Action Jar\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --order_strategy \u0026lt;orderType\u0026gt; \\ --order_by \u0026lt;col1,col2,...\u0026gt; \\ [--partition \u0026lt;partition-name\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are two new configuration in Sort Compact\nConfiguration\rDescription\r--order_strategy\rthe order strategy now support \"zorder\" and \"hilbert\" and \"order\". For example: --order_strategy zorder\r--order_by\rSpecify the order columns. For example: --order_by col0, col1\rThe sort parallelism is the same as the sink parallelism, you can dynamically specify it by add conf --table_conf sink.parallelism=\u0026lt;value\u0026gt;.\nHistorical Partition Compact\r#\rYou can run the following command to submit a compaction job for partition which has not received any new data for a period of time. Small files in those partitions will be full compacted.\nThis feature now is only used in batch mode.\rFor Table\r#\rThis is for one table. Flink SQL\rRun the following sql:\n-- history partition compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, \u0026#39;partition_idle_time\u0026#39; =\u0026gt; \u0026#39;1 d\u0026#39;) Flink Action Jar\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\ [--partition \u0026lt;partition-name\u0026gt;] \\ [--compact_strategy \u0026lt;minor / full\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are one new configuration in Historical Partition Compact\n--partition_idle_time: this is used to do a full compaction for partition which had not received any new data for \u0026lsquo;partition_idle_time\u0026rsquo;. And only these partitions will be compacted. For Databases\r#\rThis is for multiple tables in different databases. Flink SQL\rRun the following sql:\n-- history partition compact table CALL sys.compact_database( including_databases =\u0026gt; \u0026#39;includingDatabases\u0026#39;, mode =\u0026gt; \u0026#39;mode\u0026#39;, including_tables =\u0026gt; \u0026#39;includingTables\u0026#39;, excluding_tables =\u0026gt; \u0026#39;excludingTables\u0026#39;, table_options =\u0026gt; \u0026#39;tableOptions\u0026#39;, partition_idle_time =\u0026gt; \u0026#39;partition_idle_time\u0026#39; ); Example: compact historical partitions for tables in database\n-- history partition compact table CALL sys.compact_database( includingDatabases =\u0026gt; \u0026#39;test_db\u0026#39;, mode =\u0026gt; \u0026#39;combined\u0026#39;, partition_idle_time =\u0026gt; \u0026#39;1 d\u0026#39; ); Flink Action Jar\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ -D execution.runtime-mode=batch \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\ --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\ [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\ [--mode \u0026lt;compact-mode\u0026gt;] \\ [--compact_strategy \u0026lt;minor / full\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]] Example: compact historical partitions for tables in database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact_database \\ --warehouse s3:///path/to/warehouse \\ --including_databases test_db \\ --partition_idle_time 1d \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** "},{"id":50,"href":"/primary-key-table/merge-engine/first-row/","title":"First Row","section":"Merge Engine","content":"\rFirst Row\r#\rBy specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.\n`first-row` merge engine only supports `none` and `lookup` changelog producer. For streaming queries must be used with the `lookup` [changelog producer](https://example.org/primary-key-table/changelog-producer/).\r1. You can not specify [sequence.field](https://example.org/primary-key-table/sequence-rowkind/#sequence-field).\r2. Not accept `DELETE` and `UPDATE_BEFORE` message. You can config `ignore-delete` to ignore these two kinds records.\r3. Visibility guarantee: Tables with First Row engine, the files with level 0 will only be visible after compaction.\rSo by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.\rThis is of great help in replacing log deduplication in streaming computation.\n"},{"id":51,"href":"/ecosystem/hive/","title":"Hive","section":"Ecosystem","content":"\rHive\r#\rThis documentation is a guide for using Paimon in Hive.\nVersion\r#\rPaimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.\nExecution Engine\r#\rPaimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.\nInstallation\r#\rDownload the jar file with corresponding version.\nJar Hive 3.1 paimon-hive-connector-3.1-1.1.1.jar Hive 2.3 paimon-hive-connector-2.3-1.1.1.jar Hive 2.2 paimon-hive-connector-2.2-1.1.1.jar Hive 2.1 paimon-hive-connector-2.1-1.1.1.jar Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.1.1.jar You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command. mvn clean install -DskipTests\nYou can find Hive connector jar in ./paimon-hive/paimon-hive-connector-\u0026lt;hive-version\u0026gt;/target/paimon-hive-connector-\u0026lt;hive-version\u0026gt;-1.1.1.jar.\nThere are several ways to add this jar to Hive.\nYou can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-1.1.1.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-1.1.1.jar to enable paimon support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class. NOTE:\nIf you are using HDFS : Make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set. You can set paimon.hadoop-load-default-config =false to disable loading the default value from core-default.xml、hdfs-default.xml, which may lead smaller size for split. With hive cbo, it may lead to some incorrect query results, such as to query struct type with not null predicate, you can disable the cbo by set hive.cbo.enable=false; command. Hive SQL: access Paimon Tables already in Hive metastore\r#\rRun the following Hive SQL in Hive CLI to access the created table.\n-- Assume that paimon-hive-connector-\u0026lt;hive-version\u0026gt;-1.1.1.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default) SHOW TABLES; /* OK test_table */ -- Read records from test_table SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table -- Limitations: -- Only support INSERT INTO, not support INSERT OVERWRITE -- It is recommended to write to a non primary key table -- Writing to a primary key table may result in a large number of small files INSERT INTO test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ -- time travel SET paimon.scan.snapshot-id=1; SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ SET paimon.scan.snapshot-id=null; Hive SQL: create new Paimon Tables\r#\rYou can create new paimon tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-1.1.1.jar is already in auxlib directory. -- Let\u0026#39;s create a new paimon table. SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE hive_test_table( a INT COMMENT \u0026#39;The a field\u0026#39;, b STRING COMMENT \u0026#39;The b field\u0026#39; ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39;; Hive SQL: access Paimon Tables by External Table\r#\rTo access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-1.1.1.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough. CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- In addition to the way setting location above, you can also place the location setting in TBProperties -- to avoid Hive accessing Paimon\u0026#39;s location through its own file system when creating tables. -- This method is effective in scenarios using Object storage,such as s3. CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;paimon_location\u0026#39; =\u0026#39;s3://xxxxx/path/to/table/store/warehouse/default.db/test_table\u0026#39; ); -- Read records from external_test_table SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table INSERT INTO external_test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ Hive Type Conversion\r#\rThis section lists all supported type conversion between Hive and Paimon. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\nHive Data Type\rPaimon Data Type\rAtomic Type\rStructTypeInfo\rRowType\rfalse\rMapTypeInfo\rMapType\rfalse\rListTypeInfo\rArrayType\rfalse\rPrimitiveTypeInfo(\"boolean\")\rBooleanType\rtrue\rPrimitiveTypeInfo(\"tinyint\")\rTinyIntType\rtrue\rPrimitiveTypeInfo(\"smallint\")\rSmallIntType\rtrue\rPrimitiveTypeInfo(\"int\")\rIntType\rtrue\rPrimitiveTypeInfo(\"bigint\")\rBigIntType\rtrue\rPrimitiveTypeInfo(\"float\")\rFloatType\rtrue\rPrimitiveTypeInfo(\"double\")\rDoubleType\rtrue\rCharTypeInfo(length)\rCharType(length)\rtrue\rPrimitiveTypeInfo(\"string\")\rVarCharType(VarCharType.MAX_LENGTH)\rtrue\rVarcharTypeInfo(length)\rVarCharType(length), length is less than VarCharType.MAX_LENGTH\rtrue\rPrimitiveTypeInfo(\"date\")\rDateType\rtrue\rPrimitiveTypeInfo(\"timestamp\")\rTimestampType\rtrue\rDecimalTypeInfo(precision, scale)\rDecimalType(precision, scale)\rtrue\rPrimitiveTypeInfo(\"binary\")\rVarBinaryType, BinaryType\rtrue\r"},{"id":52,"href":"/migration/iceberg-compatibility/","title":"Iceberg Compatibility","section":"Migration","content":"\rIceberg Compatibility\r#\rPaimon supports generating Iceberg compatible metadata, so that Paimon tables can be consumed directly by Iceberg readers.\nSet the following table options, so that Paimon tables can generate Iceberg compatible metadata.\nOption\rDefault\rType\rDescription\rmetadata.iceberg.storage\rdisabled\rEnum\rWhen set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.\rdisabled: Disable Iceberg compatibility support.\rtable-location: Store Iceberg metadata in each table's directory.\rhadoop-catalog: Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog.\rhive-catalog: Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.\rFor most SQL users, we recommend setting 'metadata.iceberg.storage' = 'hadoop-catalog' or 'metadata.iceberg.storage' = 'hive-catalog', so that all tables can be visited as an Iceberg warehouse. For Iceberg Java API users, you might consider setting 'metadata.iceberg.storage' = 'table-location', so you can visit each table with its table path.\nAppend Tables\r#\rLet\u0026rsquo;s walk through a simple example, where we query Paimon tables with Iceberg connectors in Flink and Spark. Before trying out this example, make sure that your compute engine already supports Iceberg. Please refer to Iceberg\u0026rsquo;s document if you haven\u0026rsquo;t set up Iceberg.\nFlink: Preparation when using Flink SQL Client Spark: Using Iceberg in Spark 3 Let\u0026rsquo;s now create a Paimon append only table with Iceberg compatibility enabled and insert some data.\nFlink SQL\rCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;new york\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;berlin\u0026#39;), (\u0026#39;usa\u0026#39;, \u0026#39;chicago\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;hamburg\u0026#39;); Spark SQL\rStart spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table and insert data.\nCREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) TBLPROPERTIES ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;new york\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;berlin\u0026#39;), (\u0026#39;usa\u0026#39;, \u0026#39;chicago\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;hamburg\u0026#39;); Now let\u0026rsquo;s query this Paimon table with Iceberg connector.\nFlink SQL\rCREATE CATALOG iceberg_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;iceberg\u0026#39;, \u0026#39;catalog-type\u0026#39; = \u0026#39;hadoop\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;/iceberg\u0026#39;, \u0026#39;cache-enabled\u0026#39; = \u0026#39;false\u0026#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL\rSELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* germany berlin germany hamburg */ Let\u0026rsquo;s insert more data and query again.\nFlink SQL\rINSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;houston\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;munich\u0026#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | munich | | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL\rINSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;houston\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;munich\u0026#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* germany munich germany berlin germany hamburg */ Primary Key Tables\r#\rFlink SQL\rCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39;, \u0026#39;compaction.optimization-interval\u0026#39; = \u0026#39;1ms\u0026#39; -- ATTENTION: this option is only for testing, see \u0026#34;timeliness\u0026#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)), (2, \u0026#39;COMPLETED\u0026#39;, 200.0), (3, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)); CREATE CATALOG iceberg_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;iceberg\u0026#39;, \u0026#39;catalog-type\u0026#39; = \u0026#39;hadoop\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;/iceberg\u0026#39;, \u0026#39;cache-enabled\u0026#39; = \u0026#39;false\u0026#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;COMPLETED\u0026#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 1 | COMPLETED | 100.0 | | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ Spark SQL\rStart spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.\nCREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;order_id\u0026#39;, \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39;, \u0026#39;compaction.optimization-interval\u0026#39; = \u0026#39;1ms\u0026#39; -- ATTENTION: this option is only for testing, see \u0026#34;timeliness\u0026#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)), (2, \u0026#39;COMPLETED\u0026#39;, 200.0), (3, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* 2 COMPLETED 200.0 */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;COMPLETED\u0026#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* 2 COMPLETED 200.0 1 COMPLETED 100.0 */ Paimon primary key tables organize data files as LSM trees, so data files must be merged in memory before querying. However, Iceberg readers are not able to merge data files, so they can only query data files on the highest level of LSM trees. Data files on the highest level are produced by the full compaction process. So to conclude, for primary key tables, Iceberg readers can only query data after full compaction.\nBy default, there is no guarantee on how frequently Paimon will perform full compaction. You can configure the following table option, so that Paimon is forced to perform full compaction after several commits.\nOption\rDefault\rType\rDescription\rcompaction.optimization-interval\r(none)\rDuration\rFull compaction will be constantly triggered per time interval. First compaction after the job starts will always be full compaction.\rfull-compaction.delta-commits\r(none)\rInteger\rFull compaction will be constantly triggered after delta commits. Only implemented in Flink.\rNote that full compaction is a resource-consuming process, so the value of this table option should not be too small. We recommend full compaction to be performed once or twice per hour.\nHive Catalog\r#\rWhen creating Paimon table, set 'metadata.iceberg.storage' = 'hive-catalog'. This option value not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive. This Paimon table can be accessed from Iceberg Hive catalog later.\nTo provide information about Hive metastore, you also need to set some (or all) of the following table options when creating Paimon table.\nOption\rDefault\rType\rDescription\rmetadata.iceberg.uri\rString\rHive metastore uri for Iceberg Hive catalog.\rmetadata.iceberg.hive-conf-dir\rString\rhive-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.hadoop-conf-dir\rString\rhadoop-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.manifest-compression\rsnappy\rString\rCompression for Iceberg manifest files.\rmetadata.iceberg.manifest-legacy-version\rfalse\rBoolean\rShould use the legacy manifest version to generate Iceberg's 1.4 manifest files.\rmetadata.iceberg.hive-client-class\rorg.apache.hadoop.hive.metastore.HiveMetaStoreClient\rString\rHive client class name for Iceberg Hive Catalog.\rmetadata.iceberg.glue.skip-archive\rfalse\rBoolean\rSkip archive for AWS Glue catalog.\rAWS Glue Catalog\r#\rYou can use Hive Catalog to connect AWS Glue metastore, you can use set 'metadata.iceberg.hive-client-class' to 'com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient'.\nNote: You can use this repo to build the required jar, include it in your path and configure the AWSCatalogMetastoreClient.\nAWS Athena\r#\rAWS Athena may use old manifest reader to read Iceberg manifest by names, we should let Paimon producing legacy Iceberg manifest list file, you can enable: 'metadata.iceberg.manifest-legacy-version'.\nDuckDB\r#\rDuckdb may rely on files placed in the root/data directory, while Paimon is usually placed directly in the root directory, so you can configure this parameter for the table to achieve compatibility: 'data-file.path-directory' = 'data'.\nTrino Iceberg\r#\rIn this example, we use Trino Iceberg connector to access Paimon table through Iceberg Hive catalog. Before trying out this example, make sure that you have configured Trino Iceberg connector. See Trino\u0026rsquo;s document for more information.\nLet\u0026rsquo;s first create a Paimon table with Iceberg compatibility enabled.\nFlink SQL\rCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.animals ( kind STRING, name STRING ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hive-catalog\u0026#39;, \u0026#39;metadata.iceberg.uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); INSERT INTO paimon_catalog.`default`.animals VALUES (\u0026#39;mammal\u0026#39;, \u0026#39;cat\u0026#39;), (\u0026#39;mammal\u0026#39;, \u0026#39;dog\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;snake\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;lizard\u0026#39;); Spark SQL\rStart spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\ --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\ --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\ --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\ --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\ --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\ --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.\nCREATE TABLE paimon_catalog.`default`.animals ( kind STRING, name STRING ) TBLPROPERTIES ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hive-catalog\u0026#39;, \u0026#39;metadata.iceberg.uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); INSERT INTO paimon_catalog.`default`.animals VALUES (\u0026#39;mammal\u0026#39;, \u0026#39;cat\u0026#39;), (\u0026#39;mammal\u0026#39;, \u0026#39;dog\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;snake\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;lizard\u0026#39;); Start Trino using Iceberg catalog and query from Paimon table.\nSELECT * FROM animals WHERE class = \u0026#39;mammal\u0026#39;; /* kind | name --------+------ mammal | cat mammal | dog */ Supported Types\r#\rPaimon Iceberg compatibility currently supports the following data types.\nPaimon Data Type Iceberg Data Type BOOLEAN boolean INT int BIGINT long FLOAT float DOUBLE double DECIMAL decimal CHAR string VARCHAR string BINARY binary VARBINARY binary DATE date TIMESTAMP* timestamp TIMESTAMP_LTZ* timestamptz ARRAY list MAP map ROW struct *: TIMESTAMP and TIMESTAMP_LTZ type only support precision from 4 to 6\nTable Options\r#\rOptions for Iceberg Compatibility.\nKey\rDefault\rType\rDescription\rmetadata.iceberg.compaction.max.file-num\r50\rInteger\rIf number of small Iceberg manifest metadata files exceeds this limit, always trigger manifest metadata compaction regardless of their total size.\rmetadata.iceberg.compaction.min.file-num\r10\rInteger\rMinimum number of Iceberg manifest metadata files to trigger manifest metadata compaction.\rmetadata.iceberg.database\r(none)\rString\rMetastore database name for Iceberg Catalog. Set this as an iceberg database alias if using a centralized Catalog.\rmetadata.iceberg.delete-after-commit.enabled\rtrue\rBoolean\rWhether to delete old metadata files after each table commit\rmetadata.iceberg.glue.skip-archive\rfalse\rBoolean\rSkip archive for AWS Glue catalog.\rmetadata.iceberg.hadoop-conf-dir\r(none)\rString\rhadoop-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.hive-client-class\r\"org.apache.hadoop.hive.metastore.HiveMetaStoreClient\"\rString\rHive client class name for Iceberg Hive Catalog.\rmetadata.iceberg.hive-conf-dir\r(none)\rString\rhive-conf-dir for Iceberg Hive catalog.\rmetadata.iceberg.manifest-compression\r\"snappy\"\rString\rCompression for Iceberg manifest files.\rmetadata.iceberg.manifest-legacy-version\rfalse\rBoolean\rShould use the legacy manifest version to generate Iceberg's 1.4 manifest files.\rmetadata.iceberg.previous-versions-max\r0\rInteger\rThe number of old metadata files to keep after each table commit\rmetadata.iceberg.storage\rdisabled\rEnum\nWhen set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.\nPossible values:\"disabled\": Disable Iceberg compatibility support.\"table-location\": Store Iceberg metadata in each table's directory.\"hadoop-catalog\": Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog.\"hive-catalog\": Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.\rmetadata.iceberg.table\r(none)\rString\rMetastore table name for Iceberg Catalog.Set this as an iceberg table alias if using a centralized Catalog.\rmetadata.iceberg.uri\r(none)\rString\rHive metastore uri for Iceberg Hive catalog.\r"},{"id":53,"href":"/concepts/spec/manifest/","title":"Manifest","section":"Specification","content":"\rManifest\r#\rManifest List\r#\r├── manifest └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 Manifest List includes meta of several manifest files. Its name contains UUID, it is a avro file, the schema is:\n_FILE_NAME: STRING, manifest file name. _FILE_SIZE: BIGINT, manifest file size. _NUM_ADDED_FILES: BIGINT, number added files in manifest. _NUM_DELETED_FILES: BIGINT, number deleted files in manifest. _PARTITION_STATS: SimpleStats, partition stats, the minimum and maximum values of partition fields in this manifest are beneficial for skipping certain manifest files during queries, it is a SimpleStats. _SCHEMA_ID: BIGINT, schema id when writing this manifest file. Manifest\r#\rManifest includes meta of several data files or changelog files or table-index files. Its name contains UUID, it is an avro file.\nThe changes of the file are saved in the manifest, and the file can be added or deleted. Manifests should be in an orderly manner, and the same file may be added or deleted multiple times. The last version should be read. This design can make commit lighter to support file deletion generated by compaction.\nData Manifest\r#\rData Manifest includes meta of several data files or changelog files.\n├── manifest └── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 The schema is:\n_KIND: TINYINT, ADD or DELETE, _PARTITION: BYTES, partition spec, a BinaryRow. _BUCKET: INT, bucket of this file. _TOTAL_BUCKETS: INT, total buckets when write this file, it is used for verification after bucket changes. _FILE: data file meta. The data file meta is:\n_FILE_NAME: STRING, file name. _FILE_SIZE: BIGINT, file size. _ROW_COUNT: BIGINT, total number of rows (including add \u0026amp; delete) in this file. _MIN_KEY: STRING, the minimum key of this file. _MAX_KEY: STRING, the maximum key of this file. _KEY_STATS: SimpleStats, the statistics of the key. _VALUE_STATS: SimpleStats, the statistics of the value. _MIN_SEQUENCE_NUMBER: BIGINT, the minimum sequence number. _MAX_SEQUENCE_NUMBER: BIGINT, the maximum sequence number. _SCHEMA_ID: BIGINT, schema id when write this file. _LEVEL: INT, level of this file, in LSM. _EXTRA_FILES: ARRAY, extra files for this file, for example, data file index file. _CREATION_TIME: TIMESTAMP_MILLIS, creation time of this file. _DELETE_ROW_COUNT: BIGINT, rowCount = addRowCount + deleteRowCount. _EMBEDDED_FILE_INDEX: BYTES, if data file index is too small, store the index in manifest. _FILE_SOURCE: TINYINT, indicate whether this file is generated as an APPEND or COMPACT file. _VALUE_STATS_COLS: ARRAY, statistical column in metadata. _EXTERNAL_PATH: external path of this file, null if it is in warehouse. Index Manifest\r#\rIndex Manifest includes meta of several table-index files.\n├── manifest └── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 The schema is:\n_KIND: TINYINT, ADD or DELETE, _PARTITION: BYTES, partition spec, a BinaryRow. _BUCKET: INT, bucket of this file. _INDEX_TYPE: STRING, \u0026ldquo;HASH\u0026rdquo; or \u0026ldquo;DELETION_VECTORS\u0026rdquo;. _FILE_NAME: STRING, file name. _FILE_SIZE: BIGINT, file size. _ROW_COUNT: BIGINT, total number of rows. _DELETIONS_VECTORS_RANGES: Metadata only used by \u0026ldquo;DELETION_VECTORS\u0026rdquo;, is an array of deletion vector meta, the schema of each deletion vector meta is: f0: the data file name corresponding to this deletion vector. f1: the starting offset of this deletion vector in the index file. f2: the length of this deletion vector in the index file. _CARDINALITY: the number of deleted rows. Appendix\r#\rSimpleStats\r#\rSimpleStats is nested row, the schema is:\n_MIN_VALUES: BYTES, BinaryRow, the minimum values of the columns. _MAX_VALUES: BYTES, BinaryRow, the maximum values of the columns. _NULL_COUNTS: ARRAY, the number of nulls of the columns. BinaryRow\r#\rBinaryRow is backed by bytes instead of Object. It can significantly reduce the serialization/deserialization of Java objects.\nA Row has two part: Fixed-length part and variable-length part. Fixed-length part contains 1 byte header and null bit set and field values. Null bit set is used for null tracking and is aligned to 8-byte word boundaries. Field values holds fixed-length primitive types and variable-length values which can be stored in 8 bytes inside. If it do not fit the variable-length field, then store the length and offset of variable-length part.\n"},{"id":54,"href":"/primary-key-table/merge-engine/","title":"Merge Engine","section":"Table with PK","content":"\r"},{"id":55,"href":"/cdc-ingestion/mongo-cdc/","title":"Mongo CDC","section":"CDC Ingestion","content":"\rMongo CDC\r#\rPrepare MongoDB Bundled Jar\r#\rflink-sql-connector-mongodb-cdc-*.jar only cdc 3.1+ is supported\nSynchronizing Tables\r#\rBy using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_table \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\ [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table\rThe Paimon table name.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\r--computed_column\rThe definitions of computed columns. The argument field is from MongoDB collection field name. See here for a complete list of configurations. --mongodb_conf\rThe configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database and collection are required configurations, others are optional. See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rHere are a few points to take note of:\nThe mongodb_conf introduces the schema.start.mode parameter on top of the MongoDB CDC source configuration.schema.start.mode provides two modes: dynamic (default) and specified. In dynamic mode, MongoDB schema information is parsed at one level, which forms the basis for schema change evolution. In specified mode, synchronization takes place according to specified criteria. This can be done by configuring field.name to specify the synchronization fields and parser.path to specify the JSON parsing path for those fields. The difference between the two is that the specify mode requires the user to explicitly identify the fields to be used and create a mapping table based on those fields. Dynamic mode, on the other hand, ensures that Paimon and MongoDB always keep the top-level fields consistent, eliminating the need to focus on specific fields. Further processing of the data table is required when using values from nested fields. The mongodb_conf introduces the default.id.generation parameter as an enhancement to the MongoDB CDC source configuration. The default.id.generation setting offers two distinct behaviors: when set to true and when set to false. When default.id.generation is set to true, the MongoDB CDC source adheres to the default _id generation strategy, which involves stripping the outer $oid nesting to provide a more straightforward identifier. This mode simplifies the _id representation, making it more direct and user-friendly. On the contrary, when default.id.generation is set to false, the MongoDB CDC source retains the original _id structure, without any additional processing. This mode offers users the flexibility to work with the raw _id format as provided by MongoDB, preserving any nested elements like $oid. The choice between the two hinges on the user\u0026rsquo;s preference: the former for a cleaner, simplified _id and the latter for a direct representation of MongoDB\u0026rsquo;s _id structure. Operator\rDescription\r$\rThe root element to query. This starts all path expressions.\r@\rThe current node being processed by a filter predicate.\r*\rWildcard. Available anywhere a name or numeric are required.\r..\rDeep scan. Available anywhere a name is required.\r.\rDot-notated child.\r['{name}' (, '{name}')]\rBracket-notated child or children.\r[{number} (, {number})]\rBracket-notated child or children.\r[start:end]\rArray index or indexes.\r[?({expression})]\rFilter expression. Expression must evaluate to a boolean value.\rFunctions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.\nFunction\rDescription\rOutput type\rmin()\rProvides the min value of an array of numbers.\rDouble\rmax()\rProvides the max value of an array of numbers.\rDouble\ravg()\rProvides the average value of an array of numbers.\rDouble\rstddev()\rProvides the standard deviation value of an array of numbers\rDouble\rlength()\rProvides the length of an array\rInteger\rsum()\rProvides the sum value of an array of numbers.\rDouble\rkeys()\rProvides the property keys (An alternative for terminal tilde ~)\rSet\rconcat(X)\rProvides a concatinated version of the path output with a new item.\rlike input\rappend(X)\radd an item to the json path output array\rlike input\rappend(X)\radd an item to the json path output array\rlike input\rfirst()\rProvides the first item of an array\rDepends on the array\rlast()\rProvides the last item of an array\rDepends on the array\rindex(X)\rProvides the item of an array of index: X, if the X is negative, take from backwards\rDepends on the array\rPath Examples\n{ \u0026#34;store\u0026#34;: { \u0026#34;book\u0026#34;: [ { \u0026#34;category\u0026#34;: \u0026#34;reference\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Nigel Rees\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sayings of the Century\u0026#34;, \u0026#34;price\u0026#34;: 8.95 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Evelyn Waugh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sword of Honour\u0026#34;, \u0026#34;price\u0026#34;: 12.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Herman Melville\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Moby Dick\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-553-21311-3\u0026#34;, \u0026#34;price\u0026#34;: 8.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;J. R. R. Tolkien\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The Lord of the Rings\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-395-19395-8\u0026#34;, \u0026#34;price\u0026#34;: 22.99 } ], \u0026#34;bicycle\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;price\u0026#34;: 19.95 } }, \u0026#34;expensive\u0026#34;: 10 } JsonPath\rResult\r$.store.book[*].author\rProvides the min value of an array of numbers.\r$..author\rAll authors.\r$.store.*\rAll things, both books and bicycles.\r$.store..price\rProvides the standard deviation value of an array of numbers.\r$..book[2]\rThe third book.\r$..book[-2]\rThe second to last book.\r$..book[0,1]\rThe first two books.\r$..book[:2]\rAll books from index 0 (inclusive) until index 2 (exclusive).\r$..book[1:2]\rAll books from index 1 (inclusive) until index 2 (exclusive)\r$..book[-2:]\rLast two books\r$..book[2:]\rAll books from index 2 (inclusive) to last\r$..book[?(@.isbn)]\rAll books with an ISBN number\r$.store.book[?(@.price \u0026lt; 10)]\rAll books in store cheaper than 10\r$..book[?(@.price \u0026lt;= $['expensive'])]\rAll books in store that are not \"expensive\"\r$..book[?(@.author =~ /.*REES/i)]\rAll books matching regex (ignore case)\r$..*\rGive me every thing\r$..book.length()\rThe number of books\rThe synchronized table is required to have its primary key set as _id. This is because MongoDB\u0026rsquo;s change events are recorded before updates in messages. Consequently, we can only convert them into Flink\u0026rsquo;s UPSERT change log stream. The upstart stream demands a unique key, which is why we must declare _id as the primary key. Declaring other columns as primary keys is not feasible, as delete operations only encompass the _id and sharding key, excluding other keys and values.\nMongoDB Change Streams are designed to return simple JSON documents without any data type definitions. This is because MongoDB is a document-oriented database, and one of its core features is the dynamic schema, where documents can contain different fields, and the data types of fields can be flexible. Therefore, the absence of data type definitions in Change Streams is to maintain this flexibility and extensibility. For this reason, we have set all field data types for synchronizing MongoDB to Paimon as String to address the issue of not being able to obtain data types.\nIf the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from MongoDB collection.\nExample 1: synchronize collection into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --mongodb_conf collection=source_table1 \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: Synchronize collection into a Paimon table according to the specified field mapping.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --mongodb_conf collection=source_table1 \\ --mongodb_conf schema.start.mode=specified \\ --mongodb_conf field.name=_id,name,description \\ --mongodb_conf parser.path=$._id,$.name,$.description \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronizing Databases\r#\rBy using MongoDBSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MongoDB database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\ [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\ [--including_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table_prefix\rThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".\r--table_suffix\rThe suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".\r--including_tables\rIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.\r--excluding_tables\rIt is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\rIf the keys are not in source table, the sink table won't set partition keys.\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\rIf the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.\rOtherwise, the sink table won't set primary keys.\r--mongodb_conf\rThe configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database are required configurations, others are optional. See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rAll collections to be synchronized need to set _id as the primary key. For each MongoDB collection to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MongoDB collection. If the Paimon table already exists, its schema will be compared against the schema of all specified MongoDB collection. Any MongoDB tables created after the commencement of the task will automatically be included.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Example 2: Synchronize the specified table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables \u0026#39;product|user|address|order|custom\u0026#39; "},{"id":56,"href":"/program-api/python-api/","title":"Python API","section":"Program API","content":"\rJava-based Implementation For Python API\r#\rPython SDK has defined Python API for Paimon. Currently, there is only a Java-based implementation.\nJava-based implementation will launch a JVM and use py4j to execute Java code to read and write Paimon table.\nEnvironment Settings\r#\rSDK Installing\r#\rSDK is published at pypaimon. You can install by\npip install pypaimon Java Runtime Environment\r#\rThis SDK needs JRE 1.8. After installing JRE, make sure that at least one of the following conditions is met:\njava command is available. You can verify it by java -version. JAVA_HOME and PATH variables are set correctly. For example, you can set: export JAVA_HOME=/path/to/java-directory export PATH=$JAVA_HOME/bin:$PATH Set Environment Variables\r#\rBecause we need to launch a JVM to access Java code, JVM environment need to be set. Besides, the java code need Hadoop dependencies, so hadoop environment should be set.\nJava classpath\r#\rThe package has set necessary paimon core dependencies (Local/Hadoop FileIO, Avro/Orc/Parquet format support and FileSystem/Jdbc/Hive catalog), so If you just test codes in local or in hadoop environment, you don\u0026rsquo;t need to set classpath.\nIf you need other dependencies such as OSS/S3 filesystem jars, or special format and catalog ,please prepare jars and set classpath via one of the following ways:\nSet system environment variable: export _PYPAIMON_JAVA_CLASSPATH=/path/to/jars/* Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JAVA_CLASSPATH] = \u0026#39;/path/to/jars/*\u0026#39; JVM args (optional)\r#\rYou can set JVM args via one of the following ways:\nSet system environment variable: export _PYPAIMON_JVM_ARGS='arg1 arg2 ...' Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JVM_ARGS] = \u0026#39;arg1 arg2 ...\u0026#39; Hadoop classpath\r#\rIf the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, then you don\u0026rsquo;t need to set hadoop.\nOtherwise, you should set hadoop classpath via one of the following ways:\nSet system environment variable: export _PYPAIMON_HADOOP_CLASSPATH=/path/to/jars/* Set environment variable in Python code: import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_HADOOP_CLASSPATH] = \u0026#39;/path/to/jars/*\u0026#39; If you just want to test codes in local, we recommend to use Flink Pre-bundled hadoop jar.\nCreate Catalog\r#\rBefore coming into contact with the Table, you need to create a Catalog.\nfrom pypaimon.py4j import Catalog # Note that keys and values are all string catalog_options = { \u0026#39;metastore\u0026#39;: \u0026#39;filesystem\u0026#39;, \u0026#39;warehouse\u0026#39;: \u0026#39;file:///path/to/warehouse\u0026#39; } catalog = Catalog.create(catalog_options) Create Database \u0026amp; Table\r#\rYou can use the catalog to create table for writing data.\nCreate Database (optional)\r#\rTable is located in a database. If you want to create table in a new database, you should create it.\ncatalog.create_database( name=\u0026#39;database_name\u0026#39;, ignore_if_exists=True, # If you want to raise error if the database exists, set False properties={\u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;} # optional database properties ) Create Schema\r#\rTable schema contains fields definition, partition keys, primary keys, table options and comment. The field definition is described by pyarrow.Schema. All arguments except fields definition are optional.\nGenerally, there are two ways to build pyarrow.Schema.\nFirst, you can use pyarrow.schema method directly, for example:\nimport pyarrow as pa from pypaimon import Schema pa_schema = pa.schema([ (\u0026#39;dt\u0026#39;, pa.string()), (\u0026#39;hh\u0026#39;, pa.string()), (\u0026#39;pk\u0026#39;, pa.int64()), (\u0026#39;value\u0026#39;, pa.string()) ]) schema = Schema( pa_schema=pa_schema, partition_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], primary_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;pk\u0026#39;], options={\u0026#39;bucket\u0026#39;: \u0026#39;2\u0026#39;}, comment=\u0026#39;my test table\u0026#39; ) See Data Types for all supported pyarrow-to-paimon data types mapping.\nSecond, if you have some Pandas data, the pa_schema can be extracted from DataFrame:\nimport pandas as pd import pyarrow as pa from pypaimon import Schema # Example DataFrame data data = { \u0026#39;dt\u0026#39;: [\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;], \u0026#39;hh\u0026#39;: [\u0026#39;12\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;20\u0026#39;], \u0026#39;pk\u0026#39;: [1, 2, 3], \u0026#39;value\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], } dataframe = pd.DataFrame(data) # Get Paimon Schema record_batch = pa.RecordBatch.from_pandas(dataframe) schema = Schema( pa_schema=record_batch.schema, partition_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], primary_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;pk\u0026#39;], options={\u0026#39;bucket\u0026#39;: \u0026#39;2\u0026#39;}, comment=\u0026#39;my test table\u0026#39; ) Create Table\r#\rAfter building table schema, you can create corresponding table:\nschema = ... catalog.create_table( identifier=\u0026#39;database_name.table_name\u0026#39;, schema=schema, ignore_if_exists=True # If you want to raise error if the table exists, set False ) Get Table\r#\rThe Table interface provides tools to read and write table.\ntable = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) Batch Read\r#\rSet Read Parallelism\r#\rTableRead interface provides parallelly reading for multiple splits. You can set 'max-workers': 'N' in catalog_options to set thread numbers for reading splits. max-workers is 1 by default, that means TableRead will read splits sequentially if you doesn\u0026rsquo;t set max-workers.\nGet ReadBuilder and Perform pushdown\r#\rA ReadBuilder is used to build reading utils and perform filter and projection pushdown.\ntable = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) read_builder = table.new_read_builder() You can use PredicateBuilder to build filters and pushdown them by ReadBuilder:\n# Example filter: (\u0026#39;f0\u0026#39; \u0026lt; 3 OR \u0026#39;f1\u0026#39; \u0026gt; 6) AND \u0026#39;f3\u0026#39; = \u0026#39;A\u0026#39; predicate_builder = read_builder.new_predicate_builder() predicate1 = predicate_builder.less_than(\u0026#39;f0\u0026#39;, 3) predicate2 = predicate_builder.greater_than(\u0026#39;f1\u0026#39;, 6) predicate3 = predicate_builder.or_predicates([predicate1, predicate2]) predicate4 = predicate_builder.equal(\u0026#39;f3\u0026#39;, \u0026#39;A\u0026#39;) predicate_5 = predicate_builder.and_predicates([predicate3, predicate4]) read_builder = read_builder.with_filter(predicate_5) See Predicate for all supported filters and building methods.\nYou can also pushdown projection by ReadBuilder:\n# select f3 and f2 columns read_builder = read_builder.with_projection([\u0026#39;f3\u0026#39;, \u0026#39;f2\u0026#39;]) Scan Plan\r#\rThen you can step into Scan Plan stage to get splits:\ntable_scan = read_builder.new_scan() splits = table_scan.plan().splits() Read Splits\r#\rFinally, you can read data from the splits to various data format.\nApache Arrow\r#\rThis requires pyarrow to be installed.\nYou can read all the data into a pyarrow.Table:\ntable_read = read_builder.new_read() pa_table = table_read.to_arrow(splits) print(pa_table) # pyarrow.Table # f0: int32 # f1: string # ---- # f0: [[1,2,3],[4,5,6],...] # f1: [[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;],[\u0026#34;d\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;f\u0026#34;],...] You can also read data into a pyarrow.RecordBatchReader and iterate record batches:\ntable_read = read_builder.new_read() for batch in table_read.to_arrow_batch_reader(splits): print(batch) # pyarrow.RecordBatch # f0: int32 # f1: string # ---- # f0: [1,2,3] # f1: [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;] Pandas\r#\rThis requires pandas to be installed.\nYou can read all the data into a pandas.DataFrame:\ntable_read = read_builder.new_read() df = table_read.to_pandas(splits) print(df) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... DuckDB\r#\rThis requires duckdb to be installed.\nYou can convert the splits into an in-memory DuckDB table and query it:\ntable_read = read_builder.new_read() duckdb_con = table_read.to_duckdb(splits, \u0026#39;duckdb_table\u0026#39;) print(duckdb_con.query(\u0026#34;SELECT * FROM duckdb_table\u0026#34;).fetchdf()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... print(duckdb_con.query(\u0026#34;SELECT * FROM duckdb_table WHERE f0 = 1\u0026#34;).fetchdf()) # f0 f1 # 0 1 a Ray\r#\rThis requires ray to be installed.\nYou can convert the splits into a Ray dataset and handle it by Ray API:\ntable_read = read_builder.new_read() ray_dataset = table_read.to_ray(splits) print(ray_dataset) # MaterializedDataset(num_blocks=1, num_rows=9, schema={f0: int32, f1: string}) print(ray_dataset.take(3)) # [{\u0026#39;f0\u0026#39;: 1, \u0026#39;f1\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;f0\u0026#39;: 2, \u0026#39;f1\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;f0\u0026#39;: 3, \u0026#39;f1\u0026#39;: \u0026#39;c\u0026#39;}] print(ray_dataset.to_pandas()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... Batch Write\r#\rPaimon table write is Two-Phase Commit, you can write many times, but once committed, no more data can be written.\nCurrently, Python SDK doesn't support writing primary key table with `bucket=-1`.\rtable = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) # 1. Create table write and commit write_builder = table.new_batch_write_builder() table_write = write_builder.new_write() table_commit = write_builder.new_commit() # 2. Write data. Support 3 methods: # 2.1 Write pandas.DataFrame dataframe = ... table_write.write_pandas(dataframe) # 2.2 Write pyarrow.Table pa_table = ... table_write.write_arrow(pa_table) # 2.3 Write pyarrow.RecordBatch record_batch = ... table_write.write_arrow_batch(record_batch) # 3. Commit data commit_messages = table_write.prepare_commit() table_commit.commit(commit_messages) # 4. Close resources table_write.close() table_commit.close() By default, the data will be appended to table. If you want to overwrite table, you should use TableWrite#overwrite API:\n# overwrite whole table write_builder.overwrite() # overwrite partition \u0026#39;dt=2024-01-01\u0026#39; write_builder.overwrite({\u0026#39;dt\u0026#39;: \u0026#39;2024-01-01\u0026#39;}) Data Types\r#\rpyarrow Paimon pyarrow.int8() TINYINT pyarrow.int16() SMALLINT pyarrow.int32() INT pyarrow.int64() BIGINT pyarrow.float16() pyarrow.float32() FLOAT pyarrow.float64() DOUBLE pyarrow.string() STRING pyarrow.boolean() BOOLEAN Predicate\r#\rPredicate kind Predicate method p1 and p2 PredicateBuilder.and_predicates([p1, p2]) p1 or p2 PredicateBuilder.or_predicates([p1, p2]) f = literal PredicateBuilder.equal(f, literal) f != literal PredicateBuilder.not_equal(f, literal) f \u0026lt; literal PredicateBuilder.less_than(f, literal) f \u0026lt;= literal PredicateBuilder.less_or_equal(f, literal) f \u0026gt; literal PredicateBuilder.greater_than(f, literal) f \u0026gt;= literal PredicateBuilder.greater_or_equal(f, literal) f is null PredicateBuilder.is_null(f) f is not null PredicateBuilder.is_not_null(f) f.startswith(literal) PredicateBuilder.startswith(f, literal) f.endswith(literal) PredicateBuilder.endswith(f, literal) f.contains(literal) PredicateBuilder.contains(f, literal) f is in [l1, l2] PredicateBuilder.is_in(f, [l1, l2]) f is not in [l1, l2] PredicateBuilder.is_not_in(f, [l1, l2]) lower \u0026lt;= f \u0026lt;= upper PredicateBuilder.between(f, lower, upper) "},{"id":57,"href":"/flink/sql-query/","title":"SQL Query","section":"Engine Flink","content":"\rSQL Query\r#\rJust like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query\r#\rPaimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; Batch Time Travel\r#\rPaimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nFlink (dynamic option)\r-- read the snapshot with id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read the snapshot from specified timestamp in unix milliseconds SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read the snapshot from specified timestamp string ,it will be automatically converted to timestamp in unix milliseconds -- Supported formats include：yyyy-MM-dd, yyyy-MM-dd HH:mm:ss, yyyy-MM-dd HH:mm:ss.SSS, use default local time zone SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp\u0026#39; = \u0026#39;2023-12-09 23:09:12\u0026#39;) */; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39; = \u0026#39;my-tag\u0026#39;) */; -- read the snapshot from watermark, will match the first snapshot after the watermark SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.watermark\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Flink 1.18\u0026#43;\rFlink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY Batch Incremental\r#\rRead incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n\u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3. -- incremental between snapshot ids SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; -- incremental between snapshot time mills SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;1692169000000,1692169900000\u0026#39;) */; SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;2025-03-12 00:00:00,2025-03-12 00:08:00\u0026#39;) */; By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nIn Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can use audit_log table:\nSELECT * FROM t$audit_log /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; Batch Incremental between Auto-created Tags\r#\rYou can use incremental-between to query incremental changes between two tags. But for auto-created tag, the tag may not be created in-time because of data delay.\nFor example, assume that tags \u0026lsquo;2024-12-01\u0026rsquo;, \u0026lsquo;2024-12-02\u0026rsquo; and \u0026lsquo;2024-12-04\u0026rsquo; are auto created daily. Data for 12/03 are delayed and ingested with data for 12/04. Now if you want to query the incremental changes between tags, and you don\u0026rsquo;t know the tag of 12/03 is not created, you will use incremental-between with \u0026lsquo;2024-12-01,2024-12-02\u0026rsquo;, \u0026lsquo;2024-12-02,2024-12-03\u0026rsquo; and \u0026lsquo;2024-12-03,2024-12-04\u0026rsquo; respectively, then you will get an error that the tag \u0026lsquo;2024-12-03\u0026rsquo; doesn\u0026rsquo;t exist.\nWe introduced a new option incremental-to-auto-tag for this scenario. You can only specify the end tag, and Paimon will find an earlier tag and return changes between them. If the tag doesn\u0026rsquo;t exist or the earlier tag doesn\u0026rsquo;t exist, return empty.\nFor example, when you query \u0026lsquo;incremental-to-auto-tag=2024-12-01\u0026rsquo; or \u0026lsquo;incremental-to-auto-tag=2024-12-03\u0026rsquo;, the result is empty; Query \u0026lsquo;incremental-to-auto-tag=2024-12-02\u0026rsquo;, the result is change between 12/01 and 12/02; Query \u0026lsquo;incremental-to-auto-tag=2024-12-04\u0026rsquo;, the result is change between 12/02 and 12/04.\nStreaming Query\r#\rBy default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest changes.\nPaimon by default ensures that your startup is properly processed with all data included.\nPaimon Source in Streaming mode is unbounded, like a queue that never ends.\r-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; You can also do streaming read without the snapshot data, you can use latest scan mode:\n-- Continuously reads latest changes without producing a snapshot at the beginning. SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39; = \u0026#39;latest\u0026#39;) */; Streaming Time Travel\r#\rIf you only want to process data for today and beyond, you can do so with partitioned filters:\nSELECT * FROM t WHERE dt \u0026gt; \u0026#39;2023-06-26\u0026#39;; If it\u0026rsquo;s not a partitioned table, or you can\u0026rsquo;t filter by partition, you can use Time travel\u0026rsquo;s stream read.\nFlink (dynamic option)\r-- read changes from snapshot id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read changes from snapshot specified timestamp SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read snapshot id 1L upon first startup, and continue to read the changes SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39;=\u0026#39;from-snapshot-full\u0026#39;,\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; Flink 1.18\u0026#43;\rFlink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY Time travel\u0026rsquo;s stream read rely on snapshots, but by default, snapshot only retains data within 1 hour, which can prevent you from reading older incremental data. So, Paimon also provides another mode for streaming reads, scan.file-creation-time-millis, which provides a rough filtering to retain files generated after timeMillis.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;scan.file-creation-time-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Read Overwrite\r#\rStreaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.\nRead Parallelism\r#\rBy default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets, but not greater than scan.infer-parallelism.max.\nDisable scan.infer-parallelism, global parallelism will be used for reads.\nYou can also manually specify the parallelism from scan.parallelism.\nKey\rDefault\rType\rDescription\rscan.infer-parallelism\rtrue\rBoolean\rIf it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).\rscan.infer-parallelism.max\r1024\rInteger\rIf scan.infer-parallelism is true, limit the parallelism of source through this option.\rscan.parallelism\r(none)\rInteger\rDefine a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.\rQuery Optimization\r#\rBatch\rStreaming\rIt is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n= \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., PRIMARY KEY (catalog_id, order_id) NOT ENFORCED -- composite primary key ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "},{"id":58,"href":"/spark/sql-query/","title":"SQL Query","section":"Engine Spark","content":"\rSQL Query\r#\rJust like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query\r#\rPaimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- read all columns SELECT * FROM t; Paimon also supports reading some hidden metadata columns, currently supporting the following columns:\n__paimon_file_path: the file path of the record. __paimon_partition: the partition of the record. __paimon_bucket: the bucket of the record. __paimon_row_index: the row index of the record. -- read all columns and the corresponding file path, partition, bucket, rowIndex of the record SELECT *, __paimon_file_path, __paimon_partition, __paimon_bucket, __paimon_row_index FROM t; Batch Time Travel\r#\rPaimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nRequires Spark 3.3+.\nyou can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:\n-- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- read the snapshot from specified timestamp SELECT * FROM t TIMESTAMP AS OF \u0026#39;2023-06-01 00:00:00.123\u0026#39;; -- read the snapshot from specified timestamp in unix seconds SELECT * FROM t TIMESTAMP AS OF 1678883047; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t VERSION AS OF \u0026#39;my-tag\u0026#39;; -- read the snapshot from specified watermark. will match the first snapshot after the watermark SELECT * FROM t VERSION AS OF \u0026#39;watermark-1678883047356\u0026#39;; If tag's name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if\ryou have a tag named '1' based on snapshot 2, the statement `SELECT * FROM t VERSION AS OF '1'` actually queries snapshot 2\rinstead of snapshot 1.\rBatch Incremental\r#\rRead incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n\u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3. By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nPaimon supports that use Spark SQL to do the incremental query that implemented by Spark Table Valued Function.\n-- read the incremental data between snapshot id 12 and snapshot id 20. SELECT * FROM paimon_incremental_query(\u0026#39;tableName\u0026#39;, 12, 20); -- read the incremental data between ts 1692169900000 and ts 1692169900000. SELECT * FROM paimon_incremental_between_timestamp(\u0026#39;tableName\u0026#39;, \u0026#39;1692169000000\u0026#39;, \u0026#39;1692169900000\u0026#39;); SELECT * FROM paimon_incremental_between_timestamp(\u0026#39;tableName\u0026#39;, \u0026#39;2025-03-12 00:00:00\u0026#39;, \u0026#39;2025-03-12 00:08:00\u0026#39;); -- read the incremental data to tag \u0026#39;2024-12-04\u0026#39;. -- Paimon will find an earlier tag and return changes between them. -- If the tag doesn\u0026#39;t exist or the earlier tag doesn\u0026#39;t exist, return empty. SELECT * FROM paimon_incremental_to_auto_tag(\u0026#39;tableName\u0026#39;, \u0026#39;2024-12-04\u0026#39;); In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can query audit_log table.\nStreaming Query\r#\rPaimon currently supports Spark 3.3+ for streaming read.\rPaimon supports rich scan mode for streaming read. There is a list:\nScan Mode\rDescription\rlatest\rFor streaming sources, continuously reads latest changes without producing a snapshot at the beginning. latest-full\rFor streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes.\rfrom-timestamp\rFor streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. from-snapshot\rFor streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. from-snapshot-full\rFor streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes.\rdefault\rIt is equivalent to from-snapshot if \"scan.snapshot-id\" is specified. It is equivalent to from-timestamp if \"timestamp-millis\" is specified. Or, It is equivalent to latest-full.\rA simple example with default scan mode:\n// no any scan-related configs are provided, that will use latest-full scan mode. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming also supports a variety of streaming read modes, it can support many triggers and many read limits.\nThese read limits are supported:\nKey\rDefault\rType\rDescription\rread.stream.maxFilesPerTrigger\r(none)\rInteger\rThe maximum number of files returned in a single batch.\rread.stream.maxBytesPerTrigger\r(none)\rLong\rThe maximum number of bytes returned in a single batch.\rread.stream.maxRowsPerTrigger\r(none)\rLong\rThe maximum number of rows returned in a single batch.\rread.stream.minRowsPerTrigger\r(none)\rLong\rThe minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.\rread.stream.maxTriggerDelayMs\r(none)\rLong\rThe maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.\rExample: One\nUse org.apache.spark.sql.streaming.Trigger.AvailableNow() and maxBytesPerTrigger defined by paimon.\n// Trigger.AvailableNow()) processes all available data at the start // of the query in one or multiple batches, then terminates the query. // That set read.stream.maxBytesPerTrigger to 128M means that each // batch processes a maximum of 128 MB of data. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.maxBytesPerTrigger\u0026#34;, \u0026#34;134217728\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .trigger(Trigger.AvailableNow()) .start() Example: Two\nUse org.apache.spark.sql.connector.read.streaming.ReadMinRows.\n// It will not trigger a batch until there are more than 5,000 pieces of data, // unless the interval between the two batches is more than 300 seconds. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.minRowsPerTrigger\u0026#34;, \u0026#34;5000\u0026#34;) .option(\u0026#34;read.stream.maxTriggerDelayMs\u0026#34;, \u0026#34;300000\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming supports read row in the form of changelog (add rowkind column in row to represent its change type) in two ways:\nDirect streaming read with the system audit_log table Set read.changelog to true (default is false), then streaming read with table location Example:\n// Option 1 val query1 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .table(\u0026#34;`table_name$audit_log`\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() // Option 2 val query2 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.changelog\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() /* +I 1 Hi +I 2 Hello */ Query Optimization\r#\rIt is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n= \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;catalog_id,order_id\u0026#39; ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "},{"id":59,"href":"/append-table/update/","title":"Update","section":"Table w/o PK","content":"\rUpdate\r#\rNow, only Spark SQL supports DELETE \u0026amp; UPDATE, you can take a look at Spark Write.\nExample:\nDELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Update append table has two modes:\nCOW (Copy on Write): search for the hit files and then rewrite each file to remove the data that needs to be deleted from the files. This operation is costly. MOW (Merge on Write): By specifying 'deletion-vectors.enabled' = 'true', the Deletion Vectors mode can be enabled. Only marks certain records of the corresponding file for deletion and writes the deletion file, without rewriting the entire file. "},{"id":60,"href":"/append-table/bucketed/","title":"Bucketed","section":"Table w/o PK","content":"\rBucketed Append\r#\rYou can define the bucket and bucket-key to get a bucketed append table.\nExample to create bucketed append table:\nFlink\rCREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; ); Streaming\r#\rAn ordinary Append table has no strict ordering guarantees for its streaming writes and reads, but there are some cases where you need to define a key similar to Kafka\u0026rsquo;s.\nEvery record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue.\nCompaction in Bucket\r#\rBy default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\nKey\rDefault\rType\rDescription\rwrite-only\rfalse\rBoolean\rIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.\rcompaction.min.file-num\r5\rInteger\rFor file set [f_0,...,f_N], the minimum file number to trigger a compaction for append table.\rfull-compaction.delta-commits\r(none)\rInteger\rFull compaction will be constantly triggered after delta commits.\rStreaming Read Order\r#\rFor streaming reads, records are produced in the following order:\nFor any two records from two different partitions If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first. For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them. Watermark Definition\r#\rYou can define watermark for reading Paimon tables:\nCREATE TABLE t ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(`user`) FROM TABLE( TUMBLE(TABLE t, DESCRIPTOR(order_time), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:\nKey\rDefault\rType\rDescription\rscan.watermark.alignment.group\r(none)\rString\rA group of sources to align watermarks.\rscan.watermark.alignment.max-drift\r(none)\rDuration\rMaximal drift to align watermarks, before we pause consuming from the source/task/partition.\rBounded Stream\r#\rStreaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.\nWatermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.\nCREATE TABLE kafka_table ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(\u0026#39;scan.bounded.watermark\u0026#39;=\u0026#39;...\u0026#39;) */; Batch\r#\rBucketed table can be used to avoid shuffle if necessary in batch query, for example, you can use the following Spark SQL to read a Paimon table:\nSET spark.sql.sources.v2.bucketing.enabled = true; CREATE TABLE FACT_TABLE (order_id INT, f1 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;order_id\u0026#39;); CREATE TABLE DIM_TABLE (order_id INT, f2 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;primary-key\u0026#39; = \u0026#39;order_id\u0026#39;); SELECT * FROM FACT_TABLE JOIN DIM_TABLE on t1.order_id = t4.order_id; The spark.sql.sources.v2.bucketing.enabled config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.\nThe costly join shuffle will be avoided if two tables have the same bucketing strategy and same number of buckets.\n"},{"id":61,"href":"/primary-key-table/changelog-producer/","title":"Changelog Producer","section":"Table with PK","content":"\rChangelog Producer\r#\rStreaming write can continuously produce the latest changes for streaming read.\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from table files.\n`changelog-producer` may significantly reduce compaction performance, please do not enable it unless necessary.\rNone\r#\rBy default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided. (You can force removing \u0026ldquo;normalize\u0026rdquo; operator via 'scan.remove-normalize'.)\nInput\r#\rBy specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.\ninput changelog producer can be used when Paimon writers\u0026rsquo; inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup\r#\rIf your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing (You can also enable Async Compaction).\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\nOption\rDefault\rType\rDescription\rlookup.cache-file-retention\r1 h\rDuration\rThe cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.\rlookup.cache-max-disk-size\runlimited\rMemorySize\rMax disk size for lookup cache, you can use this option to limit the use of local disks.\rlookup.cache-max-memory-size\r256 mb\rMemorySize\rMax memory size for lookup cache.\rLookup changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\n(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).\nFull Compaction\r#\rYou can also consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer to generate changelog, and is more suitable for scenarios with large latency (For example, 30 minutes).\nBy specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions. By specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a changelog. Generally speaking, the cost and consumption of full compaction are high, so we recommend using 'lookup' changelog producer.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as\refficient as the input changelog producer and the latency to produce changelog might be high.\rFull-compaction changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\nChangelog Merging\r#\rFor input, lookup, full-compaction \u0026lsquo;changelog-producer\u0026rsquo;.\nIf Flink\u0026rsquo;s checkpoint interval is short (for example, 30 seconds) and the number of buckets is large, each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.\nIn order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.\n"},{"id":62,"href":"/flink/consumer-id/","title":"Consumer ID","section":"Engine Flink","content":"\rConsumer ID\r#\rConsumer id can help you accomplish the following two things:\nSafe consumption: When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration. Resume from breakpoint: When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state. Usage\r#\rYou can specify the consumer-id when streaming read table.\nThe consumer will prevent expiration of the snapshot. In order to prevent too many snapshots caused by mistakes, you need to specify 'consumer.expiration-time' to manage the lifetime of consumers.\nALTER TABLE t SET (\u0026#39;consumer.expiration-time\u0026#39; = \u0026#39;1 d\u0026#39;); Then, restart streaming write job of this table, expiration of consumers will be triggered in writing job.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;, \u0026#39;consumer.mode\u0026#39; = \u0026#39;at-least-once\u0026#39;) */; Ignore Progress\r#\rSometimes, you only want the feature of \u0026lsquo;Safe Consumption\u0026rsquo;. You want to get a new snapshot progress when restarting the stream consumption job , you can enable the 'consumer.ignore-progress' option.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;, \u0026#39;consumer.ignore-progress\u0026#39; = \u0026#39;true\u0026#39;) */; The startup of this job will retrieve the snapshot that should be read again.\nConsumer Mode\r#\rBy default, the consumption of snapshots is strictly aligned within the checkpoint to make \u0026lsquo;Resume from breakpoint\u0026rsquo; feature exactly-once.\nBut in some scenarios where you don\u0026rsquo;t need \u0026lsquo;Resume from breakpoint\u0026rsquo;, or you don\u0026rsquo;t need strict \u0026lsquo;Resume from breakpoint\u0026rsquo;, you can consider enabling 'consumer.mode' = 'at-least-once' mode. This mode:\nAllow readers consume snapshots at different rates and record the slowest snapshot-id among all readers into the consumer. It doesn\u0026rsquo;t affect the checkpoint time and have good performance. This mode can provide more capabilities, such as watermark alignment. About `'consumer.mode'`, since the implementation of `exactly-once` mode and `at-least-once` mode are completely\rdifferent, the state of flink is incompatible and cannot be restored from the state when switching modes.\rReset Consumer\r#\rYou can reset or delete a consumer with a given consumer ID and next snapshot ID and delete a consumer with a given consumer ID. First, you need to stop the streaming task using this consumer ID, and then execute the reset consumer action job.\nRun the following command:\nFlink SQL\rCALL sys.reset_consumer( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, consumer_id =\u0026gt; \u0026#39;consumer_id\u0026#39;, next_snapshot_id =\u0026gt; \u0026lt;snapshot_id\u0026gt; ); -- No next_snapshot_id if you want to delete the consumer Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ reset-consumer \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --consumer_id \u0026lt;consumer-id\u0026gt; \\ [--next_snapshot \u0026lt;next-snapshot-id\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] ## No next_snapshot if you want to delete the consumer Clear Consumers\r#\rYou can clear consumers in bulk with a given including consumers and excluding consumers(accept regular expression).\nRun the following command:\nFlink SQL\rCALL sys.clear_consumers( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, `including_consumers` =\u0026gt; \u0026#39;including_consumers\u0026#39;, `excluding_consumers` =\u0026gt; \u0026#39;excluding_consumers\u0026#39; ); -- No including_consumers if you want to clear all consumers except excluding_consumers in the table Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ clear_consumers \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--including_consumers \u0026lt;including-consumers\u0026gt;] \\ [--excluding_consumers \u0026lt;excluding-consumers\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] ## No including_consumers if you want to clear all consumers except excluding_consumers in the table "},{"id":63,"href":"/concepts/spec/datafile/","title":"DataFile","section":"Specification","content":"\rDataFile\r#\rPartition\r#\rConsider a Partition table via Flink SQL:\nCREATE TABLE part_t ( f0 INT, f1 STRING, dt STRING ) PARTITIONED BY (dt); INSERT INTO part_t VALUES (1, \u0026#39;11\u0026#39;, \u0026#39;20240514\u0026#39;); The file system will be:\npart_t ├── dt=20240514 │ └── bucket-0 │ └── data-ca1c3c38-dc8d-4533-949b-82e195b41bd4-0.orc ├── manifest │ ├── manifest-08995fe5-c2ac-4f54-9a5f-d3af1fcde41d-0 │ ├── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-0 │ └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 Paimon adopts the same partitioning concept as Apache Hive to separate data. The files of the partition will be placed in a separate partition directory.\nBucket\r#\rThe storage of all Paimon tables relies on buckets, and data files are stored in the bucket directory. The relationship between various table types and buckets in Paimon:\nPrimary Key Table: bucket = -1: Default mode, the dynamic bucket mode records which bucket the key corresponds to through the index files. The index records the correspondence between the hash value of the primary-key and the bucket. bucket = 10: The data is distributed to the corresponding buckets according to the hash value of bucket key ( default is primary key). Append Table: bucket = -1: Default mode, ignoring bucket concept, although all data is written to bucket-0, the parallelism of reads and writes is unrestricted. bucket = 10: You need to define bucket-key too, the data is distributed to the corresponding buckets according to the hash value of bucket key. Data File\r#\rThe name of data file is data-${uuid}-${id}.${format}. For the append table, the file stores the data of the table without adding any new columns. But for the primary key table, each row of data stores additional system columns:\nTable with Primary key Data File\r#\rPrimary key columns, _KEY_ prefix to key columns, this is to avoid conflicts with columns of the table. It\u0026rsquo;s optional, Paimon version 1.0 and above will retrieve the primary key fields from value_columns. _VALUE_KIND: TINYINT, row is deleted or added. Similar to RocksDB, each row of data can be deleted or added, which will be used for updating the primary key table. _SEQUENCE_NUMBER: BIGINT, this number is used for comparison during updates, determining which data came first and which data came later. Value columns. All columns declared in the table. For example, data file for table:\nCREATE TABLE T ( a INT PRIMARY KEY NOT ENFORCED, b INT, c INT ); Its file has 6 columns: _KEY_a, _VALUE_KIND, _SEQUENCE_NUMBER, a, b, c.\nWhen data-file.thin-mode enabled, its file has 5 columns: _VALUE_KIND, _SEQUENCE_NUMBER, a, b, c.\nTable w/o Primary key Data File\r#\rValue columns. All columns declared in the table. For example, data file for table:\nCREATE TABLE T ( a INT, b INT, c INT ); Its file has 3 columns: a, b, c.\nChangelog File\r#\rChangelog file and Data file are exactly the same, it only takes effect on the primary key table. It is similar to the Binlog in a database, recording changes to the data in the table.\n"},{"id":64,"href":"/flink/","title":"Engine Flink","section":"Apache Paimon","content":"\r"},{"id":65,"href":"/maintenance/manage-snapshots/","title":"Manage Snapshots","section":"Maintenance","content":"\rManage Snapshots\r#\rThis section will describe the management and behavior related to snapshots.\nExpire Snapshots\r#\rPaimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\nOption\rRequired\rDefault\rType\rDescription\rsnapshot.time-retained\rNo\r1 h\rDuration\rThe maximum time of completed snapshots to retain.\rsnapshot.num-retained.min\rNo\r10\rInteger\rThe minimum number of completed snapshots to retain. Should be greater than or equal to 1.\rsnapshot.num-retained.max\rNo\rInteger.MAX_VALUE\rInteger\rThe maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.\rsnapshot.expire.execution-mode\rNo\rsync\rEnum\rSpecifies the execution mode of expire.\rsnapshot.expire.limit\rNo\r10\rInteger\rThe maximum number of snapshots allowed to expire at a time.\rWhen the number of snapshots is less than snapshot.num-retained.min, no snapshots will be expired(even the condition snapshot.time-retained meet), after which snapshot.num-retained.max and snapshot.time-retained will be used to control the snapshot expiration until the remaining snapshot meets the condition.\nThe following example show more details(snapshot.num-retained.min is 2, snapshot.time-retained is 1h, snapshot.num-retained.max is 5):\nsnapshot item is described using tuple (snapshotId, corresponding time)\nNew Snapshots\rAll snapshots after expiration check\rexplanation\r(snapshots-1, 2023-07-06 10:00)\r(snapshots-1, 2023-07-06 10:00)\rNo snapshot expired\r(snapshots-2, 2023-07-06 10:20)\r(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20)\rNo snapshot expired\r(snapshots-3, 2023-07-06 10:40)\r(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40)\rNo snapshot expired\r(snapshots-4, 2023-07-06 11:00)\r(snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) No snapshot expired\r(snapshots-5, 2023-07-06 11:20)\r(snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20)\rsnapshot-1 was expired because the condition `snapshot.time-retained` is not met\r(snapshots-6, 2023-07-06 11:30)\r(snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30)\rsnapshot-2 was expired because the condition `snapshot.time-retained` is not met\r(snapshots-7, 2023-07-06 11:35)\r(snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35)\rNo snapshot expired\r(snapshots-8, 2023-07-06 11:36)\r(snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35) (snapshots-8, 2023-07-06 11:36)\rsnapshot-3 was expired because the condition `snapshot.num-retained.max` is not met\rPlease note that too short retain time or too small retain number may result in:\nBatch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files fail to restart. When the job restarts, the snapshot it recorded may have expired. (You can use Consumer Id to protect streaming reading in a small retain time of snapshot expiration). By default, paimon will delete expired snapshots synchronously. When there are too many files that need to be deleted, they may not be deleted quickly and back-pressured to the upstream operator. To avoid this situation, users can use asynchronous expiration mode by setting snapshot.expire.execution-mode to async.\nRollback to Snapshot\r#\rRollback a table to a specific snapshot ID.\nFlink SQL\rRun the following command:\nCALL sys.rollback_to(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, snapshot_id =\u0026gt; \u0026lt;snasphot-id\u0026gt;); Flink Action\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ rollback_to \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --version \u0026lt;snapshot-id\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API\rimport org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback: // snapshot-3 // snapshot-4 // snapshot-5 // snapshot-6 // snapshot-7 table.rollbackTo(5); // after rollback: // snapshot-3 // snapshot-4 // snapshot-5 } } Spark\rRun the following sql:\nCALL sys.rollback(table =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, snapshot =\u0026gt; snasphot_id); Remove Orphan Files\r#\rPaimon files are deleted physically only when expiring snapshots. However, it is possible that some unexpected errors occurred when deleting files, so that there may exist files that are not used by Paimon snapshots (so-called \u0026ldquo;orphan files\u0026rdquo;). You can submit a remove_orphan_files job to clean them:\nSpark SQL/Flink SQL\rCALL sys.remove_orphan_files(`table` =\u0026gt; \u0026#39;my_db.my_table\u0026#39;, [older_than =\u0026gt; \u0026#39;2023-10-31 12:00:00\u0026#39;]) CALL sys.remove_orphan_files(`table` =\u0026gt; \u0026#39;my_db.*\u0026#39;, [older_than =\u0026gt; \u0026#39;2023-10-31 12:00:00\u0026#39;]) Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ remove_orphan_files \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--older_than \u0026lt;timestamp\u0026gt;] \\ [--dry_run \u0026lt;false/true\u0026gt;] \\ [--parallelism \u0026lt;parallelism\u0026gt;] To avoid deleting files that are newly added by other writing jobs, this action only deletes orphan files older than 1 day by default. The interval can be modified by --older_than. For example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ remove_orphan_files \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table T \\ --older_than \u0026#39;2023-10-31 12:00:00\u0026#39; The table can be * to clean all tables in the database.\n"},{"id":66,"href":"/cdc-ingestion/pulsar-cdc/","title":"Pulsar CDC","section":"CDC Ingestion","content":"\rPulsar CDC\r#\rPrepare Pulsar Bundled Jar\r#\rflink-connector-pulsar-*.jar Supported Formats\r#\rFlink provides several Pulsar CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a pulsar topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Pulsar CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\nFormats\rSupported\rCanal CDC\rTrue\rDebezium CDC\rTrue\rMaxwell CDC\rTrue\rOGG CDC\rTrue\rJSON\rTrue\rThe JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don't contain field types; When you write JSON sources into Flink Pulsar sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\r1. If missing field types, Paimon will use 'STRING' type as default. 2. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization.\r3. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.\rSynchronizing Tables\r#\rBy using PulsarSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Pulsar\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_table \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--type_mapping to-string] \\ [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\ [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--table\rThe Paimon table name.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r--computed_column\rThe definitions of computed columns. The argument field is from Pulsar topic's table field name. See here for a complete list of configurations. --pulsar_conf\rThe configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rIf the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Pulsar topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --primary_keys pt,uid \\ --computed_column \u0026#39;_year=year(age)\u0026#39; \\ --pulsar_conf topic=order \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 If the Pulsar topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key create_time TIMESTAMP, -- the argument of computed column part part STRING, -- partition key PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\ ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table test_table \\ --partition_keys pt \\ --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=test_log \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=json \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf sink.parallelism=4 Synchronizing Databases\r#\rBy using PulsarSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_database \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\ [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\ [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\ [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\ [--type_mapping to-string] \\ [--partition_keys \u0026lt;partition_keys\u0026gt;] \\ [--primary_keys \u0026lt;primary-keys\u0026gt;] \\ [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]] Configuration\rDescription\r--warehouse\rThe path to Paimon warehouse.\r--database\rThe database name in Paimon catalog.\r--ignore_incompatible\rIt is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.\r--table_prefix\rThe prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".\r--table_suffix\rThe suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".\r--including_tables\rIt is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.\r--excluding_tables\rIt is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.\r--type_mapping\rIt is used to specify how to map MySQL data type to Paimon type.\nSupported options:\r\"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN.\r\"to-nullable\": ignores all NOT NULL constraints (except for primary keys).\rThis is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.\r\"to-string\": maps all MySQL types to STRING.\r\"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.\r\"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES.\r\"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.\r--partition_keys\rThe partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".\rIf the keys are not in source table, the sink table won't set partition keys.\r--primary_keys\rThe primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".\rIf the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys.\rOtherwise, the sink table won't set primary keys.\r--pulsar_conf\rThe configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.\r--catalog_conf\rThe configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.\r--table_conf\rThe configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.\rOnly tables with primary keys will be synchronized.\nThis action will build a single combined sink for all tables. For each Pulsar topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Pulsar record, this action will try to preform schema evolution.\nExample\nSynchronization from one Pulsar topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --pulsar_conf topic=order \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Synchronization from multiple Pulsar topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ pulsar_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --pulsar_conf topic=order,logistic_order,user \\ --pulsar_conf value.format=canal-json \\ --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\ --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\ --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 Additional pulsar_config\r#\rThere are some useful options to build Flink Pulsar Source, but they are not provided by flink-pulsar-connector document. They are:\nKey\rDefault\rType\rDescription\rvalue.format\r(none)\rString\rDefines the format identifier for encoding value data.\rtopic\r(none)\rString\rTopic name(s) from which the data is read. It also supports topic list by separating topic by semicolon like 'topic-1;topic-2'. Note, only one of \"topic-pattern\" and \"topic\" can be specified.\rtopic-pattern\r(none)\rString\rThe regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified.\rpulsar.startCursor.fromMessageId\rEARLIEST\rSting\rUsing a unique identifier of a single message to seek the start position. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to EARLIEST (-1, -1, -1) or LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).\rpulsar.startCursor.fromPublishTime\r(none)\rLong\rUsing the message publish time to seek the start position.\rpulsar.startCursor.fromMessageIdInclusive\rtrue\rBoolean\rWhether to include the given message id. This option only works when the message id is not EARLIEST or LATEST.\rpulsar.stopCursor.atMessageId\r(none)\rString\rStop consuming when the message id is equal or greater than the specified message id. Message that is equal to the specified message id will not be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).\rpulsar.stopCursor.afterMessageId\r(none)\rString\rStop consuming when the message id is greater than the specified message id. Message that is equal to the specified message id will be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).\rpulsar.stopCursor.atEventTime\r(none)\rLong\rStop consuming when message event time is greater than or equals the specified timestamp. Message that even time is equal to the specified timestamp will not be consumed.\rpulsar.stopCursor.afterEventTime\r(none)\rLong\rStop consuming when message event time is greater than the specified timestamp. Message that even time is equal to the specified timestamp will be consumed.\rpulsar.source.unbounded\rtrue\rBoolean\rTo specify the boundedness of a stream.\rschema.registry.url\r(none)\rString\rWhen configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.\r"},{"id":67,"href":"/concepts/rest/","title":"RESTCatalog","section":"Concepts","content":"\r"},{"id":68,"href":"/ecosystem/trino/","title":"Trino","section":"Ecosystem","content":"\rTrino\r#\rThis documentation is a guide for using Paimon in Trino.\nVersion\r#\rPaimon currently supports Trino 440.\nFilesystem\r#\rFrom version 0.8, Paimon share Trino filesystem for all actions, which means, you should config Trino filesystem before using trino-paimon. You can find information about how to config filesystems for Trino on Trino official website.\nPreparing Paimon Jar File\r#\rDownload\nYou can also manually build a bundled jar from the source code. However, there are a few preliminary steps that need to be taken before compiling:\nTo build from the source code, clone the git repository. Install JDK21 locally, and configure JDK21 as a global environment variable; Then,you can build bundled jar with the following command:\nmvn clean install -DskipTests You can find Trino connector jar in ./paimon-trino-\u0026lt;trino-version\u0026gt;/target/paimon-trino-\u0026lt;trino-version\u0026gt;-1.1.1-plugin.tar.gz.\nWe use hadoop-apache as a dependency for Hadoop, and the default Hadoop dependency typically supports both Hadoop 2 and Hadoop 3. If you encounter an unsupported scenario, you can specify the corresponding Apache Hadoop version.\nFor example, if you want to use Hadoop 3.3.5-1, you can use the following command to build the jar:\nmvn clean install -DskipTests -Dhadoop.apache.version=3.3.5-1 Configure Paimon Catalog\r#\rInstall Paimon Connector\r#\rtar -zxf paimon-trino-\u0026lt;trino-version\u0026gt;-1.1.1-plugin.tar.gz -C ${TRINO_HOME}/plugin NOTE: For JDK 21, when Deploying Trino, should add jvm options: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\nConfigure\r#\rCatalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\nset environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure hadoop-conf-dir in the properties. If you are using a Hadoop filesystem, you can still use trino-hdfs and trino-hive to config it. For example, if you use oss as a storage, you can write in paimon.properties according to Trino Reference:\nhive.config.resources=/path/to/core-site.xml Then, config core-site.xml according to Jindo Reference\nKerberos\r#\rYou can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/trino/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Trino.\nCreate Schema\r#\rCREATE SCHEMA paimon.test_db; Create Table\r#\rCREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = \u0026#39;ORC\u0026#39;, primary_key = ARRAY[\u0026#39;order_key\u0026#39;,\u0026#39;order_date\u0026#39;], partitioned_by = ARRAY[\u0026#39;order_date\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;order_key\u0026#39;, changelog_producer = \u0026#39;input\u0026#39; ); Add Column\r#\rCREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = \u0026#39;ORC\u0026#39;, primary_key = ARRAY[\u0026#39;order_key\u0026#39;,\u0026#39;order_date\u0026#39;], partitioned_by = ARRAY[\u0026#39;order_date\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;order_key\u0026#39;, changelog_producer = \u0026#39;input\u0026#39; ); ALTER TABLE paimon.test_db.orders ADD COLUMN shipping_address varchar; Query\r#\rSELECT * FROM paimon.test_db.orders; Query with Time Traveling\r#\r-- read the snapshot from specified timestamp SELECT * FROM t FOR TIMESTAMP AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00 Asia/Shanghai\u0026#39;; -- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t FOR VERSION AS OF 1; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t FOR VERSION AS OF \u0026#39;my-tag\u0026#39;; If tag's name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if\ryou have a tag named '1' based on snapshot 2, the statement `SELECT * FROM paimon.test_db.orders FOR VERSION AS OF '1'` actually queries snapshot 2\rinstead of snapshot 1.\rInsert\r#\rINSERT INTO paimon.test_db.orders VALUES (.....); Supports:\nprimary key table with fixed bucket. non-primary-key table with bucket -1. Trino to Paimon type mapping\r#\rThis section lists all supported type conversion between Trino and Paimon. All Trino\u0026rsquo;s data types are available in package io.trino.spi.type.\nTrino Data Type\rPaimon Data Type\rAtomic Type\rRowType\rRowType\rfalse\rMapType\rMapType\rfalse\rArrayType\rArrayType\rfalse\rBooleanType\rBooleanType\rtrue\rTinyintType\rTinyIntType\rtrue\rSmallintType\rSmallIntType\rtrue\rIntegerType\rIntType\rtrue\rBigintType\rBigIntType\rtrue\rRealType\rFloatType\rtrue\rDoubleType\rDoubleType\rtrue\rCharType(length)\rCharType(length)\rtrue\rVarCharType(VarCharType.MAX_LENGTH)\rVarCharType(VarCharType.MAX_LENGTH)\rtrue\rVarCharType(length)\rVarCharType(length), length is less than VarCharType.MAX_LENGTH\rtrue\rDateType\rDateType\rtrue\rTimestampType\rTimestampType\rtrue\rDecimalType(precision, scale)\rDecimalType(precision, scale)\rtrue\rVarBinaryType(length)\rVarBinaryType(length)\rtrue\rTimestampWithTimeZoneType\rLocalZonedTimestampType\rtrue\rTmp Dir\r#\rPaimon will unzip some jars to the tmp directory for codegen. By default, Trino will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Trino starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\n"},{"id":69,"href":"/ecosystem/amoro/","title":"Amoro","section":"Ecosystem","content":"\rApache Amoro With Paimon\r#\rApache Amoro(incubating) is a Lakehouse management system built on open data lake formats. Working with compute engines including Flink, Spark, and Trino, Amoro brings pluggable and Table Maintenance features for a Lakehouse to provide out-of-the-box data warehouse experience, and helps data platforms or products easily build infra-decoupled, stream-and-batch-fused and lake-native architecture. AMS(Amoro Management Service) provides Lakehouse management features, like self-optimizing, data expiration, etc. It also provides a unified catalog service for all compute engines, which can also be combined with existing metadata services like HMS(Hive Metastore).\nTable Format\r#\rApache Amoro supports all catalog types supported by paimon, including common catalog: Hadoop, Hive, Glue, JDBC, Nessie and other third-party catalog. Amoro supports all storage types supported by Paimon, including common store: Hadoop, S3, GCS, ECS, OSS, and so on.\nIn the future, Paimon automatic optimization strategy will be supported, and users can achieve the best balance experience by cooperating with Amoro automatic optimization\n"},{"id":70,"href":"/cdc-ingestion/debezium-bson/","title":"Debezium BSON","section":"CDC Ingestion","content":"\rDebezium BSON Format\r#\rThe debezium-bson format is one of the formats supported by Kafka CDC. It is the format obtained by collecting mongodb through debezium, which is similar to debezium-json format. However, MongoDB does not have a fixed schema, and the field types of each document may be different, so the before/after fields in JSON are all string types, while the debezium-json format requires a JSON object type.\nPrepare MongoDB BSON Jar\r#\rCan be downloaded from the Maven repository\nbson-*.jar Introduction\r#\rThe debezium bson format requires insert/update/delete event messages include the full document, and include a field that represents the state of the document before the change.\rThis requires setting debezium's capture.mode to change_streams_update_full_with_pre_image and [capture.mode.full.update.type](https://debezium.io/documentation/reference/stable/connectors/mongodb.html#mongodb-property-capture-mode-full-update-type) to post_image.\rBefore version 6.0 of MongoDB, it was not possible to obtain 'Update Before' information. Therefore, using the id field in the Kafka Key as 'Update before' information\rHere is a simple example for an update operation captured from a Mongodb customers collection in JSON format:\n{ \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;struct\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;optional\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;io.debezium.data.Json\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;field\u0026#34;: \u0026#34;before\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;optional\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;io.debezium.data.Json\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;field\u0026#34;: \u0026#34;after\u0026#34; }, ... ] }, \u0026#34;payload\u0026#34;: { \u0026#34;before\u0026#34;: \u0026#34;{\\\u0026#34;_id\\\u0026#34;: {\\\u0026#34;$oid\\\u0026#34; : \\\u0026#34;596e275826f08b2730779e1f\\\u0026#34;}, \\\u0026#34;name\\\u0026#34; : \\\u0026#34;Anne\\\u0026#34;, \\\u0026#34;create_time\\\u0026#34; : {\\\u0026#34;$numberLong\\\u0026#34; : \\\u0026#34;1558965506000\\\u0026#34;}, \\\u0026#34;tags\\\u0026#34;:[\\\u0026#34;success\\\u0026#34;]}\u0026#34;, \u0026#34;after\u0026#34;: \u0026#34;{\\\u0026#34;_id\\\u0026#34;: {\\\u0026#34;$oid\\\u0026#34; : \\\u0026#34;596e275826f08b2730779e1f\\\u0026#34;}, \\\u0026#34;name\\\u0026#34; : \\\u0026#34;Anne\\\u0026#34;, \\\u0026#34;create_time\\\u0026#34; : {\\\u0026#34;$numberLong\\\u0026#34; : \\\u0026#34;1558965506000\\\u0026#34;}, \\\u0026#34;tags\\\u0026#34;:[\\\u0026#34;passion\\\u0026#34;,\\\u0026#34;success\\\u0026#34;]}\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;db\u0026#34;: \u0026#34;inventory\u0026#34;, \u0026#34;rs\u0026#34;: \u0026#34;rs0\u0026#34;, \u0026#34;collection\u0026#34;: \u0026#34;customers\u0026#34;, ... }, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1558965515240, \u0026#34;ts_us\u0026#34;: 1558965515240142, \u0026#34;ts_ns\u0026#34;: 1558965515240142879 } } This document from the MongoDB collection customers has 4 columns, the _id is a BSON ObjectID, name is a string, create_time is a long, tags is an array of string. The following is the processing result in debezium-bson format:\nDocument Schema:\nField Name Field Type Key _id STRING Primary Key name STRING create_time STRING tags STRING Records:\nRowKind _id name create_time tags -U 596e275826f08b2730779e1f Anne 1558965506000 [\u0026ldquo;success\u0026rdquo;] +U 596e275826f08b2730779e1f Anne 1558965506000 [\u0026ldquo;passion\u0026rdquo;,\u0026ldquo;success\u0026rdquo;] How it works\r#\rBecause the schema field of the event message does not have the field information of the document, the debezium-bson format does not require event messages to have schema information. The specific operations are as follows:\nParse the before/after fields of the event message into BSONDocument. Recursive traversal all fields of BSONDocument and convert BsonValue to Java Object. All top-level fields of before/after are converted to string type, and _id is fixed to primary key If the top-level fields of before/after is a basic type(such as Integer/Long, etc.), it is directly converted to a string, if not, it is converted to a JSON string Below is a list of top-level field BsonValue conversion examples:\nBsonValue Type\rJson Value\rConversion Result String\rBsonString\r\"hello\"\r\"hello\"\rBsonInt32\r123\r\"123\"\rBsonInt64\r1735934393769\r{\"$numberLong\": \"1735934393769\"}\r\"1735934393769\"\rBsonDouble\r{\"$numberDouble\": \"3.14\"}\r{\"$numberDouble\": \"NaN\"}\r{\"$numberDouble\": \"Infinity\"}\r{\"$numberDouble\": \"-Infinity\"}\r\"3.14\"\r\"NaN\"\r\"Infinity\"\r\"-Infinity\"\rBsonBoolean\rtrue\rfalse\r\"true\"\r\"false\"\rBsonArray\r[1,2,{\"$numberLong\": \"1735934393769\"}]\r\"[1,2,1735934393769]\"\rBsonObjectId\r{\"$oid\": \"596e275826f08b2730779e1f\"}\r\"596e275826f08b2730779e1f\"\rBsonDateTime\r{\"$date\": 1735934393769 }\r\"1735934393769\"\rBsonNull\rnull\rnull\rBsonUndefined\r{\"$undefined\": true}\rnull\rBsonBinary\r{\"$binary\": \"uE2/4v5MSVOiJZkOo3APKQ==\", \"$type\": \"0\"}\r\"uE2/4v5MSVOiJZkOo3APKQ==\"\rBsonBinary(type=UUID)\r{\"$binary\": \"uE2/4v5MSVOiJZkOo3APKQ==\", \"$type\": \"4\"}\r\"b84dbfe2-fe4c-4953-a225-990ea3700f29\"\rBsonDecimal128\r{\"$numberDecimal\": \"3.14\"}\r{\"$numberDecimal\": \"NaN\"}\r\"3.14\"\r\"NaN\"\rBsonRegularExpression\r{\"$regularExpression\": {\"pattern\": \"^pass$\", \"options\": \"i\"}}\r\"/^pass$/i\"\rBsonSymbol\r{\"$symbol\": \"symbol\"}\r\"symbol\"\rBsonTimestamp\r{\"$timestamp\": {\"t\": 1736997330, \"i\": 2}}\r\"1736997330\"\rBsonMinKey\r{\"$minKey\": 1}\r\"BsonMinKey\"\rBsonMaxKey\r{\"$maxKey\": 1}\r\"BsonMaxKey\"\rBsonJavaScript\r{\"$code\": \"function(){}\"}\r\"function(){}\"\rBsonJavaScriptWithScope\r{\"$code\": \"function(){}\", \"$scope\": {\"name\": \"Anne\"}}\r'{\"$code\": \"function(){}\", \"$scope\": {\"name\": \"Anne\"}}'\rBsonDocument\r{\r\"decimalPi\": {\"$numberDecimal\": \"3.14\"},\r\"doublePi\": {\"$numberDouble\": \"3.14\"},\r\"doubleNaN\": {\"$numberDouble\": \"NaN\"},\r\"decimalNaN\": {\"$numberDecimal\": \"NaN\"},\r\"long\": {\"$numberLong\": \"100\"},\r\"bool\": true,\r\"array\": [\r{\"$numberInt\": \"1\"},\r{\"$numberLong\": \"2\"}\r]\r}\r'{\r\"decimalPi\":3.14,\r\"doublePi\":3.14,\r\"doubleNaN\":\"NaN\",\r\"decimalNaN\":\"NaN\",\r\"long\":100,\r\"bool\":true,\r\"array\":[1,2]\r}'\rHow to use\r#\rUse debezium-bson by adding the kafka_conf parameter value.format=debezium-bson. Let’s take table synchronization as an example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ kafka_sync_table \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --table ods_mongodb_customers \\ --primary_keys _id \\ --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\ --kafka_conf topic=customers \\ --kafka_conf properties.group.id=123456 \\ --kafka_conf value.format=debezium-bson \\ --catalog_conf metastore=filesystem \\ --table_conf bucket=4 \\ --table_conf changelog-producer=input \\ --table_conf sink.parallelism=4 "},{"id":71,"href":"/spark/","title":"Engine Spark","section":"Apache Paimon","content":"\r"},{"id":72,"href":"/primary-key-table/sequence-rowkind/","title":"Sequence \u0026 Rowkind","section":"Table with PK","content":"\rSequence and Rowkind\r#\rWhen creating a table, you can specify the 'sequence.field' by specifying fields to determine the order of updates, or you can specify the 'rowkind.field' to determine the changelog kind of record.\nSequence Field\r#\rBy default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nFlink\rCREATE TABLE my_table ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, update_time TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;update_time\u0026#39; ); The record with the largest sequence.field value will be the last to merge, if the values are the same, the input order will be used to determine which one is the last one. sequence.field supports fields of all data types.\nYou can define multiple fields for sequence.field, for example 'update_time,flag', multiple fields will be compared in order.\nUser defined sequence fields conflict with features such as `first_row` and `first_value`, which may result in unexpected results.\rRow Kind Field\r#\rBy default, the primary key table determines the row kind according to the input row. You can also define the 'rowkind.field' to use a field to extract row kind.\nThe valid row kind string should be '+I', '-U', '+U' or '-D'.\n"},{"id":73,"href":"/spark/sql-alter/","title":"SQL Alter","section":"Engine Spark","content":"\rAltering Tables\r#\rChanging/Adding Table Properties\r#\rThe following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties\r#\rThe following SQL removes write-buffer-size table property.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment\r#\rThe following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment\r#\rThe following SQL removes table comment.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;comment\u0026#39;); Rename Table Name\r#\rThe following SQL rename the table name to new name.\nThe simplest sql to call is:\nALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:\nALTER TABLE [catalog.[database.]]test1 RENAME to [database.]test2; But we can\u0026rsquo;t put catalog name before the renamed-to table, it will throw an error if we write sql like this:\nALTER TABLE catalog.database.test1 RENAME to catalog.database.test2; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.\rAdding New Columns\r#\rThe following SQL adds two columns c1 and c2 to table my_table.\nALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING ); The following SQL adds a nested column f3 to a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table ADD COLUMN v.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ADD COLUMN v.element.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ADD COLUMN v.value.f3 STRING; Renaming Column Name\r#\rThe following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME COLUMN c0 TO c1; The following SQL renames a nested column f1 to f100 in a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table RENAME COLUMN v.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table RENAME COLUMN v.element.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table RENAME COLUMN v.value.f1 to f100; Dropping Columns\r#\rThe following SQL drops two columns c1 and c2 from table my_table.\nALTER TABLE my_table DROP COLUMNS (c1, c2); The following SQL drops a nested column f2 from a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table DROP COLUMN v.f2; The following SQL drops a nested column f2 from a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table DROP COLUMN v.element.f2; The following SQL drops a nested column f2 from a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table DROP COLUMN v.value.f2; In hive catalog, you need to ensure:\ndisable hive.metastore.disallow.incompatible.col.type.changes in your hive server or spark-sql --conf spark.hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your spark. Otherwise this operation may fail, throws an exception like The following columns have types incompatible with the\rexisting columns in their respective positions.\nDropping Partitions\r#\rThe following SQL drops the partitions of the paimon table. For spark sql, you need to specify all the partition columns.\nALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); Adding Partitions\r#\rThe following SQL adds the partitions of the paimon table. For spark sql, you need to specify all the partition columns, only with metastore configured metastore.partitioned-table=true.\nALTER TABLE my_table ADD PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); Changing Column Comment\r#\rThe following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position\r#\rALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b; Changing Column Position\r#\rALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b; Changing Column Type\r#\rALTER TABLE my_table ALTER COLUMN col_a TYPE DOUBLE; The following SQL changes the type of a nested column f2 to BIGINT in a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table ALTER COLUMN v.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ALTER COLUMN v.element.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ALTER COLUMN v.value.f2 TYPE BIGINT; ALTER DATABASE\r#\rThe following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\nALTER { DATABASE | SCHEMA | NAMESPACE } my_database SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] ) Altering Database Location\r#\rThe following SQL sets the location of the specified database to file:/temp/my_database.db.\nALTER DATABASE my_database SET LOCATION \u0026#39;file:/temp/my_database.db\u0026#39; "},{"id":74,"href":"/flink/sql-lookup/","title":"SQL Lookup","section":"Engine Flink","content":"\rLookup Joins\r#\rLookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nPaimon supports lookup joins on tables with primary keys and append tables in Flink. The following example illustrates this feature.\nPrepare\r#\rFirst, let\u0026rsquo;s create a Paimon table and update it in real-time.\n-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); Normal Lookup\r#\rYou can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Retry Lookup\r#\rIf the records of orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup. Only for Flink 1.16+.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Async Retry Lookup\r#\rThe problem with synchronous retry is that one record will block subsequent records, causing the entire job to be blocked. You can consider using async + allow_unordered to avoid blocking, the records that join missing will no longer block other records.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;output-mode\u0026#39;=\u0026#39;allow_unordered\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.async\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;lookup.async-thread-number\u0026#39;=\u0026#39;16\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; If the main table (`orders`) is CDC stream, `allow_unordered` will be ignored by Flink SQL (only supports append stream),\ryour streaming job may be blocked. You can try to use `audit_log` system table feature of Paimon to walk around\r(convert CDC stream to append stream).\rDynamic Partition\r#\rIn traditional data warehouses, each partition often maintains the latest full data, so this partition table only needs to join the latest partition. Paimon has specifically developed the max_pt feature for this scenario.\nCreate Paimon Partitioned Table\nCREATE TABLE customers ( id INT, name STRING, country STRING, zip STRING, dt STRING, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt); Lookup Join\nSELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;scan.partitions\u0026#39;=\u0026#39;max_pt()\u0026#39;, \u0026#39;lookup.dynamic-partition.refresh-interval\u0026#39;=\u0026#39;1 h\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The Lookup node will automatically refresh the latest partition and query the data of the latest partition.\nThe option scan.partitions can also specify fixed partitions in the form of key1=value1,key2=value2. Multiple partitions should be separated by semicolon (;). When specifying fixed partitions, this option can also be used in batch joins.\nQuery Service\r#\rYou can run a Flink Streaming Job to start query service for the table. When QueryService exists, Flink Lookup Join will prioritize obtaining data from it, which will effectively improve query performance.\nFlink SQL\rCALL sys.query_service(\u0026#39;database_name.table_name\u0026#39;, parallelism); Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ query_service \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--parallelism \u0026lt;parallelism\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] "},{"id":75,"href":"/concepts/spec/tableindex/","title":"Table Index","section":"Specification","content":"\rTable index\r#\rTable Index files is in the index directory.\nDynamic Bucket Index\r#\rDynamic bucket index is used to store the correspondence between the hash value of the primary-key and the bucket.\nIts structure is very simple, only storing hash values in the file:\nHASH_VALUE | HASH_VALUE | HASH_VALUE | HASH_VALUE | \u0026hellip;\nHASH_VALUE is the hash value of the primary-key. 4 bytes, BIG_ENDIAN.\nDeletion Vectors\r#\rDeletion file is used to store the deleted records position for each data file. Each bucket has one deletion file for primary key table.\nThe deletion file is a binary file, and the format is as follows:\nFirst, record version by a byte. Current version is 1. Then, record \u0026lt;size of serialized bin, serialized bin, checksum of serialized bin\u0026gt; in sequence. Size and checksum are BIG_ENDIAN Integer. For each serialized bin:\nFirst, record a const magic number by an int (BIG_ENDIAN). Current the magic number is 1581511376. Then, record serialized bitmap. Which is a RoaringBitmap (org.roaringbitmap.RoaringBitmap). "},{"id":76,"href":"/concepts/table-types/","title":"Table Types","section":"Concepts","content":"\rTable Types\r#\rPaimon supports table types:\ntable with pk: Paimon Data Table with Primary key table w/o pk: Paimon Data Table without Primary key view: metastore required, views in SQL are a kind of virtual table format-table: file format table refers to a directory that contains multiple files of the same format, where operations on this table allow for reading or writing to these files, compatible with Hive tables object table: provides metadata indexes for unstructured data objects in the specified Object Storage directory. materialized-table: aimed at simplifying both batch and stream data pipelines, providing a consistent development experience, see Flink Materialized Table Table with PK\r#\rSee Paimon with Primary key.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing streaming update and streaming changelog read.\nThe definition of primary key is similar to that of standard SQL, as it ensures that there is only one data entry for the same primary key during batch queries.\nFlink SQL\rCREATE TABLE my_table ( a INT PRIMARY KEY NOT ENFORCED, b STRING ) WITH ( \u0026#39;bucket\u0026#39;=\u0026#39;8\u0026#39; ) Spark SQL\rCREATE TABLE my_table ( a INT, b STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;a\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39; ) Table w/o PK\r#\rSee Paimon w/o Primary key.\nIf a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through streaming upsert. It can only receive incoming data from append data.\nHowever, it also supports batch sql: DELETE, UPDATE, and MERGE-INTO.\nCREATE TABLE my_table ( a INT, b STRING ) View\r#\rView is supported when the metastore can support view, for example, hive metastore. If you don\u0026rsquo;t have metastore, you can only use temporary View, which only exists in the current session. This chapter mainly describes persistent views.\nView will currently save the original SQL. If you need to use View across engines, you can write a cross engine SQL statement. For example:\nFlink SQL\rCREATE VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS; SHOW CREATE VIEW my_view; Spark SQL\rCREATE [OR REPLACE] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS; Format Table\r#\rFormat table is supported when the metastore can support format table, for example, hive metastore. The Hive tables inside the metastore will be mapped to Paimon\u0026rsquo;s Format Table for computing engines (Spark, Hive, Flink) to read and write.\nFormat table refers to a directory that contains multiple files of the same format, where operations on this table allow for reading or writing to these files, facilitating the retrieval of existing data and the addition of new files.\nPartitioned file format table just like the standard hive format. Partitions are discovered and inferred based on directory structure.\nFormat Table is enabled by default, you can disable it by configuring Catalog option: 'format-table.enabled'.\nCurrently only support CSV, Parquet, ORC, JSON formats.\nFlink-CSV\rCREATE TABLE my_csv_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;csv\u0026#39;, \u0026#39;field-delimiter\u0026#39;=\u0026#39;,\u0026#39; ) Spark-CSV\rCREATE TABLE my_csv_table ( a INT, b STRING ) USING csv OPTIONS (\u0026#39;field-delimiter\u0026#39; \u0026#39;,\u0026#39;) Flink-Parquet\rCREATE TABLE my_parquet_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;parquet\u0026#39; ) Spark-Parquet\rCREATE TABLE my_parquet_table ( a INT, b STRING ) USING parquet Flink-JSON\rCREATE TABLE my_json_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;json\u0026#39; ) Spark-JSON\rCREATE TABLE my_json_table ( a INT, b STRING ) USING json Object Table\r#\rObject Table provides metadata indexes for unstructured data objects in the specified Object Storage storage directory. Object tables allow users to analyze unstructured data in Object Storage:\nUse Python API to manipulate these unstructured data, such as converting images to PDF format. Model functions can also be used to perform inference, and then the results of these operations can be concatenated with other structured data in the Catalog. The object table is managed by Catalog and can also have access permissions and the ability to manage blood relations.\nFlink-SQL\r-- Create Object Table CREATE TABLE `my_object_table` WITH ( \u0026#39;type\u0026#39; = \u0026#39;object-table\u0026#39;, \u0026#39;object-location\u0026#39; = \u0026#39;oss://my_bucket/my_location\u0026#39; ); -- Refresh Object Table CALL sys.refresh_object_table(\u0026#39;mydb.my_object_table\u0026#39;); -- Query Object Table SELECT * FROM `my_object_table`; -- Query Object Table with Time Travel SELECT * FROM `my_object_table` /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; Spark-SQL\r-- Create Object Table CREATE TABLE `my_object_table` TBLPROPERTIES ( \u0026#39;type\u0026#39; = \u0026#39;object-table\u0026#39;, \u0026#39;object-location\u0026#39; = \u0026#39;oss://my_bucket/my_location\u0026#39; ); -- Refresh Object Table CALL sys.refresh_object_table(\u0026#39;mydb.my_object_table\u0026#39;); -- Query Object Table SELECT * FROM `my_object_table`; -- Query Object Table with Time Travel SELECT * FROM `my_object_table` VERSION AS OF 1; Materialized Table\r#\rMaterialized Table aimed at simplifying both batch and stream data pipelines, providing a consistent development experience, see Flink Materialized Table.\nNow only Flink SQL integrate to Materialized Table, we plan to support it in Spark SQL too.\nCREATE MATERIALIZED TABLE continuous_users_shops PARTITIONED BY (ds) FRESHNESS = INTERVAL \u0026#39;30\u0026#39; SECOND AS SELECT user_id, ds, SUM (payment_amount_cents) AS payed_buy_fee_sum, SUM (1) AS PV FROM ( SELECT user_id, order_created_at AS ds, payment_amount_cents FROM json_source ) AS tmp GROUP BY user_id, ds; "},{"id":77,"href":"/spark/auxiliary/","title":"Auxiliary","section":"Engine Spark","content":"\rAuxiliary Statements\r#\rSet / Reset\r#\rThe SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values.\nTo set dynamic options globally, you need add the spark.paimon. prefix. You can also set dynamic table options at this format: spark.paimon.${catalogName}.${dbName}.${tableName}.${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. Dynamic table options will override global options if there are conflicts.\n-- set spark conf SET spark.sql.sources.partitionOverwriteMode=dynamic; -- set paimon conf SET spark.paimon.file.block-size=512M; -- reset conf RESET spark.paimon.file.block-size; -- set scan.snapshot-id=1 for the table default.T in any catalogs SET spark.paimon.*.default.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=1 for the table T in any databases and catalogs SET spark.paimon.*.*.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=2 for the table default.T1 in any catalogs and scan.snapshot-id=1 on other tables SET spark.paimon.scan.snapshot-id=1; SET spark.paimon.*.default.T1.scan.snapshot-id=2; SELECT * FROM default.T1 JOIN default.T2 ON xxxx; Describe table\r#\rDESCRIBE TABLE statement returns the basic metadata information of a table or view. The metadata information includes column name, column type and column comment.\n-- describe table or view DESCRIBE TABLE my_table; -- describe table or view with additional metadata DESCRIBE TABLE EXTENDED my_table; Show create table\r#\rSHOW CREATE TABLE returns the CREATE TABLE statement or CREATE VIEW statement that was used to create a given table or view.\nSHOW CREATE TABLE my_table; Show columns\r#\rReturns the list of columns in a table. If the table does not exist, an exception is thrown.\nSHOW COLUMNS FROM my_table; Show partitions\r#\rThe SHOW PARTITIONS statement is used to list partitions of a table. An optional partition spec may be specified to return the partitions matching the supplied partition spec.\n-- Lists all partitions for my_table SHOW PARTITIONS my_table; -- Lists partitions matching the supplied partition spec for my_table SHOW PARTITIONS my_table PARTITION (dt=\u0026#39;20230817\u0026#39;); Show Table Extended\r#\rThe SHOW TABLE EXTENDED statement is used to list table or partition information.\n-- Lists tables that satisfy regular expressions SHOW TABLE EXTENDED IN db_name LIKE \u0026#39;test*\u0026#39;; -- Lists the specified partition information for the table SHOW TABLE EXTENDED IN db_name LIKE \u0026#39;table_name\u0026#39; PARTITION(pt = \u0026#39;2024\u0026#39;); Show views\r#\rThe SHOW VIEWS statement returns all the views for an optionally specified database.\n-- Lists all views SHOW VIEWS; -- Lists all views that satisfy regular expressions SHOW VIEWS LIKE \u0026#39;test*\u0026#39;; Analyze table\r#\rThe ANALYZE TABLE statement collects statistics about the table, that are to be used by the query optimizer to find a better query execution plan. Paimon supports collecting table-level statistics and column statistics through analyze.\n-- collect table-level statistics ANALYZE TABLE my_table COMPUTE STATISTICS; -- collect table-level statistics and column statistics for col1 ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1; -- collect table-level statistics and column statistics for all columns ANALYZE TABLE my_table COMPUTE STATISTICS FOR ALL COLUMNS; "},{"id":78,"href":"/primary-key-table/compaction/","title":"Compaction","section":"Table with PK","content":"\rCompaction\r#\rWhen more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adopts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nCompaction solves:\nReduce Level 0 files to avoid poor query performance. Produce changelog via changelog-producer. Produce deletion vectors for MOW mode. Snapshot Expiration, Tag Expiration, Partitions Expiration. Limitation:\nThere can only be one job working on the same partition\u0026rsquo;s compaction, otherwise it will cause conflicts and one side will throw an exception failure. Writing performance is almost always affected by compaction, so its tuning is crucial.\nAsynchronous Compaction\r#\rCompaction is inherently asynchronous, but if you want it to be completely asynchronous without blocking writes, expecting a mode for maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:\nnum-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 lookup-wait = false This configuration will generate more files during peak write periods and gradually merge them for optimal read performance during low write periods.\nDedicated compaction job\r#\rIn general, if you expect multiple jobs to be written to the same table, you need to separate the compaction. You can use dedicated compaction job.\nRecord-Level expire\r#\rIn compaction, you can configure record-Level expire time to expire records, you should configure:\n'record-level.expire-time': time retain for records. 'record-level.time-field': time field for record level expire. Expiration happens in compaction, and there is no strong guarantee to expire records in time. You can trigger a full compaction manually to expire records which were not expired in time.\nFull Compaction\r#\rPaimon Compaction uses Universal-Compaction. By default, when there is too much incremental data, Full Compaction will be automatically performed. You don\u0026rsquo;t usually have to worry about it.\nPaimon also provides a configuration that allows for regular execution of Full Compaction.\n\u0026lsquo;compaction.optimization-interval\u0026rsquo;: Implying how often to perform an optimization full compaction, this configuration is used to ensure the query timeliness of the read-optimized system table. \u0026lsquo;full-compaction.delta-commits\u0026rsquo;: Full compaction will be constantly triggered after delta commits. Its disadvantage is that it can only perform compaction synchronously, which will affect writing efficiency. Lookup Compaction\r#\rWhen primary key table is configured with lookup changelog producer or first-row merge-engine or has enabled deletion vectors for MOW mode, Paimon will use a radical compaction strategy to force compacting level 0 files to higher levels for every compaction trigger.\nPaimon also provides configurations to optimize the frequency of this compaction.\n\u0026rsquo;lookup-compact\u0026rsquo;: compact mode used for lookup compaction. Possible values: radical, will use ForceUpLevel0Compaction strategy to radically compact new files; gentle, will use UniversalCompaction strategy to gently compact new files; \u0026rsquo;lookup-compact.max-interval\u0026rsquo;: The max interval for a forced L0 lookup compaction to be triggered in gentle mode. This option is only valid when lookup-compact mode is gentle. By configuring \u0026rsquo;lookup-compact\u0026rsquo; as gentle, new files in L0 will not be compacted immediately, this may greatly reduce the overall resource usage at the expense of worse data freshness in certain cases.\nCompaction Options\r#\rNumber of Sorted Runs to Pause Writing\r#\rWhen the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\nOption\rRequired\rDefault\rType\rDescription\rnum-sorted-run.stop-trigger\rNo\r(none)\rInteger\rThe number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.\rWrite stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size.\nOption\rRequired\rDefault\rType\rDescription\rsort-spill-threshold\rNo\r(none)\rInteger\rIf the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.\rNumber of Sorted Runs to Trigger Compaction\r#\rPaimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\nOption\rRequired\rDefault\rType\rDescription\rnum-sorted-run.compaction-trigger\rNo\r5\rInteger\rThe sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).\rCompaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\n"},{"id":79,"href":"/concepts/spec/fileindex/","title":"File Index","section":"Specification","content":"\rFile index\r#\rDefine file-index.${index_type}.columns, Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, or in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nIndex File\r#\rFile index file format. Put all column and offset in the header.\n______________________________________ _____________________\r| magic ｜version｜head length |\r|--------------------------------------|\r| column number |\r|--------------------------------------|\r| column 1 ｜ index number |\r|--------------------------------------|\r| index name 1 ｜start pos ｜length |\r|--------------------------------------|\r| index name 2 ｜start pos ｜length |\r|--------------------------------------|\r| index name 3 ｜start pos ｜length |\r|--------------------------------------| HEAD\r| column 2 ｜ index number |\r|--------------------------------------|\r| index name 1 ｜start pos ｜length |\r|--------------------------------------|\r| index name 2 ｜start pos ｜length |\r|--------------------------------------|\r| index name 3 ｜start pos ｜length |\r|--------------------------------------|\r| ... |\r|--------------------------------------|\r| ... |\r|--------------------------------------|\r| redundant length ｜redundant bytes |\r|--------------------------------------| ---------------------\r| BODY |\r| BODY |\r| BODY | BODY\r| BODY |\r|______________________________________| _____________________\r*\rmagic: 8 bytes long, value is 1493475289347502L, BIG_ENDIAN\rversion: 4 bytes int, BIG_ENDIAN\rhead length: 4 bytes int, BIG_ENDIAN\rcolumn number: 4 bytes int, BIG_ENDIAN\rcolumn x name: 2 bytes short BIG_ENDIAN and Java modified-utf-8\rindex number: 4 bytes int (how many column items below), BIG_ENDIAN\rindex name x: 2 bytes short BIG_ENDIAN and Java modified-utf-8\rstart pos: 4 bytes int, BIG_ENDIAN\rlength: 4 bytes int, BIG_ENDIAN\rredundant length: 4 bytes int (for compatibility with later versions, in this version, content is zero)\rredundant bytes: var bytes (for compatibility with later version, in this version, is empty)\rBODY: column index bytes + column index bytes + column index bytes + .......\rIndex: BloomFilter\r#\rDefine 'file-index.bloom-filter.columns'.\nContent of bloom filter index is simple:\nnumHashFunctions 4 bytes int, BIG_ENDIAN bloom filter bytes This class use (64-bits) long hash. Store the num hash function (one integer) and bit set bytes only. Hash bytes type (like varchar, binary, etc.) using xx hash, hash numeric type by specified number hash.\nIndex: Bitmap\r#\rfile-index.bitmap.columns: specify the columns that need bitmap index. file-index.bitmap.\u0026lt;column_name\u0026gt;.index-block-size: to config secondary index block size, default value is 16kb. V2\rBitmap file index format (V2):\nBitmap file index format (V2)\r+-------------------------------------------------+-----------------\r｜ version (1 byte) = 2 ｜\r+-------------------------------------------------+\r｜ row count (4 bytes int) ｜\r+-------------------------------------------------+\r｜ non-null value bitmap number (4 bytes int) ｜\r+-------------------------------------------------+\r｜ has null value (1 byte) ｜\r+-------------------------------------------------+\r｜ null value offset (4 bytes if has null value) ｜ HEAD\r+-------------------------------------------------+\r｜ null bitmap length (4 bytes if has null value) ｜\r+-------------------------------------------------+\r｜ bitmap index block number (4 bytes int) ｜\r+-------------------------------------------------+\r｜ value 1 | offset 1 ｜\r+-------------------------------------------------+\r｜ value 2 | offset 2 ｜\r+-------------------------------------------------+\r｜ ... ｜\r+-------------------------------------------------+\r｜ bitmap body offset (4 bytes int) ｜\r+-------------------------------------------------+-----------------\r｜ bitmap index block 1 ｜\r+-------------------------------------------------+\r｜ bitmap index block 2 ｜ INDEX BLOCKS\r+-------------------------------------------------+\r｜ ... ｜\r+-------------------------------------------------+-----------------\r｜ serialized bitmap 1 ｜\r+-------------------------------------------------+\r｜ serialized bitmap 2 ｜\r+-------------------------------------------------+ BITMAP BLOCKS\r｜ serialized bitmap 3 ｜\r+-------------------------------------------------+\r｜ ... ｜\r+-------------------------------------------------+-----------------\rindex block format:\r+-------------------------------------------------+\r｜ entry number (4 bytes int) ｜\r+-------------------------------------------------+\r｜ value 1 | offset 1 | length 1 ｜\r+-------------------------------------------------+\r｜ value 2 | offset 2 | length 2 ｜\r+-------------------------------------------------+\r｜ ... ｜\r+-------------------------------------------------+\rvalue x: var bytes for any data type (as bitmap identifier)\roffset: 4 bytes int (when it is negative, it represents that there is only one value\rand its position is the inverse of the negative value)\rlength: 4 bytes int\rV1 (Legacy)\r(Legacy) Bitmap file index format (V1):\nYou can configure file-index.bitmap.version to use legacy bitmap version 1.\nBitmap file index format (V1)\r+-------------------------------------------------+-----------------\r| version (1 byte) |\r+-------------------------------------------------+\r| row count (4 bytes int) |\r+-------------------------------------------------+\r| non-null value bitmap number (4 bytes int) |\r+-------------------------------------------------+\r| has null value (1 byte) |\r+-------------------------------------------------+\r| null value offset (4 bytes if has null value) | HEAD\r+-------------------------------------------------+\r| value 1 | offset 1 |\r+-------------------------------------------------+\r| value 2 | offset 2 |\r+-------------------------------------------------+\r| value 3 | offset 3 |\r+-------------------------------------------------+\r| ... |\r+-------------------------------------------------+-----------------\r| serialized bitmap 1 |\r+-------------------------------------------------+\r| serialized bitmap 2 |\r+-------------------------------------------------+ BODY\r| serialized bitmap 3 |\r+-------------------------------------------------+\r| ... |\r+-------------------------------------------------+-----------------\r*\rvalue x: var bytes for any data type (as bitmap identifier)\roffset: 4 bytes int (when it is negative, it represents that there is only one value\rand its position is the inverse of the negative value)\rIntegers are all BIG_ENDIAN.\nBitmap only support the following data type: TinyIntType, SmallIntType, IntType, BigIntType, DateType, TimeType, LocalZonedTimestampType, TimestampType, CharType, VarCharType, StringType, BooleanType.\nIndex: Bit-Slice Index Bitmap\r#\rBSI file index is a numeric range index, used to accelerate range query, it can be used with bitmap index.\nDefine 'file-index.bsi.columns'.\nBSI file index format (V1):\nBSI file index format (V1)\r+-------------------------------------------------+\r| version (1 byte) |\r+-------------------------------------------------+\r| row count (4 bytes int) |\r+-------------------------------------------------+\r| has positive value (1 byte) |\r+-------------------------------------------------+\r| positive BSI serialized (if has positive value) | +-------------------------------------------------+\r| has negative value (1 byte) |\r+-------------------------------------------------+\r| negative BSI serialized (if has negative value) | +-------------------------------------------------+\rBSI serialized format (V1):\nBSI serialized format (V1)\r+-------------------------------------------------+\r| version (1 byte) |\r+-------------------------------------------------+\r| min value (8 bytes long) |\r+-------------------------------------------------+\r| max value (8 bytes long) |\r+-------------------------------------------------+\r| serialized existence bitmap | +-------------------------------------------------+\r| bit slice bitmap count (4 bytes int) |\r+-------------------------------------------------+\r| serialized bit 0 bitmap |\r+-------------------------------------------------+\r| serialized bit 1 bitmap |\r+-------------------------------------------------+\r| serialized bit 2 bitmap |\r+-------------------------------------------------+\r| ... |\r+-------------------------------------------------+\rBSI only support the following data type: TinyIntType, SmallIntType, IntType, BigIntType, DateType, LocalZonedTimestamp, TimestampType, DecimalType.\n"},{"id":80,"href":"/maintenance/rescale-bucket/","title":"Rescale Bucket","section":"Maintenance","content":"\rRescale Bucket\r#\rSince the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite\r#\r-- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;); -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec]; Please note that\nALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first. For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...; During overwrite period, make sure there are no other jobs writing the same table/partition. __Note:__ For the table which enables log system(*e.g.* Kafka), please rescale the topic's partition as well to keep consistency.\rUse Case\r#\rRescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\nSuspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\ --savepointPath /tmp/flink-savepoints \\ $JOB_ID Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;); Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;); After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; "},{"id":81,"href":"/flink/sql-alter/","title":"SQL Alter","section":"Engine Flink","content":"\rAltering Tables\r#\rChanging/Adding Table Properties\r#\rThe following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties\r#\rThe following SQL removes write-buffer-size table property.\nALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment\r#\rThe following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment\r#\rThe following SQL removes table comment.\nALTER TABLE my_table RESET (\u0026#39;comment\u0026#39;); Rename Table Name\r#\rThe following SQL rename the table name to new name.\nALTER TABLE my_table RENAME TO my_table_new; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.\rAdding New Columns\r#\rThe following SQL adds two columns c1 and c2 to table my_table.\nTo add a column in a row type, see [Changing Column Type](#changing-column-type).\rALTER TABLE my_table ADD (c1 INT, c2 STRING); Renaming Column Name\r#\rThe following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME c0 TO c1; Dropping Columns\r#\rThe following SQL drops two columns c1 and c2 from table my_table.\nALTER TABLE my_table DROP (c1, c2); To drop a column in a row type, see [Changing Column Type](#changing-column-type).\rIn hive catalog, you need to ensure:\ndisable hive.metastore.disallow.incompatible.col.type.changes in your hive server or set hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your paimon catalog. Otherwise this operation may fail, throws an exception like The following columns have types incompatible with the\rexisting columns in their respective positions.\nDropping Partitions\r#\rThe following SQL drops the partitions of the paimon table.\nFor flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time.\nALTER TABLE my_table DROP PARTITION (`id` = 1); ALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); ALTER TABLE my_table DROP PARTITION (`id` = 1), PARTITION (`id` = 2); Adding Partitions\r#\rThe following SQL adds the partitions of the paimon table.\nFor flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time, only with metastore configured metastore.partitioned-table=true.\nALTER TABLE my_table ADD PARTITION (`id` = 1); ALTER TABLE my_table ADD PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); ALTER TABLE my_table ADD PARTITION (`id` = 1), PARTITION (`id` = 2); Changing Column Nullability\r#\rThe following SQL changes nullability of column coupon_info.\nCREATE TABLE my_table (id INT PRIMARY KEY NOT ENFORCED, coupon_info FLOAT NOT NULL); -- Change column `coupon_info` from NOT NULL to nullable ALTER TABLE my_table MODIFY coupon_info FLOAT; -- Change column `coupon_info` from nullable to NOT NULL -- If there are NULL values already, set table option as below to drop those records silently before altering table. SET \u0026#39;table.exec.sink.not-null-enforcer\u0026#39; = \u0026#39;DROP\u0026#39;; ALTER TABLE my_table MODIFY coupon_info FLOAT NOT NULL; Changing nullable column to NOT NULL is only supported by Flink currently.\rChanging Column Comment\r#\rThe following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table MODIFY buy_count BIGINT COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position\r#\rTo add a new column with specified position, use FIRST or AFTER col_name.\nALTER TABLE my_table ADD c INT FIRST; ALTER TABLE my_table ADD c INT AFTER b; Changing Column Position\r#\rTo modify an existent column to a new position, use FIRST or AFTER col_name.\nALTER TABLE my_table MODIFY col_a DOUBLE FIRST; ALTER TABLE my_table MODIFY col_a DOUBLE AFTER col_b; Changing Column Type\r#\rThe following SQL changes type of column col_a to DOUBLE.\nALTER TABLE my_table MODIFY col_a DOUBLE; Paimon also supports changing columns of row type, array type, and map type.\n-- col_a previously has type ARRAY\u0026lt;MAP\u0026lt;INT, ROW(f1 INT, f2 STRING)\u0026gt;\u0026gt; -- the following SQL changes f1 to BIGINT, drops f2, and adds f3 ALTER TABLE my_table MODIFY col_a ARRAY\u0026lt;MAP\u0026lt;INT, ROW(f1 BIGINT, f3 DOUBLE)\u0026gt;\u0026gt;; Adding watermark\r#\rThe following SQL adds a computed column ts from existing column log_ts, and a watermark with strategy ts - INTERVAL '1' HOUR on column ts which is marked as event time attribute of table my_table.\nALTER TABLE my_table ADD ( ts AS TO_TIMESTAMP(log_ts) AFTER log_ts, WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; HOUR ); Dropping watermark\r#\rThe following SQL drops the watermark of table my_table.\nALTER TABLE my_table DROP WATERMARK; Changing watermark\r#\rThe following SQL modifies the watermark strategy to ts - INTERVAL '2' HOUR.\nALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL \u0026#39;2\u0026#39; HOUR; ALTER DATABASE\r#\rThe following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\nALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...); Altering Database Location\r#\rThe following SQL changes location of database my_database to file:/temp/my_database.\nALTER DATABASE my_database SET (\u0026#39;location\u0026#39; = \u0026#39;file:/temp/my_database\u0026#39;); "},{"id":82,"href":"/concepts/system-tables/","title":"System Tables","section":"Concepts","content":"\rSystem Tables\r#\rPaimon provides a very rich set of system tables to help users better analyze and query the status of Paimon tables:\nQuery the status of the data table: Data System Table. Query the global status of the entire Catalog: Global System Table. Data System Table\r#\rData System tables contain metadata and information about each Paimon data table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark, Trino and StarRocks support querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`my_table$snapshots`; Snapshots Table\r#\rYou can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM my_table$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | base_manifest_list | delta_manifest_list | changelog_manifest_list | total_record_count | delta_record_count | changelog_record_count | watermark | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | 2 | 2 | 0 | 1666755855600 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | 1 | 1 | 0 | 1666755855148 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table\r#\rYou can query the historical schemas of the table through schemas table.\nSELECT * FROM my_table$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | schema_id | fields | partition_keys | primary_keys | options | comment | update_time | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-28 11:44:20.600 | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-27 11:44:15.600 | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-26 11:44:10.600 | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM my_table$snapshots s JOIN my_table$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table\r#\rYou can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM my_table$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table\r#\rIf you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n+I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation. SELECT * FROM my_table$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Binlog Table\r#\rYou can query the binlog through binlog table. In the binlog system table, the update before and update after will be packed in one row.\nCurrently, the binlog table is unable to display Flink\u0026rsquo;s computed columns.\nSELECT * FROM T$binlog; /* +------------------+----------------------+-----------------------+ | rowkind | column_0 | column_1 | +------------------+----------------------+-----------------------+ | +I | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ | +U | [col_0_ub, col_0_ua] | [col_1_ub, col_1_ua] | +------------------+----------------------+-----------------------+ | -D | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ */ Read-optimized Table\r#\rIf you require extreme reading performance and can accept reading slightly old data, you can use the ro (read-optimized) system table. Read-optimized system table improves reading performance by only scanning files which does not need merging.\nFor primary-key tables, ro system table only scans files on the topmost level. That is to say, ro system table only produces the result of the latest full compaction.\nIt is possible that different buckets carry out full compaction at difference times,\rso it is possible that the values of different keys come from different snapshots.\rFor append tables, as all files can be read without merging, ro system table acts like the normal append table.\nSELECT * FROM my_table$ro; Files Table\r#\rYou can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM my_table$files; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {2} | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| | {5} | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} | 1691551246788 | 1691551246152 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246722 | 1691551246273 |2023-02-24T16:06:21.166| | {4} | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} | 1691551246321 | 1691551246109 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 6 rows in set */ -- You can also query the files with specific snapshot SELECT * FROM my_table$files /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 3 rows in set */ Tags Table\r#\rYou can query the tag history information of the table through tags table, including which snapshots are the tags based on and some historical information of the snapshots. You can also get all tag names and time travel to a specific tag data by name.\nSELECT * FROM my_table$tags; /* +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag_name | snapshot_id | schema_id | commit_time | record_count | branches | +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag1 | 1 | 0 | 2023-06-28 14:55:29.344 | 3 | [] | | tag3 | 3 | 0 | 2023-06-28 14:58:24.691 | 7 | [branch-1] | +----------+-------------+-----------+-------------------------+--------------+--------------+ 2 rows in set */ Branches Table\r#\rYou can query the branches of the table.\nSELECT * FROM my_table$branches; /* +----------------------+-------------------------+ | branch_name | create_time | +----------------------+-------------------------+ | branch1 | 2024-07-18 20:31:39.084 | | branch2 | 2024-07-18 21:11:14.373 | +----------------------+-------------------------+ 2 rows in set */ Consumers Table\r#\rYou can query all consumers which contains next snapshot.\nSELECT * FROM my_table$consumers; /* +-------------+------------------+ | consumer_id | next_snapshot_id | +-------------+------------------+ | id1 | 1 | | id2 | 3 | +-------------+------------------+ 2 rows in set */ Manifests Table\r#\rYou can query all manifest files contained in the latest snapshot or the specified snapshot of the current table.\n-- Query the manifest of latest snapshot SELECT * FROM my_table$manifests; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | | manifest-f4dcab43-ef6b-4713... | 1648 | 1 | 0 | 0 | {20230115, 00} | {20230316, 23} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 2 rows in set */ -- You can also query the manifest with specified snapshot SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified tagName SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39;=\u0026#39;tag1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified timestamp in unix milliseconds SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39;=\u0026#39;1678883047356\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ Aggregation fields Table\r#\rYou can query the historical aggregation of the table through aggregation fields table.\nSELECT * FROM my_table$aggregation_fields; /* +------------+-----------------+--------------+--------------------------------+---------+ | field_name | field_type | function | function_options | comment | +------------+-----------------+--------------+--------------------------------+---------+ | product_id | BIGINT NOT NULL | [] | [] | \u0026lt;NULL\u0026gt; | | price | INT | [true,count] | [fields.price.ignore-retrac... | \u0026lt;NULL\u0026gt; | | sales | BIGINT | [sum] | [fields.sales.aggregate-fun... | \u0026lt;NULL\u0026gt; | +------------+-----------------+--------------+--------------------------------+---------+ 3 rows in set */ Partitions Table\r#\rYou can query the partition files of the table.\nSELECT * FROM my_table$partitions; /* +---------------+----------------+--------------------+--------------------+------------------------+ | partition | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+----------------+--------------------+--------------------+------------------------+ | {1} | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+----------------+--------------------+--------------------+------------------------+ */ Buckets Table\r#\rYou can query the bucket files of the table.\nSELECT * FROM my_table$buckets; /* +---------------+--------+----------------+--------------------+--------------------+------------------------+ | partition | bucket | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+--------+----------------+--------------------+--------------------+------------------------+ | [1] | 0 | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+--------+----------------+--------------------+--------------------+------------------------+ */ Statistic Table\r#\rYou can query the statistic information through statistic table.\nSELECT * FROM T$statistics; /* +--------------+------------+-----------------------+------------------+----------+ | snapshot_id | schema_id | mergedRecordCount | mergedRecordSize | colstat | +--------------+------------+-----------------------+------------------+----------+ | 2 | 0 | 2 | 2 | {} | +--------------+------------+-----------------------+------------------+----------+ 1 rows in set */ Table Indexes Table\r#\rYou can query the table\u0026rsquo;s index files generated for dynamic bucket table (index_type = HASH) and deletion vectors (index_type = DELETION_VECTORS) through indexes table.\nSELECT * FROM my_table$table_indexes; /* +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | partition | bucket | index_type | file_name | file_size | row_count | dv_ranges | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | {2024-10-01} | 0 | HASH | index-70abfebf-149e-4796-9f... | 12 | 3 | \u0026lt;NULL\u0026gt; | | {2024-10-01} | 0 | DELETION_VECTORS | index-633857e7-cdce-47d2-87... | 33 | 1 | [(data-346cb9c8-4032-4d66-a... | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ 2 rows in set */ Global System Table\r#\rGlobal system tables contain the statistical information of all the tables exists in paimon. For convenient of searching, we create a reference system database called sys. We can display all the global system tables by sql in flink:\nUSE sys; SHOW TABLES; ALL Options Table\r#\rThis table is similar to Options Table, but it shows all the table options is all database.\nSELECT * FROM sys.all_table_options; /* +---------------+--------------------------------+--------------------------------+------------------+ | database_name | table_name | key | value | +---------------+--------------------------------+--------------------------------+------------------+ | my_db | Orders_orc | bucket | -1 | | my_db | Orders2 | bucket | -1 | | my_db | Orders2 | sink.parallelism | 7 | | my_db2| OrdersSum | bucket | 1 | +---------------+--------------------------------+--------------------------------+------------------+ 7 rows in set */ Catalog Options Table\r#\rYou can query the catalog\u0026rsquo;s option information through catalog options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM sys.catalog_options; /* +-----------+---------------------------+ | key | value | +-----------+---------------------------+ | warehouse | hdfs:///path/to/warehouse | +-----------+---------------------------+ 1 rows in set */ "},{"id":83,"href":"/concepts/data-types/","title":"Data Types","section":"Concepts","content":"\rData Types\r#\rA data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of operations.\nAll data types supported by Paimon are as follows:\nDataType\rDescription\rBOOLEAN\rData type of a boolean with a (possibly) three-valued logic of TRUE, FALSE, and UNKNOWN.\rCHAR\nCHAR(n)\rData type of a fixed-length character string.\nThe type can be declared using CHAR(n) where n is the number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1. VARCHAR\nVARCHAR(n)\nSTRING\rData type of a variable-length character string.\nThe type can be declared using VARCHAR(n) where n is the maximum number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1. STRING is a synonym for VARCHAR(2147483647).\rBINARY\nBINARY(n)\nData type of a fixed-length binary string (=a sequence of bytes).\nThe type can be declared using BINARY(n) where n is the number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.\rVARBINARY\nVARBINARY(n)\nBYTES\rData type of a variable-length binary string (=a sequence of bytes).\nThe type can be declared using VARBINARY(n) where n is the maximum number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.\nBYTES is a synonym for VARBINARY(2147483647).\rDECIMAL\nDECIMAL(p)\nDECIMAL(p, s)\rData type of a decimal number with fixed precision and scale.\nThe type can be declared using DECIMAL(p, s) where p is the number of digits in a number (precision) and s is the number of digits to the right of the decimal point in a number (scale). p must have a value between 1 and 38 (both inclusive). s must have a value between 0 and p (both inclusive). The default value for p is 10. The default value for s is 0.\rTINYINT\rData type of a 1-byte signed integer with values from -128 to 127.\rSMALLINT\rData type of a 2-byte signed integer with values from -32,768 to 32,767.\rINT\rData type of a 4-byte signed integer with values from -2,147,483,648 to 2,147,483,647.\rBIGINT\rData type of an 8-byte signed integer with values from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.\rFLOAT\rData type of a 4-byte single precision floating point number.\nCompared to the SQL standard, the type does not take parameters.\rDOUBLE\rData type of an 8-byte double precision floating point number.\rDATE\rData type of a date consisting of year-month-day with values ranging from 0000-01-01 to 9999-12-31.\nCompared to the SQL standard, the range starts at year 0000.\rTIME\nTIME(p)\rData type of a time without time zone consisting of hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 00:00:00.000000000 to 23:59:59.999999999.\nThe type can be declared using TIME(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 0.\rTIMESTAMP\nTIMESTAMP(p)\rData type of a timestamp without time zone consisting of year-month-day hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 to 9999-12-31 23:59:59.999999999.\nThe type can be declared using TIMESTAMP(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.\rTIMESTAMP WITH LOCAL TIME ZONE\nTIMESTAMP(p) WITH LOCAL TIME ZONE\rData type of a timestamp with local time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.\nThis type fills the gap between time zone free and time zone mandatory timestamp types by allowing the interpretation of UTC timestamps according to the configured session time zone. A conversion from and to int describes the number of seconds since epoch. A conversion from and to long describes the number of milliseconds since epoch.\rARRAY\u0026lt;t\u0026gt;\rData type of an array of elements with same subtype.\nCompared to the SQL standard, the maximum cardinality of an array cannot be specified but is fixed at 2,147,483,647. Also, any valid type is supported as a subtype.\nThe type can be declared using ARRAY\u0026lt;t\u0026gt; where t is the data type of the contained elements.\rMAP\u0026lt;kt, vt\u0026gt;\rData type of an associative array that maps keys (including NULL) to values (including NULL). A map cannot contain duplicate keys; each key can map to at most one value.\nThere is no restriction of element types; it is the responsibility of the user to ensure uniqueness.\nThe type can be declared using MAP\u0026lt;kt, vt\u0026gt; where kt is the data type of the key elements and vt is the data type of the value elements.\rMULTISET\u0026lt;t\u0026gt;\rData type of a multiset (=bag). Unlike a set, it allows for multiple instances for each of its elements with a common subtype. Each unique value (including NULL) is mapped to some multiplicity.\nThere is no restriction of element types; it is the responsibility of the user to ensure uniqueness.\nThe type can be declared using MULTISET\u0026lt;t\u0026gt; where t is the data type of the contained elements.\rROW\u0026lt;n0 t0, n1 t1, ...\u0026gt;\nROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt;\rData type of a sequence of fields.\nA field consists of a field name, field type, and an optional description. The most specific type of a row of a table is a row type. In this case, each column of the row corresponds to the field of the row type that has the same ordinal position as the column.\nCompared to the SQL standard, an optional field description simplifies the handling with complex structures.\nA row type is similar to the STRUCT type known from other non-standard-compliant frameworks.\nThe type can be declared using ROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt; where n is the unique name of a field, t is the logical type of a field, d is the description of a field.\r"},{"id":84,"href":"/maintenance/manage-tags/","title":"Manage Tags","section":"Maintenance","content":"\rManage Tags\r#\rPaimon\u0026rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.\nTo solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nAutomatic Creation\r#\rPaimon supports automatic creation of tags in writing job.\nStep 1: Choose Creation Mode\nYou can set creation mode by table option 'tag.automatic-creation'. Supported values are:\nprocess-time: Create TAG based on the time of the machine. watermark: Create TAG based on the watermark of the Sink input. batch: In a batch processing scenario, a tag is generated after the current task is completed. If you choose Watermark, you may need to specify the time zone of watermark, if watermark is not in the\rUTC time zone, please configure `'sink.watermark-time-zone'`.\rStep 2: Choose Creation Period\nWhat frequency is used to generate tags. You can choose 'daily', 'hourly' and 'two-hours' for 'tag.creation-period'.\nIf you need to wait for late data, you can configure a delay time: 'tag.creation-delay'.\nStep 3: Automatic deletion of tags\nYou can configure 'tag.num-retained-max' or tag.default-time-retained to delete tags automatically.\nExample, configure table to create a tag at 0:10 every day, with a maximum retention time of 3 months:\n-- Flink SQL CREATE TABLE t ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, ... ) WITH ( \u0026#39;tag.automatic-creation\u0026#39; = \u0026#39;process-time\u0026#39;, \u0026#39;tag.creation-period\u0026#39; = \u0026#39;daily\u0026#39;, \u0026#39;tag.creation-delay\u0026#39; = \u0026#39;10 m\u0026#39;, \u0026#39;tag.num-retained-max\u0026#39; = \u0026#39;90\u0026#39; ); INSERT INTO t SELECT ...; -- Spark SQL -- Read latest snapshot SELECT * FROM t; -- Read Tag snapshot SELECT * FROM t VERSION AS OF \u0026#39;2023-07-26\u0026#39;; -- Read Incremental between Tags SELECT * FROM paimon_incremental_query(\u0026#39;t\u0026#39;, \u0026#39;2023-07-25\u0026#39;, \u0026#39;2023-07-26\u0026#39;); See Query Tables to see more query for Spark.\nCreate Tags\r#\rYou can create a tag with given name and snapshot ID.\nFlink SQL\rRun the following command:\nCALL sys.create_tag(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;, [snapshot_id =\u0026gt; \u0026lt;snapshot-id\u0026gt;]); If snapshot_id unset, snapshot_id defaults to the latest.\nFlink Action\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ create_tag \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --tag_name \u0026lt;tag-name\u0026gt; \\ [--snapshot \u0026lt;snapshot_id\u0026gt;] \\ [--time_retained \u0026lt;time-retained\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] If snapshot unset, snapshot_id defaults to the latest.\nJava API\rimport org.apache.paimon.table.Table; public class CreateTag { public static void main(String[] args) { Table table = ...; table.createTag(\u0026#34;my-tag\u0026#34;, 1); table.createTag(\u0026#34;my-tag-retained-12-hours\u0026#34;, 1, Duration.ofHours(12)); } } Spark\rRun the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2); To create a tag with retained 1 day, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2, time_retained =\u0026gt; \u0026#39;1 d\u0026#39;); To create a tag based on the latest snapshot id, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;); Delete Tags\r#\rYou can delete a tag by its name.\nFlink SQL\rRun the following command:\nCALL sys.delete_tag(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;); Flink Action\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ delete_tag \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --tag_name \u0026lt;tag-name\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API\rimport org.apache.paimon.table.Table; public class DeleteTag { public static void main(String[] args) { Table table = ...; table.deleteTag(\u0026#34;my-tag\u0026#34;); } } Spark\rRun the following sql:\nCALL sys.delete_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;); Rollback to Tag\r#\rRollback table to a specific tag. All snapshots and tags whose snapshot id is larger than the tag will be deleted (and the data will be deleted too).\nFlink SQL\rRun the following command:\nCALL sys.rollback_to(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;); Flink Action\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ rollback_to \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --version \u0026lt;tag-name\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API\rimport org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback: // snapshot-3 [expired] -\u0026gt; tag3 // snapshot-4 [expired] // snapshot-5 -\u0026gt; tag5 // snapshot-6 // snapshot-7 table.rollbackTo(\u0026#34;tag3\u0026#34;); // after rollback: // snapshot-3 -\u0026gt; tag3 } } Spark\rRun the following sql:\nCALL sys.rollback(table =\u0026gt; \u0026#39;test.t\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;); "},{"id":85,"href":"/primary-key-table/query-performance/","title":"Query Performance","section":"Table with PK","content":"\rQuery Performance\r#\rTable Mode\r#\rThe table schema has the greatest impact on query performance. See Table Mode.\nFor Merge On Read table, the most important thing you should pay attention to is the number of buckets, which will limit the concurrency of reading data.\nFor MOW (Deletion Vectors) or COW table or Read Optimized table, there is no limit to the concurrency of reading data, and they can also utilize some filtering conditions for non-primary-key columns.\nData Skipping By Primary Key Filter\r#\rFor a regular bucketed table (For example, bucket = 5), the filtering conditions of the primary key will greatly accelerate queries and reduce the reading of a large number of files.\nData Skipping By File Index\r#\rYou can use file index to table with Deletion Vectors enabled, it filters files by index on the read side.\nCREATE TABLE \u0026lt;PAIMON_TABLE\u0026gt; WITH ( \u0026#39;deletion-vectors.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;file-index.bloom-filter.columns\u0026#39; = \u0026#39;c1,c2\u0026#39;, \u0026#39;file-index.bloom-filter.c1.items\u0026#39; = \u0026#39;200\u0026#39; ); Supported filter types:\nBloom Filter:\nfile-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.fpp to config false positive probability. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.items to config the expected distinct items in one data file. Bitmap:\nfile-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap. Bit-Slice Index Bitmap\nfile-index.bsi.columns: specify the columns that need bsi index. More filter types will be supported\u0026hellip;\nIf you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.\u0026lt;filter-type\u0026gt;.columns to the table.\nHow to invoke: see flink procedures\n"},{"id":86,"href":"/maintenance/metrics/","title":"Metrics","section":"Maintenance","content":"\rPaimon Metrics\r#\rPaimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.\nIn Paimon\u0026rsquo;s metrics system, metrics are updated and reported at table granularity.\nThere are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.\nGauge: Provides a value of any type at a point in time. Counter: Used to count values by incrementing and decrementing. Histogram: Measure the statistical distribution of a set of values including the min, max, mean, standard deviation and percentile. Paimon has supported built-in metrics to measure operations of commits, scans, writes and compactions, which can be bridged to any computing engine that supports, like Flink, Spark etc.\nMetrics List\r#\rBelow is lists of Paimon built-in metrics. They are summarized into types of scan metrics, commit metrics, write metrics, write buffer metrics and compaction metrics.\nScan Metrics\r#\rMetrics Name\rType\rDescription\rlastScanDuration\rGauge\rThe time it took to complete the last scan.\rscanDuration\rHistogram\rDistributions of the time taken by the last few scans.\rlastScannedManifests\rGauge\rNumber of scanned manifest files in the last scan.\rlastScanSkippedTableFiles\rGauge\rTotal skipped table files in the last scan.\rlastScanResultedTableFiles\rGauge\rResulted table files in the last scan.\rCommit Metrics\r#\rMetrics Name\rType\rDescription\rlastCommitDuration\rGauge\rThe time it took to complete the last commit.\rcommitDuration\rHistogram\rDistributions of the time taken by the last few commits.\rlastCommitAttempts\rGauge\rThe number of attempts the last commit made.\rlastTableFilesAdded\rGauge\rNumber of added table files in the last commit, including newly created data files and compacted after.\rlastTableFilesDeleted\rGauge\rNumber of deleted table files in the last commit, which comes from compacted before.\rlastTableFilesAppended\rGauge\rNumber of appended table files in the last commit, which means the newly created data files.\rlastTableFilesCommitCompacted\rGauge\rNumber of compacted table files in the last commit, including compacted before and after.\rlastChangelogFilesAppended\rGauge\rNumber of appended changelog files in last commit.\rlastChangelogFileCommitCompacted\rGauge\rNumber of compacted changelog files in last commit.\rlastGeneratedSnapshots\rGauge\rNumber of snapshot files generated in the last commit, maybe 1 snapshot or 2 snapshots.\rlastDeltaRecordsAppended\rGauge\rDelta records count in last commit with APPEND commit kind.\rlastChangelogRecordsAppended\rGauge\rChangelog records count in last commit with APPEND commit kind.\rlastDeltaRecordsCommitCompacted\rGauge\rDelta records count in last commit with COMPACT commit kind.\rlastChangelogRecordsCommitCompacted\rGauge\rChangelog records count in last commit with COMPACT commit kind.\rlastPartitionsWritten\rGauge\rNumber of partitions written in the last commit.\rlastBucketsWritten\rGauge\rNumber of buckets written in the last commit.\rlastCompactionInputFileSize\rGauge\rTotal size of the input files for the last compaction.\rlastCompactionOutputFileSize\rGauge\rTotal size of the output files for the last compaction.\rWrite Buffer Metrics\r#\rMetrics Name\rType\rDescription\rnumWriters\rGauge\rNumber of writers in this parallelism.\rbufferPreemptCount\rGauge\rThe total number of memory preempted.\rusedWriteBufferSizeByte\rGauge\rCurrent used write buffer size in byte.\rtotalWriteBufferSizeByte\rGauge\rThe total write buffer size configured in byte.\rCompaction Metrics\r#\rMetrics Name\rType\rDescription\rmaxLevel0FileCount\rGauge\rThe maximum number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.\ravgLevel0FileCount\rGauge\rThe average number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.\rcompactionThreadBusy\rGauge\rThe maximum business of compaction threads in this task. Currently, there is only one compaction thread in each parallelism, so value of business ranges from 0 (idle) to 100 (compaction running all the time).\ravgCompactionTime\rGauge\rThe average runtime of compaction threads, calculated based on recorded compaction time data in milliseconds. The value represents the average duration of compaction operations. Higher values indicate longer average compaction times, which may suggest the need for performance optimization.\rcompactionCompletedCount\rCounter\rThe total number of compactions that have completed.\rcompactionQueuedCount\rCounter\rThe total number of compactions that are queued/running.\rmaxCompactionInputSize\rGauge\rThe maximum input file size for this task's compaction.\ravgCompactionInputSize/td\u003e\rGauge\rThe average input file size for this task's compaction.\rmaxCompactionOutputSize\rGauge\rThe maximum output file size for this task's compaction.\ravgCompactionOutputSize\rGauge\rThe average output file size for this task's compaction.\rmaxTotalFileSize\rGauge\rThe maximum total file size of an active (currently being written) bucket.\ravgTotalFileSize\rGauge\rThe average total file size of all active (currently being written) buckets.\rBridging To Flink\r#\rPaimon has implemented bridging metrics to Flink\u0026rsquo;s metrics system, which can be reported by Flink, and the lifecycle of metric groups are managed by Flink.\nPlease join the \u0026lt;scope\u0026gt;.\u0026lt;infix\u0026gt;.\u0026lt;metric_name\u0026gt; to get the complete metric identifier when using Flink to access Paimon, metric_name can be got from Metric List.\nFor example, the identifier of metric lastPartitionsWritten for table word_count in Flink job named insert_word_count is:\nlocalhost.taskmanager.localhost:60340-775a20.insert_word_count.Global Committer : word_count.0.paimon.table.word_count.commit.lastPartitionsWritten.\nFrom Flink Web-UI, go to the committer operator\u0026rsquo;s metrics, it\u0026rsquo;s shown as:\n0.Global_Committer___word_count.paimon.table.word_count.commit.lastPartitionsWritten.\n1. Please refer to [System Scope](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/#system-scope) to understand Flink `scope`\r2. Scan metrics are only supported by Flink versions \u003e= 1.18\rScope\rInfix\rScan Metrics\r\u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt;\r\u0026lt;source_operator_name\u0026gt;.coordinator. enumerator.paimon.table.\u0026lt;table_name\u0026gt;.scan\rCommit Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\rpaimon.table.\u0026lt;table_name\u0026gt;.commit\rWrite Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\rpaimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.writer\rWrite Buffer Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\rpaimon.table.\u0026lt;table_name\u0026gt;.writeBuffer\rCompaction Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\rpaimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.compaction\rFlink Source Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;source_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\r-\rFlink Sink Metrics\r\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;\r-\rFlink Connector Standard Metrics\r#\rWhen using Flink to read and write, Paimon has implemented some key standard Flink connector metrics to measure the source latency and output of sink, see FLIP-33: Standardize Connector Metrics. Flink source / sink metrics implemented are listed here.\nSource Metrics (Flink)\r#\rMetrics Name\rLevel\rType\rDescription\rcurrentEmitEventTimeLag\rFlink Source Operator\rGauge\rTime difference between sending the record out of source and file creation.\rcurrentFetchEventTimeLag\rFlink Source Operator\rGauge\rTime difference between reading the data file and file creation.\rPlease note that if you specified `consumer-id` in your streaming query, the level of source metrics should turn into the reader operator, which is behind the `Monitor` operator.\rSink Metrics (Flink)\r#\rMetrics Name\rLevel\rType\rDescription\rnumBytesOut\rTable\rCounter\rThe total number of output bytes.\rnumBytesOutPerSecond\rTable\rMeter\rThe output bytes per second.\rnumRecordsOut\rTable\rCounter\rThe total number of output records.\rnumRecordsOutPerSecond\rTable\rMeter\rThe output records per second.\r"},{"id":87,"href":"/concepts/spec/","title":"Specification","section":"Concepts","content":"\r"},{"id":88,"href":"/maintenance/manage-privileges/","title":"Manage Privileges","section":"Maintenance","content":"\rManage Privileges\r#\rPaimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.\nCurrently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.\nThis privilege system only prevents unwanted users from accessing tables through catalogs.\rIt does not block access through temporary table (by specifying table path on filesystem),\rnor does it prevent user from directly modifying data files on filesystem.\rIf you need more serious protection, use a filesystem with access management instead.\rBasic Concepts\r#\rWe now introduce the basic concepts of the privilege system.\nObject\r#\rAn object is an entity to which access can be granted. Unless allowed by a grant, access is denied.\nCurrently, the privilege system in Paimon has three types of objects: CATALOG, DATABASE and TABLE. Objects have a logical hierarchy, which is related to the concept they represent. For example:\nIf a user is granted a privilege on the catalog, he will also have this privilege on all databases and all tables in the catalog. If a user is granted a privilege on the database, he will also have this privilege on all tables in that database. If a user is revoked a privilege from the catalog, he will also lose this privilege on all databases and all tables in the catalog. If a user is revoked a privilege from the database, he will also lose this privilege on all tables in that database. Privilege\r#\rA privilege is a defined level of access to an object. Multiple privileges can be used to control the granularity of access granted on an object. Privileges are object-specific. Different objects may have different privileges.\nCurrently, we support the following privileges.\nPrivilege Description Can be Granted on SELECT Queries data in a table. TABLE, DATABASE, CATALOG INSERT Inserts, updates or drops data in a table. Creates or drops tags and branches in a table. TABLE, DATABASE, CATALOG ALTER_TABLE Alters metadata of a table, including table name, column names, table options, etc. TABLE, DATABASE, CATALOG DROP_TABLE Drops a table. TABLE, DATABASE, CATALOG CREATE_TABLE Creates a table in a database. DATABASE, CATALOG DROP_DATABASE Drops a database. DATABASE, CATALOG CREATE_DATABASE Creates a database in the catalog. CATALOG ADMIN Creates or drops privileged users, grants or revokes privileges from users in a catalog. CATALOG User\r#\rThe entity to which privileges can be granted. Users are authenticated by their password.\nWhen the privilege system is enabled, two special users will be created automatically.\nThe root user, which is identified by the provided root password when enabling the privilege system. This user always has all privileges in the catalog. The anonymous user. This is the default user if no username and password is provided when creating the catalog. Enable Privileges\r#\rPaimon currently only supports file-based privilege system. Only catalogs with 'metastore' = 'filesystem' (the default value) or 'metastore' = 'hive' support such privilege system.\nTo enable the privilege system on a filesystem / Hive catalog, do the following steps.\nFlink 1.18\u0026#43;\rRun the following Flink SQL.\n-- use the catalog where you want to enable the privilege system USE CATALOG `my-catalog`; -- initialize privilege system by providing a root password -- change \u0026#39;root-password\u0026#39; to the password you want CALL sys.init_file_based_privilege(\u0026#39;root-password\u0026#39;); After the privilege system is enabled, please re-create the catalog and authenticate as root to create other users and grant them privileges.\nPrivilege system does not affect existing catalogs.\rThat is, these catalogs can still access and modify the tables freely.\rPlease drop and re-create all catalogs with the desired warehouse path\rif you want to use the privilege system in these catalogs.\rAccessing Privileged Catalogs\r#\rTo access a privileged catalog and to be authenticated as a user, you need to define user and password catalog options when creating the catalog. For example, the following SQL creates a catalog while trying to be authenticated as root, whose password is mypassword.\nFlink\rCREATE CATALOG `my-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, -- ... \u0026#39;user\u0026#39; = \u0026#39;root\u0026#39;, \u0026#39;password\u0026#39; = \u0026#39;mypassword\u0026#39; ); Creating Users\r#\rYou must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to create a user in the privilege system.\nFlink 1.18\u0026#43;\rRun the following Flink SQL.\n-- use the catalog where you want to create a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- create a user authenticated by the specified password -- change \u0026#39;user\u0026#39; and \u0026#39;password\u0026#39; to the username and password you want CALL sys.create_privileged_user(\u0026#39;user\u0026#39;, \u0026#39;password\u0026#39;); Dropping Users\r#\rYou must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to drop a user in the privilege system.\nFlink 1.18\u0026#43;\rRun the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- change \u0026#39;user\u0026#39; to the username you want to drop CALL sys.drop_privileged_user(\u0026#39;user\u0026#39;); Granting Privileges to Users\r#\rYou must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to grant a user with privilege in the privilege system.\nFlink 1.18\u0026#43;\rRun the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;); Revoking Privileges to Users\r#\rYou must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to revoke a privilege from user in the privilege system.\nFlink 1.18\u0026#43;\rRun the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;); "},{"id":89,"href":"/maintenance/manage-branches/","title":"Manage Branches","section":"Maintenance","content":"\rManage Branches\r#\rIn streaming data processing, it\u0026rsquo;s difficult to correct data for it may affect the existing data, and users will see the streaming provisional results, which is not expected.\nWe suppose the branch that the existing workflow is processing on is \u0026lsquo;main\u0026rsquo; branch, by creating custom data branch, it can help to do experimental tests and data validating for the new job on the existing table, which doesn\u0026rsquo;t need to stop the existing reading / writing workflows and no need to copy data from the main branch.\nBy merge or replace branch operations, users can complete the correcting of data.\nCreate Branches\r#\rPaimon supports creating branch from a specific tag, or just creating an empty branch which means the initial state of the created branch is like an empty table.\nFlink SQL\rRun the following sql:\n-- create branch named \u0026#39;branch1\u0026#39; from tag \u0026#39;tag1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;, \u0026#39;tag1\u0026#39;); -- create empty branch named \u0026#39;branch1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ create_branch \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --branch_name \u0026lt;branch-name\u0026gt; \\ [--tag_name \u0026lt;tag-name\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Spark SQL\rRun the following sql:\n-- create branch named \u0026#39;branch1\u0026#39; from tag \u0026#39;tag1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;, \u0026#39;tag1\u0026#39;); -- create empty branch named \u0026#39;branch1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;empty_branch\u0026#39;); Delete Branches\r#\rYou can delete branch by its name.\n__Note:__ The `Delete Branches` operation only deletes the metadata file. If you want to clear the data written during the branch, use [remove_orphan_files](https://example.org/flink/procedures/)\rFlink SQL\rRun the following sql:\nCALL sys.delete_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ delete_branch \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --branch_name \u0026lt;branch-name\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Spark SQL\rRun the following sql:\nCALL sys.delete_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Read / Write With Branch\r#\rYou can read or write with branch as below.\nFlink\r-- read from branch \u0026#39;branch1\u0026#39; SELECT * FROM `t$branch_branch1`; SELECT * FROM `t$branch_branch1` /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;) */; -- write to branch \u0026#39;branch1\u0026#39; INSERT INTO `t$branch_branch1` SELECT ... Spark SQL\r-- read from branch \u0026#39;branch1\u0026#39; SELECT * FROM `t$branch_branch1`; -- write to branch \u0026#39;branch1\u0026#39; INSERT INTO `t$branch_branch1` SELECT ... Spark DataFrame\r-- read from branch \u0026#39;branch1\u0026#39; spark.read.format(\u0026#34;paimon\u0026#34;).option(\u0026#34;branch\u0026#34;, \u0026#34;branch1\u0026#34;).table(\u0026#34;t\u0026#34;) Fast Forward\r#\rFast-Forward the custom branch to main will delete all the snapshots, tags and schemas in the main branch that are created after the branch\u0026rsquo;s initial tag. And copy snapshots, tags and schemas from the branch to the main branch.\nIf your branch modifies the schema, after Fast Forward, if it is Spark SQL, you can execute REFRESH TABLE my_table to clean up the cache to avoid inconsistencies caused by caching.\nFlink SQL\rCALL sys.fast_forward(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar\rRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ fast_forward \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --branch_name \u0026lt;branch-name\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Batch Reading from Fallback Branch\r#\rYou can set the table option scan.fallback-branch so that when a batch job reads from the current branch, if a partition does not exist, the reader will try to read this partition from the fallback branch. For streaming read jobs, this feature is currently not supported, and will only produce results from the current branch.\nWhat\u0026rsquo;s the use case of this feature? Say you have created a Paimon table partitioned by date. You have a long-running streaming job which inserts records into Paimon, so that today\u0026rsquo;s data can be queried in time. You also have a batch job which runs at every night to insert corrected records of yesterday into Paimon, so that the preciseness of the data can be promised.\nWhen you query from this Paimon table, you would like to first read from the results of batch job. But if a partition (for example, today\u0026rsquo;s partition) does not exist in its result, then you would like to read from the results of streaming job. In this case, you can create a branch for streaming job, and set scan.fallback-branch to this streaming branch.\nLet\u0026rsquo;s look at an example.\nFlink\r-- create Paimon table CREATE TABLE T ( dt STRING NOT NULL, name STRING NOT NULL, amount BIGINT ) PARTITIONED BY (dt); -- create a branch for streaming job CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;test\u0026#39;); -- set primary key and bucket number for the branch ALTER TABLE `T$branch_test` SET ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,name\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;changelog-producer\u0026#39; = \u0026#39;lookup\u0026#39; ); -- set fallback branch ALTER TABLE T SET ( \u0026#39;scan.fallback-branch\u0026#39; = \u0026#39;test\u0026#39; ); -- write records into the streaming branch INSERT INTO `T$branch_test` VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 4), (\u0026#39;20240725\u0026#39;, \u0026#39;peach\u0026#39;, 10), (\u0026#39;20240726\u0026#39;, \u0026#39;cherry\u0026#39;, 3), (\u0026#39;20240726\u0026#39;, \u0026#39;pear\u0026#39;, 6); -- write records into the default branch INSERT INTO T VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 5), (\u0026#39;20240725\u0026#39;, \u0026#39;banana\u0026#39;, 7); SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | | 20240726 | cherry | 3 | | 20240726 | pear | 6 | +------------------+------------------+--------+ */ -- reset fallback branch ALTER TABLE T RESET ( \u0026#39;scan.fallback-branch\u0026#39; ); -- now it only reads from default branch SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | +------------------+------------------+--------+ */ "},{"id":90,"href":"/maintenance/manage-partitions/","title":"Manage Partitions","section":"Maintenance","content":"\rManage Partitions\r#\rPaimon provides multiple ways to manage partitions, including expire historical partitions by different strategies or mark a partition done to notify the downstream application that the partition has finished writing.\nExpiring Partitions\r#\rYou can set partition.expiration-time when creating a partitioned table. Paimon streaming sink will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: you can set partition.expiration-strategy when creating a partitioned table, this strategy determines how to extract the partition time and compare it with the current time to see if survival time has exceeded the partition.expiration-time. Expiration strategy supported values are:\nvalues-time : The strategy compares the time extracted from the partition value with the current time, this strategy as the default. update-time : The strategy compares the last update time of the partition with the current time. What is the scenario for this strategy: Your partition value is non-date formatted. You only want to keep data that has been updated in the last n days/months/years. Data initialization imports a large amount of historical data. __Note:__ After the partition expires, it is logically deleted and the latest snapshot cannot query its data. But the\rfiles in the file system are not immediately physically deleted, it depends on when the corresponding snapshot expires.\rSee [Expire Snapshots](https://example.org/maintenance/manage-snapshots/#expire-snapshots).\rAn example for single partition field:\nvalues-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; -- this is required in `values-time` strategy. ); -- Let\u0026#39;s say now the date is 2024-07-09，so before the date of 2024-07-02 will expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-07-01\u0026#39;); -- An example for multiple partition fields CREATE TABLE t (...) PARTITIONED BY (other_key, dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39; = \u0026#39;$dt\u0026#39; ); update-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.expiration-strategy\u0026#39; = \u0026#39;update-time\u0026#39; ); -- The last update time of the partition is now, so it will not expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-01-01\u0026#39;); -- Support non-date formatted partition. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;par-1\u0026#39;); More options:\nOption\rDefault\rType\rDescription\rpartition.expiration-strategy\rvalues-time\rString\rSpecifies the expiration strategy for partition expiration. Possible values:\rvalues-time: The strategy compares the time extracted from the partition value with the current time.\rupdate-time: The strategy compares the last update time of the partition with the current time.\rpartition.expiration-check-interval\r1 h\rDuration\rThe check interval of partition expiration.\rpartition.expiration-time\r(none)\rDuration\rThe expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.\rpartition.timestamp-formatter\r(none)\rString\rThe formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.\rpartition.timestamp-pattern\r(none)\rString\rYou can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.\rend-input.check-partition-expire\rfalse\rBoolean\rWhether check partition expire after batch mode or bounded stream job finish.\rPartition Mark Done\r#\rYou can use the option 'partition.mark-done-action' to configure the action when a partition needs to be mark done.\nsuccess-file: add \u0026lsquo;_success\u0026rsquo; file to directory. done-partition: add \u0026lsquo;xxx.done\u0026rsquo; partition to metastore. mark-event: mark partition event to metastore. http-report: report partition mark done to remote http server. custom: use policy class to create a mark-partition policy. These actions can be configured at the same time: \u0026lsquo;done-partition,success-file,mark-event,custom\u0026rsquo;. Paimon partition mark done can be triggered both by streaming write and batch write.\nStreaming Mark Done\r#\rYou can use the options 'partition.idle-time-to-done' to set a partition idle time to done duration. When a partition has no new data after this time duration, the mark done action will be triggered to indicate that the data is ready.\nBy default, Flink will use process time as idle time to trigger partition mark done. You can also use watermark to trigger partition mark done. This will make the partition mark done time more accurate when data is delayed. You can enable this by setting 'partition.mark-done-action.mode' = 'watermark'.\nBatch Mark Done\r#\rFor batch mode, you can trigger partition mark done when end input by setting 'partition.end-input-to-done'='true'.\n"},{"id":91,"href":"/ecosystem/","title":"Ecosystem","section":"Apache Paimon","content":"\r"},{"id":92,"href":"/cdc-ingestion/","title":"CDC Ingestion","section":"Apache Paimon","content":"\r"},{"id":93,"href":"/flink/clone-tables/","title":"Clone Tables","section":"Engine Flink","content":"\rClone Tables\r#\rPaimon supports cloning tables for data migration. Currently, only table files used by the latest snapshot will be cloned.\nTo clone a table, run the following command to submit a clone job. If the table you clone is not modified at the same time, it is recommended to submit a Flink batch job for better performance. However, if you want to clone the table while writing it at the same time, submit a Flink streaming job for automatic failure recovery.\nFlink SQL\rCALL sys.clone( warehouse =\u0026gt; \u0026#39;source_warehouse_path\u0026#39;, [`database` =\u0026gt; \u0026#39;source_database_name\u0026#39;,] [`table` =\u0026gt; \u0026#39;source_table_name\u0026#39;,] target_warehouse =\u0026gt; \u0026#39;target_warehouse_path\u0026#39;, [target_database =\u0026gt; \u0026#39;target_database_name\u0026#39;,] [target_table =\u0026gt; \u0026#39;target_table_name\u0026#39;,] [parallelism =\u0026gt; \u0026lt;parallelism\u0026gt;] ); If database is not specified, all tables in all databases of the specified warehouse will be cloned. If table is not specified, all tables of the specified database will be cloned. Example: Clone test_db.test_table from source warehouse to target warehouse.\nCALL sys.clone( `warehouse` =\u0026gt; \u0026#39;s3:///path/to/warehouse_source\u0026#39;, `database` =\u0026gt; \u0026#39;test_db\u0026#39;, `table` =\u0026gt; \u0026#39;test_table\u0026#39;, `catalog_conf` =\u0026gt; \u0026#39;s3.endpoint=https://****.com;s3.access-key=*****;s3.secret-key=*****\u0026#39;, `target_warehouse` =\u0026gt; \u0026#39;s3:///path/to/warehouse_target\u0026#39;, `target_database` =\u0026gt; \u0026#39;test_db\u0026#39;, `target_table` =\u0026gt; \u0026#39;test_table\u0026#39;, `target_catalog_conf` =\u0026gt; \u0026#39;s3.endpoint=https://****.com;s3.access-key=*****;s3.secret-key=*****\u0026#39; ); Flink Action\r\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ clone \\ --warehouse \u0026lt;source-warehouse-path\u0026gt; \\ [--database \u0026lt;source-database-name\u0026gt;] \\ [--table \u0026lt;source-table-name\u0026gt;] \\ [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; ...]] \\ --target_warehouse \u0026lt;target-warehouse-path\u0026gt; \\ [--target_database \u0026lt;target-database\u0026gt;] \\ [--target_table \u0026lt;target-table-name\u0026gt;] \\ [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; ...]] [--parallelism \u0026lt;parallelism\u0026gt;] If database is not specified, all tables in all databases of the specified warehouse will be cloned. If table is not specified, all tables of the specified database will be cloned. Example: Clone test_db.test_table from source warehouse to target warehouse.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ clone \\ --warehouse s3:///path/to/warehouse_source \\ --database test_db \\ --table test_table \\ --catalog_conf s3.endpoint=https://****.com \\ --catalog_conf s3.access-key=***** \\ --catalog_conf s3.secret-key=***** \\ --target_warehouse s3:///path/to/warehouse_target \\ --target_database test_db \\ --target_table test_table \\ --target_catalog_conf s3.endpoint=https://****.com \\ --target_catalog_conf s3.access-key=***** \\ --target_catalog_conf s3.secret-key=***** For more usage of the clone action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ clone --help "},{"id":94,"href":"/maintenance/","title":"Maintenance","section":"Apache Paimon","content":"\r"},{"id":95,"href":"/program-api/","title":"Program API","section":"Apache Paimon","content":"\r"},{"id":96,"href":"/migration/","title":"Migration","section":"Apache Paimon","content":"\r"},{"id":97,"href":"/flink/procedures/","title":"Procedures","section":"Engine Flink","content":"\rProcedures\r#\rFlink 1.18 and later versions support Call Statements, which make it easier to manipulate data and metadata of Paimon table by writing SQLs instead of submitting Flink jobs.\nIn 1.18, the procedure only supports passing arguments by position. You must pass all arguments in order, and if you don\u0026rsquo;t want to pass some arguments, you must use '' as placeholder. For example, if you want to compact table default.t with parallelism 4, but you don\u0026rsquo;t want to specify partitions and sort strategy, the call statement should be CALL sys.compact('default.t', '', '', '', 'sink.parallelism=4').\nIn higher versions, the procedure supports passing arguments by name. You can pass arguments in any order and any optional argument can be omitted. For the above example, the call statement is CALL sys.compact(`table` =\u0026gt; 'default.t', options =\u0026gt; 'sink.parallelism=4').\nSpecify partitions: we use string to represent partition filter. \u0026ldquo;,\u0026rdquo; means \u0026ldquo;AND\u0026rdquo; and \u0026ldquo;;\u0026rdquo; means \u0026ldquo;OR\u0026rdquo;. For example, if you want to specify two partitions date=01 and date=02, you need to write \u0026lsquo;date=01;date=02\u0026rsquo;; If you want to specify one partition with date=01 and day=01, you need to write \u0026lsquo;date=01,day=01\u0026rsquo;.\nTable options syntax: we use string to represent table options. The format is \u0026lsquo;key1=value1,key2=value2\u0026hellip;\u0026rsquo;.\nAll available procedures are listed below.\nProcedure Name\rUsage\rExplanation\rExample\rcompact\r-- Use named argument\rCALL [catalog.]sys.compact(\r`table` =\u003e 'table', partitions =\u003e 'partitions', order_strategy =\u003e 'order_strategy', order_by =\u003e 'order_by', options =\u003e 'options', `where` =\u003e 'where', partition_idle_time =\u003e 'partition_idle_time',\rcompact_strategy =\u003e 'compact_strategy') -- Use indexed argument\rCALL [catalog.]sys.compact('table') CALL [catalog.]sys.compact('table', 'partitions') CALL [catalog.]sys.compact('table', 'order_strategy', 'order_by') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time') CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time', 'compact_strategy') To compact a table. Arguments:\rtable(required): the target table identifier.\rpartitions(optional): partition filter.\rorder_strategy(optional): 'order' or 'zorder' or 'hilbert' or 'none'.\rorder_by(optional): the columns need to be sort. Left empty if 'order_strategy' is 'none'.\roptions(optional): additional dynamic options of the table.\rwhere(optional): partition predicate(Can't be used together with \"partitions\"). Note: as where is a keyword,a pair of backticks need to add around like `where`.\rpartition_idle_time(optional): this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.\rcompact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.\r-- use partition filter CALL sys.compact(`table` =\u003e 'default.T', partitions =\u003e 'p=0', order_strategy =\u003e 'zorder', order_by =\u003e 'a,b', options =\u003e 'sink.parallelism=4') -- use partition predicate CALL sys.compact(`table` =\u003e 'default.T', `where` =\u003e 'dt\u003e10 and h\u003c20', order_strategy =\u003e 'zorder', order_by =\u003e 'a,b', options =\u003e 'sink.parallelism=4')\rcompact_database\r-- Use named argument\rCALL [catalog.]sys.compact_database(\rincluding_databases =\u003e 'includingDatabases', mode =\u003e 'mode', including_tables =\u003e 'includingTables', excluding_tables =\u003e 'excludingTables', table_options =\u003e 'tableOptions', partition_idle_time =\u003e 'partitionIdleTime',\rcompact_strategy =\u003e 'compact_strategy') -- Use indexed argument\rCALL [catalog.]sys.compact_database() CALL [catalog.]sys.compact_database('includingDatabases') CALL [catalog.]sys.compact_database('includingDatabases', 'mode') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime')\rCALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime', 'compact_strategy')\rTo compact databases. Arguments:\rincludingDatabases: to specify databases. You can use regular expression.\rmode: compact mode. \"divided\": start a sink for each table, detecting the new table requires restarting the job;\r\"combined\" (default): start a single combined sink for all tables, the new table will be automatically detected.\rincludingTables: to specify tables. You can use regular expression.\rexcludingTables: to specify tables that are not compacted. You can use regular expression.\rtableOptions: additional dynamic options of the table.\rpartition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted.\rcompact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.\rCALL sys.compact_database(\rincluding_databases =\u003e 'db1|db2', mode =\u003e 'combined', including_tables =\u003e 'table_.*', excluding_tables =\u003e 'ignore', table_options =\u003e 'sink.parallelism=4',\rcompat_strategy =\u003e 'full')\rcreate_tag\r-- Use named argument\r-- based on the specified snapshot CALL [catalog.]sys.create_tag(`table` =\u003e 'identifier', tag =\u003e 'tagName', snapshot_id =\u003e snapshotId) -- based on the latest snapshot CALL [catalog.]sys.create_tag(`table` =\u003e 'identifier', tag =\u003e 'tagName') -- Use indexed argument\r-- based on the specified snapshot CALL [catalog.]sys.create_tag('identifier', 'tagName', snapshotId) -- based on the latest snapshot CALL [catalog.]sys.create_tag('identifier', 'tagName')\rTo create a tag based on given snapshot. Arguments:\rtable: the target table identifier. Cannot be empty.\rtagName: name of the new tag.\rsnapshotId (Long): id of the snapshot which the new tag is based on.\rtime_retained: The maximum time retained for newly created tags.\rCALL sys.create_tag(`table` =\u003e 'default.T', tag =\u003e 'my_tag', snapshot_id =\u003e cast(10 as bigint), time_retained =\u003e '1 d')\rcreate_tag_from_timestamp\r-- Create a tag from the first snapshot whose commit-time greater than the specified timestamp. -- Use named argument\rCALL [catalog.]sys.create_tag_from_timestamp(`table` =\u003e 'identifier', tag =\u003e 'tagName', timestamp =\u003e timestamp, time_retained =\u003e time_retained) -- Use indexed argument\rCALL [catalog.]sys.create_tag_from_timestamp('identifier', 'tagName', timestamp, time_retained)\rTo create a tag based on given timestamp. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the new tag.\rtimestamp (Long): Find the first snapshot whose commit-time greater than this timestamp.\rtime_retained : The maximum time retained for newly created tags.\r-- for Flink 1.18\rCALL sys.create_tag_from_timestamp('default.T', 'my_tag', 1724404318750, '1 d')\r-- for Flink 1.19 and later\rCALL sys.create_tag_from_timestamp(`table` =\u003e 'default.T', `tag` =\u003e 'my_tag', `timestamp` =\u003e 1724404318750, time_retained =\u003e '1 d')\rcreate_tag_from_watermark\r-- Create a tag from the first snapshot whose watermark greater than the specified timestamp.\r-- Use named argument\rCALL [catalog.]sys.create_tag_from_watermark(`table` =\u003e 'identifier', tag =\u003e 'tagName', watermark =\u003e watermark, time_retained =\u003e time_retained) -- Use indexed argument\rCALL [catalog.]sys.create_tag_from_watermark('identifier', 'tagName', watermark, time_retained)\rTo create a tag based on given watermark timestamp. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the new tag.\rwatermark (Long): Find the first snapshot whose watermark greater than the specified watermark.\rtime_retained : The maximum time retained for newly created tags.\r-- for Flink 1.18\rCALL sys.create_tag_from_watermark('default.T', 'my_tag', 1724404318750, '1 d')\r-- for Flink 1.19 and later\rCALL sys.create_tag_from_watermark(`table` =\u003e 'default.T', `tag` =\u003e 'my_tag', `watermark` =\u003e 1724404318750, time_retained =\u003e '1 d')\rdelete_tag\r-- Use named argument\rCALL [catalog.]sys.delete_tag(`table` =\u003e 'identifier', tag =\u003e 'tagName') -- Use indexed argument\rCALL [catalog.]sys.delete_tag('identifier', 'tagName')\rTo delete a tag. Arguments:\rtable: the target table identifier. Cannot be empty.\rtagName: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.\rCALL sys.delete_tag(`table` =\u003e 'default.T', tag =\u003e 'my_tag')\rreplace_tag\r-- Use named argument\r-- replace tag with new time retained CALL [catalog.]sys.replace_tag(`table` =\u003e 'identifier', tag =\u003e 'tagName', time_retained =\u003e 'timeRetained') -- replace tag with new snapshot id and time retained CALL [catalog.]sys.replace_tag(`table` =\u003e 'identifier', snapshot_id =\u003e 'snapshotId') -- Use indexed argument\r-- replace tag with new snapshot id and time retained CALL [catalog.]sys.replace_tag('identifier', 'tagName', 'snapshotId', 'timeRetained') To replace an existing tag with new tag info. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the existed tag. Cannot be empty.\rsnapshot(Long): id of the snapshot which the tag is based on, it is optional.\rtime_retained: The maximum time retained for the existing tag, it is optional.\r-- for Flink 1.18\rCALL sys.replace_tag('default.T', 'my_tag', 5, '1 d')\r-- for Flink 1.19 and later\rCALL sys.replace_tag(`table` =\u003e 'default.T', tag =\u003e 'my_tag', snapshot_id =\u003e 5, time_retained =\u003e '1 d')\rexpire_tags\rCALL [catalog.]sys.expire_tags('identifier', 'older_than')\rTo expire tags by time. Arguments:\rtable: the target table identifier. Cannot be empty.\rolder_than: tagCreateTime before which tags will be removed.\rCALL sys.expire_tags(table =\u003e 'default.T', older_than =\u003e '2024-09-06 11:00:00')\rmerge_into\r-- for Flink 1.18\rCALL [catalog.]sys.merge_into('identifier','targetAlias',\r'sourceSqls','sourceTable','mergeCondition',\r'matchedUpsertCondition','matchedUpsertSetting',\r'notMatchedInsertCondition','notMatchedInsertValues',\r'matchedDeleteCondition')\r-- for Flink 1.19 and later CALL [catalog.]sys.merge_into(\rtarget_table =\u003e 'identifier',\rtarget_alias =\u003e 'targetAlias',\rsource_sqls =\u003e 'sourceSqls',\rsource_table =\u003e 'sourceTable',\rmerge_condition =\u003e 'mergeCondition',\rmatched_upsert_condition =\u003e 'matchedUpsertCondition',\rmatched_upsert_setting =\u003e 'matchedUpsertSetting',\rnot_matched_insert_condition =\u003e 'notMatchedInsertCondition',\rnot_matched_insert_values =\u003e 'notMatchedInsertValues',\rmatched_delete_condition =\u003e 'matchedDeleteCondition',\rnot_matched_by_source_upsert_condition =\u003e 'notMatchedBySourceUpsertCondition',\rnot_matched_by_source_upsert_setting =\u003e 'notMatchedBySourceUpsertSetting',\rnot_matched_by_source_delete_condition =\u003e 'notMatchedBySourceDeleteCondition') To perform \"MERGE INTO\" syntax. See merge_into action for\rdetails of arguments.\r-- for matched order rows,\r-- increase the price,\r-- and if there is no match, -- insert the order from\r-- the source table\r-- for Flink 1.18\rCALL sys.merge_into('default.T','','','default.S','T.id=S.order_id','','price=T.price+20','','*','')\r-- for Flink 1.19 and later CALL sys.merge_into(\rtarget_table =\u003e 'default.T',\rsource_table =\u003e 'default.S',\rmerge_condition =\u003e 'T.id=S.order_id',\rmatched_upsert_setting =\u003e 'price=T.price+20',\rnot_matched_insert_values =\u003e '*')\rremove_orphan_files\r-- Use named argument\rCALL [catalog.]sys.remove_orphan_files(`table` =\u003e 'identifier', older_than =\u003e 'olderThan', dry_run =\u003e 'dryRun', mode =\u003e 'mode') -- Use indexed argument\rCALL [catalog.]sys.remove_orphan_files('identifier')\rCALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan')\rCALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun')\rCALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism')\rCALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism','mode')\rTo remove the orphan data files and metadata files. Arguments:\rtable: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.\rolderThan: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.\rdryRun: when true, view only orphan files, don't actually remove files. Default is false.\rparallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.\rmode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.\rCALL sys.remove_orphan_files(`table` =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00')\rCALL sys.remove_orphan_files(`table` =\u003e 'default.*', older_than =\u003e '2023-10-31 12:00:00')\rCALL sys.remove_orphan_files(`table` =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e true)\rCALL sys.remove_orphan_files(`table` =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e false, parallelism =\u003e '5')\rCALL sys.remove_orphan_files(`table` =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e false, parallelism =\u003e '5', mode =\u003e 'local')\rremove_unexisting_files\r-- Use named argument\rCALL [catalog.]sys.remove_unexisting_files(`table` =\u003e 'identifier', dry_run =\u003e 'dryRun', parallelism =\u003e 'parallelism') -- Use indexed argument\rCALL [catalog.]sys.remove_unexisting_files('identifier')\rCALL [catalog.]sys.remove_unexisting_files('identifier', 'dryRun', 'parallelism')\rProcedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments:\rtable: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.\rdry_run (optional): only check what files will be removed, but not really remove them. Default is false.\rparallelism (optional): number of parallelisms to check files in the manifests.\rNote that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.\r-- remove unexisting data files in the table `mydb.myt`\rCALL sys.remove_unexisting_files(`table` =\u003e 'mydb.myt')\r-- only check what files will be removed, but not really remove them (dry run)\rCALL sys.remove_unexisting_files(`table` =\u003e 'mydb.myt', `dry_run` = true)\rreset_consumer\r-- Use named argument\rCALL [catalog.]sys.reset_consumer(`table` =\u003e 'identifier', consumer_id =\u003e 'consumerId', next_snapshot_id =\u003e 'nextSnapshotId') -- Use indexed argument\r-- reset the new next snapshot id in the consumer\rCALL [catalog.]sys.reset_consumer('identifier', 'consumerId', nextSnapshotId)\r-- delete consumer\rCALL [catalog.]sys.reset_consumer('identifier', 'consumerId')\rTo reset or delete consumer. Arguments:\rtable: the target table identifier. Cannot be empty.\rconsumerId: consumer to be reset or deleted.\rnextSnapshotId (Long): the new next snapshot id of the consumer.\rCALL sys.reset_consumer(`table` =\u003e 'default.T', consumer_id =\u003e 'myid', next_snapshot_id =\u003e cast(10 as bigint))\rclear_consumers\r-- Use named argument\rCALL [catalog.]sys.clear_consumers(`table` =\u003e 'identifier', including_consumers =\u003e 'includingConsumers', excluding_consumers =\u003e 'excludingConsumers') -- Use indexed argument\r-- clear all consumers in the table\rCALL [catalog.]sys.clear_consumers('identifier')\r-- clear some consumers in the table (accept regular expression)\rCALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers')\r-- exclude some consumers (accept regular expression)\rCALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers', 'excludingConsumers')\rTo reset or delete consumer. Arguments:\rtable: the target table identifier. Cannot be empty.\rincludingConsumers: consumers to be cleared.\rexcludingConsumers: consumers which not to be cleared.\rCALL sys.clear_consumers(`table` =\u003e 'default.T')\rCALL sys.clear_consumers(`table` =\u003e 'default.T', including_consumers =\u003e 'myid.*')\rCALL sys.clear_consumers(table =\u003e 'default.T', including_consumers =\u003e '', excluding_consumers =\u003e 'myid1.*')\rCALL sys.clear_consumers(table =\u003e 'default.T', including_consumers =\u003e 'myid.*', excluding_consumers =\u003e 'myid1.*')\rrollback_to\r-- for Flink 1.18\r-- rollback to a snapshot\rCALL [catalog.]sys.rollback_to('identifier', snapshotId)\r-- rollback to a tag\rCALL [catalog.]sys.rollback_to('identifier', 'tagName')\r-- for Flink 1.19 and later\r-- rollback to a snapshot\rCALL [catalog.]sys.rollback_to(`table` =\u003e 'identifier', snapshot_id =\u003e snapshotId)\r-- rollback to a tag\rCALL [catalog.]sys.rollback_to(`table` =\u003e 'identifier', tag =\u003e 'tagName')\rTo rollback to a specific version of target table. Argument:\rtable: the target table identifier. Cannot be empty.\rsnapshotId (Long): id of the snapshot that will roll back to.\rtagName: name of the tag that will roll back to.\r-- for Flink 1.18\rCALL sys.rollback_to('default.T', 10)\r-- for Flink 1.19 and later\rCALL sys.rollback_to(`table` =\u003e 'default.T', snapshot_id =\u003e 10)\rrollback_to_timestamp\r-- for Flink 1.18\r-- rollback to the snapshot which earlier or equal than timestamp.\rCALL [catalog.]sys.rollback_to_timestamp('identifier', timestamp)\r-- for Flink 1.19 and later\r-- rollback to the snapshot which earlier or equal than timestamp.\rCALL [catalog.]sys.rollback_to_timestamp(`table` =\u003e 'default.T', `timestamp` =\u003e timestamp)\rTo rollback to the snapshot which earlier or equal than timestamp. Argument:\rtable: the target table identifier. Cannot be empty.\rtimestamp (Long): Roll back to the snapshot which earlier or equal than timestamp.\r-- for Flink 1.18\rCALL sys.rollback_to_timestamp('default.T', 10)\r-- for Flink 1.19 and later\rCALL sys.rollback_to_timestamp(`table` =\u003e 'default.T', timestamp =\u003e 1730292023000)\rrollback_to_watermark\r-- for Flink 1.18\r-- rollback to the snapshot which earlier or equal than watermark.\rCALL [catalog.]sys.rollback_to_watermark('identifier', watermark)\r-- for Flink 1.19 and later\r-- rollback to the snapshot which earlier or equal than watermark.\rCALL [catalog.]sys.rollback_to_watermark(`table` =\u003e 'default.T', `watermark` =\u003e watermark)\rTo rollback to the snapshot which earlier or equal than watermark. Argument:\rtable: the target table identifier. Cannot be empty.\rwatermark (Long): Roll back to the snapshot which earlier or equal than watermark.\r-- for Flink 1.18\rCALL sys.rollback_to_watermark('default.T', 1730292023000)\r-- for Flink 1.19 and later\rCALL sys.rollback_to_watermark(`table` =\u003e 'default.T', watermark =\u003e 1730292023000)\rpurge_files\r-- clear table with purge files.\rCALL [catalog.]sys.purge_files('identifier')\rTo clear table with purge files. Argument:\rtable: the target table identifier. Cannot be empty.\rCALL sys.purge_files('default.T')\rmigrate_database\r-- for Flink 1.18\r-- migrate all hive tables in database to paimon tables.\rCALL [catalog.]sys.migrate_database('connector', 'dbIdentifier', 'options'[, \u0026ltparallelism\u0026gt])\r-- for Flink 1.19 and later\r-- migrate all hive tables in database to paimon tables.\rCALL [catalog.]sys.migrate_database(connector =\u003e 'connector', source_database =\u003e 'dbIdentifier', options =\u003e 'options'[, \u0026ltparallelism =\u003e parallelism\u0026gt])\rTo migrate all hive tables in database to paimon table. Argument:\rconnector: the origin database's type to be migrated, such as hive. Cannot be empty.\rsource_database: name of the origin database to be migrated. Cannot be empty.\roptions: the table options of the paimon table to migrate.\rparallelism: the parallelism for migrate process, default is core numbers of machine.\r-- for Flink 1.18\rCALL sys.migrate_database('hive', 'db01', 'file.format=parquet', 6)\r-- for Flink 1.19 and later\rCALL sys.migrate_database(connector =\u003e 'hive', source_database =\u003e 'db01', options =\u003e 'file.format=parquet', parallelism =\u003e 6)\rmigrate_table\r-- migrate hive table to a paimon table.\rCALL [catalog.]sys.migrate_table(connector =\u003e 'connector', source_table =\u003e 'tableIdentifier', options =\u003e 'options'[, \u0026ltparallelism =\u003e parallelism\u0026gt])\rTo migrate hive table to a paimon table. Argument:\rconnector: the origin table's type to be migrated, such as hive. Cannot be empty.\rsource_table: name of the origin table to be migrated. Cannot be empty.\rtarget_table: name of the target paimon table to migrate. If not set would keep the same name with origin table\roptions: the table options of the paimon table to migrate.\rparallelism: the parallelism for migrate process, default is core numbers of machine.\rdelete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true\rCALL sys.migrate_table(connector =\u003e 'hive', source_table =\u003e 'db01.t1', options =\u003e 'file.format=parquet', parallelism =\u003e 6)\rmigrate_iceberg_table\r-- Use named argument\rCALL sys.migrate_iceberg_table(source_table =\u003e 'database_name.table_name', iceberg_options =\u003e 'iceberg_options', options =\u003e 'paimon_options', parallelism =\u003e parallelism);\r-- Use indexed argument\rCALL sys.migrate_iceberg_table('source_table','iceberg_options', 'options', 'parallelism');\rTo migrate iceberg table to paimon. Arguments:\rsource_table: string type, is used to specify the source iceberg table to migrate, it's required.\riceberg_options: string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it's required.\roptions: string type, is used to specify the additional options for the target paimon table, it's optional.\rparallelism: integer type, is used to specify the parallelism of the migration job, it's optional.\rCALL sys.migrate_iceberg_table(source_table =\u003e 'iceberg_db.iceberg_tbl',iceberg_options =\u003e 'metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse');\rexpire_snapshots\r-- Use named argument\rCALL [catalog.]sys.expire_snapshots(\r`table` =\u003e 'identifier', retain_max =\u003e 'retain_max', retain_min =\u003e 'retain_min', older_than =\u003e 'older_than', max_deletes =\u003e 'max_deletes') -- Use indexed argument\r-- for Flink 1.18\rCALL [catalog.]sys.expire_snapshots(table, retain_max)\r-- for Flink 1.19 and later\rCALL [catalog.]sys.expire_snapshots(table, retain_max, retain_min, older_than, max_deletes)\rTo expire snapshots. Argument:\rtable: the target table identifier. Cannot be empty.\rretain_max: the maximum number of completed snapshots to retain.\rretain_min: the minimum number of completed snapshots to retain.\rorder_than: timestamp before which snapshots will be removed.\rmax_deletes: the maximum number of snapshots that can be deleted at once.\r-- for Flink 1.18\rCALL sys.expire_snapshots('default.T', 2)\r-- for Flink 1.19 and later\rCALL sys.expire_snapshots(`table` =\u003e 'default.T', retain_max =\u003e 2)\rCALL sys.expire_snapshots(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00')\rCALL sys.expire_snapshots(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00', retain_min =\u003e 10)\rCALL sys.expire_snapshots(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00', max_deletes =\u003e 10)\rexpire_changelogs\r-- Use named argument\rCALL [catalog.]sys.expire_changelogs(\r`table` =\u003e 'identifier', retain_max =\u003e 'retain_max', retain_min =\u003e 'retain_min', older_than =\u003e 'older_than', max_deletes =\u003e 'max_deletes') delete_all =\u003e 'delete_all') -- Use indexed argument\r-- for Flink 1.18\rCALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes)\rCALL [catalog.]sys.expire_changelogs(table, delete_all)\r-- for Flink 1.19 and later\rCALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes, delete_all)\rTo expire changelogs. Argument:\rtable: the target table identifier. Cannot be empty.\rretain_max: the maximum number of completed changelogs to retain.\rretain_min: the minimum number of completed changelogs to retain.\rorder_than: timestamp before which changelogs will be removed.\rmax_deletes: the maximum number of changelogs that can be deleted at once.\rdelete_all: whether to delete all separated changelogs.\r-- for Flink 1.18\rCALL sys.expire_changelogs('default.T', 4, 2, '2024-01-01 12:00:00', 2)\rCALL sys.expire_changelogs('default.T', true)\r-- for Flink 1.19 and later\rCALL sys.expire_changelogs(`table` =\u003e 'default.T', retain_max =\u003e 2)\rCALL sys.expire_changelogs(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00')\rCALL sys.expire_changelogs(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00', retain_min =\u003e 10)\rCALL sys.expire_changelogs(`table` =\u003e 'default.T', older_than =\u003e '2024-01-01 12:00:00', max_deletes =\u003e 10)\rCALL sys.expire_changelogs(`table` =\u003e 'default.T', delete_all =\u003e true)\rexpire_partitions\rCALL [catalog.]sys.expire_partitions(table, expiration_time, timestamp_formatter, expire_strategy)\rTo expire partitions. Argument:\rtable: the target table identifier. Cannot be empty.\rexpiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.\rtimestamp_formatter: the formatter to format timestamp from string.\rtimestamp_pattern: the pattern to get a timestamp from partitions.\rexpire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.\rmax_expires: The maximum of limited expired partitions, it is optional.\r-- for Flink 1.18\rCALL sys.expire_partitions('default.T', '1 d', 'yyyy-MM-dd', '$dt', 'values-time')\r-- for Flink 1.19 and later\rCALL sys.expire_partitions(`table` =\u003e 'default.T', expiration_time =\u003e '1 d', timestamp_formatter =\u003e 'yyyy-MM-dd', expire_strategy =\u003e 'values-time')\rCALL sys.expire_partitions(`table` =\u003e 'default.T', expiration_time =\u003e '1 d', timestamp_formatter =\u003e 'yyyy-MM-dd HH:mm', timestamp_pattern =\u003e '$dt $hm', expire_strategy =\u003e 'values-time')\rrepair\r-- repair all databases and tables in catalog\rCALL [catalog.]sys.repair()\r-- repair all tables in a specific database\rCALL [catalog.]sys.repair('databaseName')\r-- repair a table\rCALL [catalog.]sys.repair('databaseName.tableName')\r-- repair database and table in a string if you specify multiple tags, delimiter is ','\rCALL [catalog.]sys.repair('databaseName01,database02.tableName01,database03')\rSynchronize information from the file system to Metastore. Argument:\rempty: all databases and tables in catalog.\rdatabaseName : the target database name.\rtableName: the target table identifier.\rCALL sys.repair(`table` =\u003e 'test_db.T')\rrewrite_file_index\r-- Use named argument\rCALL [catalog.]sys.rewrite_file_index(\u0026lt`table` =\u003e identifier\u0026gt [, \u0026ltpartitions =\u003e partitions\u0026gt])\r-- Use indexed argument\rCALL [catalog.]sys.rewrite_file_index(\u0026ltidentifier\u0026gt [, \u0026ltpartitions\u0026gt])\rRewrite the file index for the table. Argument:\rtable: \u0026ltdatabaseName\u0026gt.\u0026lttableName\u0026gt.\rpartitions : specific partitions.\r-- rewrite the file index for the whole table\rCALL sys.rewrite_file_index(`table` =\u003e 'test_db.T')\r-- repair all tables in a specific partition\rCALL sys.rewrite_file_index(`table` =\u003e 'test_db.T', partitions =\u003e 'pt=a')\rcreate_branch\r-- Use named argument\rCALL [catalog.]sys.create_branch(`table` =\u003e 'identifier', branch =\u003e 'branchName', tag =\u003e 'tagName')\r-- Use indexed argument\r-- based on the specified tag CALL [catalog.]sys.create_branch('identifier', 'branchName', 'tagName')\r-- based on the specified branch's tag CALL [catalog.]sys.create_branch('branch_table', 'branchName', 'tagName')\r-- create empty branch CALL [catalog.]sys.create_branch('identifier', 'branchName')\rTo create a branch based on given tag, or just create empty branch. Arguments:\rtable: the target table identifier or branch identifier. Cannot be empty.\rbranchName: name of the new branch.\rtagName: name of the tag which the new branch is based on.\rCALL sys.create_branch(`table` =\u003e 'default.T', branch =\u003e 'branch1', tag =\u003e 'tag1')\r-- based on the specified branch's tag CALL sys.create_branch(`table` =\u003e 'default.T$branch_existBranchName', branch =\u003e 'branch1', tag =\u003e 'tag1')\rCALL sys.create_branch(`table` =\u003e 'default.T', branch =\u003e 'branch1')\rdelete_branch\r-- Use named argument\rCALL [catalog.]sys.delete_branch(`table` =\u003e 'identifier', branch =\u003e 'branchName')\r-- Use indexed argument\rCALL [catalog.]sys.delete_branch('identifier', 'branchName')\rTo delete a branch. Arguments:\rtable: the target table identifier. Cannot be empty.\rbranchName: name of the branch to be deleted. If you specify multiple branches, delimiter is ','.\rCALL sys.delete_branch(`table` =\u003e 'default.T', branch =\u003e 'branch1')\rfast_forward\r-- Use named argument\rCALL [catalog.]sys.fast_forward(`table` =\u003e 'identifier', branch =\u003e 'branchName')\r-- Use indexed argument\rCALL [catalog.]sys.fast_forward('identifier', 'branchName')\rTo fast_forward a branch to main branch. Arguments:\rtable: the target table identifier. Cannot be empty.\rbranchName: name of the branch to be merged.\rCALL sys.fast_forward(`table` =\u003e 'default.T', branch =\u003e 'branch1')\rrefresh_object_table\rCALL [catalog.]sys.refresh_object_table('identifier')\rTo refresh_object_table a object table. Arguments:\rtable: the target table identifier. Cannot be empty.\rCALL sys.refresh_object_table('default.T')\rcompact_manifest\rCALL [catalog.]sys.compact_manifest(`table` =\u003e 'identifier')\rTo compact_manifest the manifests. Arguments:\rtable: the target table identifier. Cannot be empty.\rCALL sys.compact_manifest(`table` =\u003e 'default.T')\rrescale\rCALL [catalog.]sys.rescale(`table` =\u003e 'identifier', `bucket_num` =\u003e bucket_num, `partition` =\u003e 'partition', `scan_parallelism` =\u003e 'scan_parallelism', `sink_parallelism` =\u003e 'sink_parallelism')\rRescale one partition of a table. Arguments:\rtable: The target table identifier. Cannot be empty.\rbucket_num: Resulting bucket number after rescale. The default value of argument bucket_num is the current bucket number of the table. Cannot be empty for postpone bucket tables.\rpartition: What partition to rescale. For partitioned table this argument cannot be empty.\rscan_parallelism: Parallelism of source operator. The default value is the current bucket number of the partition.\rsink_parallelism: Parallelism of sink operator. The default value is equal to bucket_num.\rCALL sys.rescale(`table` =\u003e 'default.T', `bucket_num` =\u003e 16, `partition` =\u003e 'dt=20250217,hh=08')\ralter_view_dialect\r-- add dialect in the view\rCALL [catalog.]sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query')\rCALL [catalog.]sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'add', `query` =\u003e 'query')\r-- update dialect in the view\rCALL [catalog.]sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query')\rCALL [catalog.]sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'update', `query` =\u003e 'query')\r-- drop dialect in the view\rCALL [catalog.]sys.alter_view_dialect('view_identifier', 'drop', 'flink')\rCALL [catalog.]sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'drop')\rTo alter view dialect. Arguments:\rview: the target view identifier. Cannot be empty.\raction: define change action like: add, update, drop. Cannot be empty.\rengine: when engine which is not flink need define it.\rquery: query for the dialect when action is add and update it couldn't be empty.\r-- add dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'add', `query` =\u003e 'query')\r-- update dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'update', `query` =\u003e 'query')\r-- drop dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'drop', 'flink')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'drop')\r"},{"id":98,"href":"/flink/action-jars/","title":"Action Jars","section":"Engine Flink","content":"\rAction Jars\r#\rAfter the Flink Local Cluster has been started, you can execute the action jar by using the following command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ \u0026lt;action\u0026gt; \u0026lt;args\u0026gt; The following command is used to compact a table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ compact \\ --path \u0026lt;TABLE_PATH\u0026gt; Merging into table\r#\rPaimon supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge_into\u0026rsquo; job through flink run.\nImportant table properties setting:\r1. Only [primary key table](https://example.org/primary-key-table/overview/) supports this feature.\r2. The action won't produce UPDATE_BEFORE, so it's not recommended to set 'changelog-producer' = 'input'.\rThe design referenced such syntax:\nMERGE INTO target-table USING source_table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not_matched_condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge_into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nRun the following command to submit a \u0026lsquo;merge_into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;target-table\u0026gt; \\ [--target_as \u0026lt;target-table-alias\u0026gt;] \\ --source_table \u0026lt;source_table-name\u0026gt; \\ [--source_sql \u0026lt;sql\u0026gt; ...]\\ --on \u0026lt;merge-condition\u0026gt; \\ --merge_actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\ --matched_upsert_condition \u0026lt;matched-condition\u0026gt; \\ --matched_upsert_set \u0026lt;upsert-changes\u0026gt; \\ --matched_delete_condition \u0026lt;matched-condition\u0026gt; \\ --not_matched_insert_condition \u0026lt;not-matched-condition\u0026gt; \\ --not_matched_insert_values \u0026lt;insert-values\u0026gt; \\ --not_matched_by_source_upsert_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\ --not_matched_by_source_upsert_set \u0026lt;not-matched-upsert-changes\u0026gt; \\ --not_matched_by_source_delete_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] You can pass sqls by \u0026#39;--source_sql \u0026lt;sql\u0026gt; [, --source_sql \u0026lt;sql\u0026gt; ...]\u0026#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table T \\ --source_table S \\ --on \u0026#34;T.id = S.order_id\u0026#34; \\ --merge_actions \\ matched-upsert,matched-delete \\ --matched_upsert_condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\ --matched_upsert_set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\ --matched_delete_condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table T \\ --source_table S \\ --on \u0026#34;T.id = S.order_id\u0026#34; \\ --merge_actions \\ matched-upsert,not-matched-insert \\ --matched_upsert_set \u0026#34;price = T.price + 20\u0026#34; \\ --not_matched_insert_values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table T \\ --source_table S \\ --on \u0026#34;T.id = S.order_id\u0026#34; \\ --merge_actions \\ not-matched-by-source-upsert,not-matched-by-source-delete \\ --not_matched_by_source_upsert_condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\ --not_matched_by_source_upsert_set \u0026#34;price = T.price - 20\u0026#34; \\ --not_matched_by_source_delete_condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- A --source_sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table T \\ --source_sql \u0026#34;CREATE CATALOG test_cat WITH (...)\u0026#34; \\ --source_sql \u0026#34;CREATE TEMPORARY VIEW test_cat.`default`.S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\ --source_table test_cat.default.S \\ --on \u0026#34;T.id = S.order_id\u0026#34; \\ --merge_actions not-matched-insert\\ --not_matched_insert_values * The term \u0026lsquo;matched\u0026rsquo; explanation:\nmatched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not_matched_condition (source - target). not matched by source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source). Parameters format:\nmatched_upsert_changes:\ncol = \u0026lt;source_table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target_table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target_table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_upsert_changes is similar to matched_upsert_changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert_values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source_table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_condition cannot use target table\u0026rsquo;s columns to construct condition expression. not_matched_by_source_condition cannot use source table\u0026rsquo;s columns to construct condition expression. 1. Target alias cannot be duplicated with existed table name.\r2. If the source table is not in the current catalog and current database, the source-table-name must be\rqualified (database.table or catalog.database.table if created a new catalog).\rFor examples:\\\r(1) If source table 'my_source' is in 'my_db', qualify it:\\\r\\--source_table \"my_db.my_source\"\\\r(2) Example for sqls:\\\rWhen sqls changed current catalog and database, it's OK to not qualify the source table name:\\\r\\--source_sql \"CREATE CATALOG my_cat WITH (...)\"\\\r\\--source_sql \"USE CATALOG my_cat\"\\\r\\--source_sql \"CREATE DATABASE my_db\"\\\r\\--source_sql \"USE my_db\"\\\r\\--source_sql \"CREATE TABLE S ...\"\\\r\\--source_table S\\\rbut you must qualify it in the following case:\\\r\\--source_sql \"CREATE CATALOG my_cat WITH (...)\"\\\r\\--source_sql \"CREATE TABLE my_cat.\\`default`.S ...\"\\\r\\--source_table my_cat.default.S\\\rYou can use just 'S' as source table name in following arguments.\r3. At least one merge action must be specified.\r4. If both matched-upsert and matched-delete actions are present, their conditions must both be present too\r(same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional.\r5. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally\rin Shell, please quote them with \\\"\\\" to escape blank spaces and use '\\\\' to escape special characters in statement.\rFor example:\\\r\\--source_sql \"CREATE TABLE T (k INT) WITH ('special-key' = '123\\\\!')\"\rFor more information of \u0026lsquo;merge_into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ merge_into --help Deleting from table\r#\rIn Flink 1.16 and previous versions, Paimon only supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nRun the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ delete \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ --where \u0026lt;filter_spec\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ delete --help Drop Partition\r#\rRun the following command to submit a \u0026lsquo;drop_partition\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ drop_partition \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --database \u0026lt;database-name\u0026gt; \\ --table \u0026lt;table-name\u0026gt; \\ [--partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...]] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] partition_spec: key1=value1,key2=value2... For more information of \u0026lsquo;drop_partition\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ drop_partition --help Rewrite File Index\r#\rRun the following command to submit a \u0026lsquo;rewrite_file_index\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ rewrite_file_index \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --identifier \u0026lt;database.table\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] For more information of \u0026lsquo;rewrite_file_index\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ rewrite_file_index --help "},{"id":99,"href":"/project/","title":"Project","section":"Apache Paimon","content":"\r"},{"id":100,"href":"/learn-paimon/","title":"Learn Paimon","section":"Apache Paimon","content":"\r"},{"id":101,"href":"/spark/procedures/","title":"Procedures","section":"Engine Spark","content":"\rProcedures\r#\rThis section introduce all available spark procedures about paimon.\nProcedure Name\rExplanation\rExample\rcompact\rTo compact files. Argument:\rtable: the target table identifier. Cannot be empty.\rpartitions: partition filter. the comma (\",\") represents \"AND\", the semicolon (\";\") represents \"OR\". If you want to compact one partition with date=01 and day=01, you need to write 'date=01,day=01'. Left empty for all partitions. (Can't be used together with \"where\")\rwhere: partition predicate. Left empty for all partitions. (Can't be used together with \"partitions\") order_strategy: 'order' or 'zorder' or 'hilbert' or 'none'. Left empty for 'none'.\rorder_columns: the columns need to be sort. Left empty if 'order_strategy' is 'none'.\rpartition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.\rcompact_strategy: this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.\rSET spark.sql.shuffle.partitions=10; --set the compact parallelism CALL sys.compact(table =\u003e 'T', partitions =\u003e 'p=0;p=1', order_strategy =\u003e 'zorder', order_by =\u003e 'a,b') CALL sys.compact(table =\u003e 'T', where =\u003e 'p\u003e0 and p\u003c3', order_strategy =\u003e 'zorder', order_by =\u003e 'a,b') CALL sys.compact(table =\u003e 'T', partition_idle_time =\u003e '60s')\rCALL sys.compact(table =\u003e 'T', compact_strategy =\u003e 'minor')\rexpire_snapshots\rTo expire snapshots. Argument:\rtable: the target table identifier. Cannot be empty.\rretain_max: the maximum number of completed snapshots to retain.\rretain_min: the minimum number of completed snapshots to retain.\rolder_than: timestamp before which snapshots will be removed.\rmax_deletes: the maximum number of snapshots that can be deleted at once.\rCALL sys.expire_snapshots(table =\u003e 'default.T', retain_max =\u003e 10)\rexpire_partitions\rTo expire partitions. Argument:\rtable: the target table identifier. Cannot be empty.\rexpiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.\rtimestamp_formatter: the formatter to format timestamp from string.\rtimestamp_pattern: the pattern to get a timestamp from partitions.\rexpire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.\rmax_expires: The maximum of limited expired partitions, it is optional.\rCALL sys.expire_partitions(table =\u003e 'default.T', expiration_time =\u003e '1 d', timestamp_formatter =\u003e 'yyyy-MM-dd', timestamp_pattern =\u003e '$dt', expire_strategy =\u003e 'values-time')\rcreate_tag\rTo create a tag based on given snapshot. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the new tag. Cannot be empty.\rsnapshot(Long): id of the snapshot which the new tag is based on.\rtime_retained: The maximum time retained for newly created tags.\r-- based on snapshot 10 with 1d CALL sys.create_tag(table =\u003e 'default.T', tag =\u003e 'my_tag', snapshot =\u003e 10, time_retained =\u003e '1 d') -- based on the latest snapshot CALL sys.create_tag(table =\u003e 'default.T', tag =\u003e 'my_tag')\rcreate_tag_from_timestamp\rTo create a tag based on given timestamp. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the new tag.\rtimestamp (Long): Find the first snapshot whose commit-time is greater than this timestamp.\rtime_retained : The maximum time retained for newly created tags.\rCALL sys.create_tag_from_timestamp(`table` =\u003e 'default.T', `tag` =\u003e 'my_tag', `timestamp` =\u003e 1724404318750, time_retained =\u003e '1 d')\rrename_tag\rRename a tag with a new tag name. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the tag. Cannot be empty.\rtarget_tag: the new tag name to rename. Cannot be empty.\rCALL sys.rename_tag(table =\u003e 'default.T', tag =\u003e 'tag1', target_tag =\u003e 'tag2')\rreplace_tag\rReplace an existing tag with new tag info. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the existed tag. Cannot be empty.\rsnapshot(Long): id of the snapshot which the tag is based on, it is optional.\rtime_retained: The maximum time retained for the existing tag, it is optional.\rCALL sys.replace_tag(table =\u003e 'default.T', tag_name =\u003e 'tag1', snapshot =\u003e 10, time_retained =\u003e '1 d')\rdelete_tag\rTo delete a tag. Arguments:\rtable: the target table identifier. Cannot be empty.\rtag: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.\rCALL sys.delete_tag(table =\u003e 'default.T', tag =\u003e 'my_tag')\rexpire_tags\rTo expire tags by time. Arguments:\rtable: the target table identifier. Cannot be empty.\rolder_than: tagCreateTime before which tags will be removed.\rCALL sys.expire_tags(table =\u003e 'default.T', older_than =\u003e '2024-09-06 11:00:00')\rrollback\rTo rollback to a specific version of target table, note version/snapshot/tag must set one of them. Argument:\rtable: the target table identifier. Cannot be empty.\rversion: id of the snapshot or name of tag that will roll back to, version would be Deprecated.\rsnapshot: snapshot that will roll back to.\rtag: tag that will roll back to.\rCALL sys.rollback(table =\u003e 'default.T', version =\u003e 'my_tag')\rCALL sys.rollback(table =\u003e 'default.T', version =\u003e 10)\rCALL sys.rollback(table =\u003e 'default.T', tag =\u003e 'tag1')\rCALL sys.rollback(table =\u003e 'default.T', snapshot =\u003e 2)\rrollback_to_timestamp\rTo rollback to the snapshot which earlier or equal than timestamp. Argument:\rtable: the target table identifier. Cannot be empty.\rtimestamp: roll back to the snapshot which earlier or equal than timestamp.\rCALL sys.rollback_to_timestamp(table =\u003e 'default.T', timestamp =\u003e 1730292023000)\rrollback_to_watermark\rTo rollback to the snapshot which earlier or equal than watermark. Argument:\rtable: the target table identifier. Cannot be empty.\rwatermark: roll back to the snapshot which earlier or equal than watermark.\rCALL sys.rollback_to_watermark(table =\u003e 'default.T', watermark =\u003e 1730292023000)\rpurge_files\rTo clear table with purge files. Argument:\rtable: the target table identifier. Cannot be empty.\rCALL sys.purge_files(table =\u003e 'default.T')\rmigrate_database\rMigrate all hive tables in database to paimon tables. Arguments:\rsource_type: the origin database's type to be migrated, such as hive. Cannot be empty.\rdatabase: name of the origin database to be migrated. Cannot be empty.\roptions: the table options of the paimon table to migrate.\roptions_map: Options map for adding key-value options which is a map.\rparallelism: the parallelism for migrate process, default is core numbers of machine.\rCALL sys.migrate_database(source_type =\u003e 'hive', database =\u003e 'db01', options =\u003e 'file.format=parquet', options_map =\u003e map('k1','v1'), parallelism =\u003e 6)\rmigrate_table\rMigrate hive table to a paimon table. Arguments:\rsource_type: the origin table's type to be migrated, such as hive. Cannot be empty.\rtable: name of the origin table to be migrated. Cannot be empty.\roptions: the table options of the paimon table to migrate.\rtarget_table: name of the target paimon table to migrate. If not set would keep the same name with origin table\rdelete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true\roptions_map: Options map for adding key-value options which is a map.\rparallelism: the parallelism for migrate process, default is core numbers of machine.\rCALL sys.migrate_table(source_type =\u003e 'hive', table =\u003e 'default.T', options =\u003e 'file.format=parquet', options_map =\u003e map('k1','v1'), parallelism =\u003e 6)\rremove_orphan_files\rTo remove the orphan data files and metadata files. Arguments:\rtable: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.\rolder_than: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.\rdry_run: when true, view only orphan files, don't actually remove files. Default is false.\rparallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.\rmode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.\rCALL sys.remove_orphan_files(table =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00')\rCALL sys.remove_orphan_files(table =\u003e 'default.*', older_than =\u003e '2023-10-31 12:00:00')\rCALL sys.remove_orphan_files(table =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e true)\rCALL sys.remove_orphan_files(table =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e true, parallelism =\u003e '5')\rCALL sys.remove_orphan_files(table =\u003e 'default.T', older_than =\u003e '2023-10-31 12:00:00', dry_run =\u003e true, parallelism =\u003e '5', mode =\u003e 'local')\rremove_unexisting_files\rProcedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments:\rtable: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database.\rdry_run (optional): only check what files will be removed, but not really remove them. Default is false.\rparallelism (optional): number of parallelisms to check files in the manifests.\rNote that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.\r-- remove unexisting data files in the table `mydb.myt`\rCALL sys.remove_unexisting_files(table =\u003e 'mydb.myt')\r-- only check what files will be removed, but not really remove them (dry run)\rCALL sys.remove_unexisting_files(table =\u003e 'mydb.myt', dry_run = true)\rrepair\rSynchronize information from the file system to Metastore. Argument:\rdatabase_or_table: empty or the target database name or the target table identifier, if you specify multiple tags, delimiter is ','\rCALL sys.repair('test_db.T')\rCALL sys.repair('test_db.T,test_db01,test_db.T2')\rcreate_branch\rTo merge a branch to main branch. Arguments:\rtable: the target table identifier or branch identifier. Cannot be empty.\rbranch: name of the branch to be merged.\rtag: name of the new tag. Cannot be empty.\rCALL sys.create_branch(table =\u003e 'test_db.T', branch =\u003e 'test_branch')\rCALL sys.create_branch(table =\u003e 'test_db.T', branch =\u003e 'test_branch', tag =\u003e 'my_tag')\rCALL sys.create_branch(table =\u003e 'test_db.T$branch_existBranchName', branch =\u003e 'test_branch', tag =\u003e 'my_tag')\rdelete_branch\rTo merge a branch to main branch. Arguments:\rtable: the target table identifier. Cannot be empty.\rbranch: name of the branch to be merged. If you specify multiple branches, delimiter is ','.\rCALL sys.delete_branch(table =\u003e 'test_db.T', branch =\u003e 'test_branch')\rfast_forward\rTo fast_forward a branch to main branch. Arguments:\rtable: the target table identifier. Cannot be empty.\rbranch: name of the branch to be merged.\rCALL sys.fast_forward(table =\u003e 'test_db.T', branch =\u003e 'test_branch')\rreset_consumer\rTo reset or delete consumer. Arguments:\rtable: the target table identifier. Cannot be empty.\rconsumerId: consumer to be reset or deleted.\rnextSnapshotId (Long): the new next snapshot id of the consumer.\r-- reset the new next snapshot id in the consumer\rCALL sys.reset_consumer(table =\u003e 'default.T', consumerId =\u003e 'myid', nextSnapshotId =\u003e 10)\r-- delete consumer\rCALL sys.reset_consumer(table =\u003e 'default.T', consumerId =\u003e 'myid')\rclear_consumers\rTo clear consumers. Arguments:\rtable: the target table identifier. Cannot be empty.\rincludingConsumers: consumers to be cleared.\rexcludingConsumers: consumers which not to be cleared.\r-- clear all consumers in the table\rCALL sys.clear_consumers(table =\u003e 'default.T')\r-- clear some consumers in the table (accept regular expression)\rCALL sys.clear_consumers(table =\u003e 'default.T', includingConsumers =\u003e 'myid.*')\r-- clear all consumers except excludingConsumers in the table (accept regular expression)\rCALL sys.clear_consumers(table =\u003e 'default.T', includingConsumers =\u003e '', excludingConsumers =\u003e 'myid1.*')\r-- clear all consumers with includingConsumers and excludingConsumers (accept regular expression)\rCALL sys.clear_consumers(table =\u003e 'default.T', includingConsumers =\u003e 'myid.*', excludingConsumers =\u003e 'myid1.*')\rmark_partition_done\rTo mark partition to be done. Arguments:\rtable: the target table identifier. Cannot be empty.\rpartitions: partitions need to be mark done, If you specify multiple partitions, delimiter is ';'.\r-- mark single partition done\rCALL sys.mark_partition_done(table =\u003e 'default.T', parititions =\u003e 'day=2024-07-01')\r-- mark multiple partitions done\rCALL sys.mark_partition_done(table =\u003e 'default.T', parititions =\u003e 'day=2024-07-01;day=2024-07-02')\rrefresh_object_table\rTo refresh_object_table a object table. Arguments:\rtable: the target table identifier. Cannot be empty.\rCALL sys.refresh_object_table('default.T')\rcompact_manifest\rTo compact_manifest the manifests. Arguments:\rtable: the target table identifier. Cannot be empty.\rCALL sys.compact_manifest(`table` =\u003e 'default.T')\ralter_view_dialect\rTo alter view dialect. Arguments:\rview: the target view identifier. Cannot be empty.\raction: define change action like: add, update, drop. Cannot be empty.\rengine: when engine which is not spark need define it.\rquery: query for the dialect when action is add and update it couldn't be empty.\r-- add dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'add', 'spark', 'query')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'add', `query` =\u003e 'query')\r-- update dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'update', 'spark', 'query')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'update', `query` =\u003e 'query')\r-- drop dialect in the view\rCALL sys.alter_view_dialect('view_identifier', 'drop', 'spark')\rCALL sys.alter_view_dialect(`view` =\u003e 'view_identifier', `action` =\u003e 'drop')\r"},{"id":102,"href":"/flink/savepoint/","title":"Savepoint","section":"Engine Flink","content":"\rSavepoint\r#\rPaimon has its own snapshot management, this may conflict with Flink\u0026rsquo;s checkpoint management, causing exceptions when restoring from savepoint (don\u0026rsquo;t worry, it will not cause the storage to be damaged).\nIt is recommended that you use the following methods to savepoint:\nUse Flink Stop with savepoint. Use Paimon Tag with Flink Savepoint, and rollback-to-tag before restoring from savepoint. Stop with savepoint\r#\rThis feature of Flink ensures that the last checkpoint is fully processed, which means there will be no more uncommitted metadata left. This is very safe, so we recommend using this feature to stop and start job.\nTag with Savepoint\r#\rIn Flink, we may consume from Kafka and then write to Paimon. Since Flink\u0026rsquo;s checkpoint only retains a limited number, we will trigger a savepoint at certain time (such as code upgrades, data updates, etc.) to ensure that the state can be retained for a longer time, so that the job can be restored incrementally.\nPaimon\u0026rsquo;s snapshot is similar to Flink\u0026rsquo;s checkpoint, and both will automatically expire, but the tag feature of Paimon allows snapshots to be retained for a long time. Therefore, we can combine the two features of Paimon\u0026rsquo;s tag and Flink\u0026rsquo;s savepoint to achieve incremental recovery of job from the specified savepoint.\nStarting from Flink 1.15 intermediate savepoints (savepoints other than created with\r[stop-with-savepoint](https://nightlies.apache.org/flink/flink-docs-stable/docs/ops/state/savepoints/#stopping-a-job-with-savepoint))\rare not used for recovery and do not commit any side effects.\rFor savepoint created with [stop-with-savepoint](https://nightlies.apache.org/flink/flink-docs-stable/docs/ops/state/savepoints/#stopping-a-job-with-savepoint),\rtags will be created automatically. For other savepoints, tags will be created after the next checkpoint succeeds.\rStep 1: Enable automatically create tags for savepoint.\nYou can set sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for savepoint.\nStep 2: Trigger savepoint.\nYou can refer to Flink savepoint to learn how to configure and trigger savepoint.\nStep 3: Choose the tag corresponding to the savepoint.\nThe tag corresponding to the savepoint will be named in the form of savepoint-${savepointID}. You can refer to Tags Table to query.\nStep 4: Rollback the paimon table.\nRollback the Paimon table to the specified tag.\nStep 5: Restart from the savepoint.\nYou can refer to here to learn how to restart from a specified savepoint.\n"},{"id":103,"href":"/maintenance/configurations/","title":"Configurations","section":"Maintenance","content":"\rConfiguration\r#\rCoreOptions\r#\rCore options for paimon.\nKey\rDefault\rType\rDescription\raggregation.remove-record-on-delete\rfalse\rBoolean\rWhether to remove the whole row in aggregation engine when -D records are received.\rasync-file-write\rtrue\rBoolean\rWhether to enable asynchronous IO writing when writing files.\rauto-create\rfalse\rBoolean\rWhether to create underlying storage when reading and writing the table.\rbucket\r-1\rInteger\rBucket number for file store.\nIt should either be equal to -1 (dynamic bucket mode), -2 (postpone bucket mode), or it must be greater than 0 (fixed bucket mode).\rbucket-key\r(none)\rString\rSpecify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.\rcache-page-size\r64 kb\rMemorySize\rMemory page size for caching.\rchangelog-file.compression\r(none)\rString\rChangelog file compression.\rchangelog-file.format\r(none)\rString\rSpecify the message format of changelog files, currently parquet, avro and orc are supported.\rchangelog-file.prefix\r\"changelog-\"\rString\rSpecify the file name prefix of changelog files.\rchangelog-file.stats-mode\r(none)\rString\rChangelog file metadata stats collection. none, counts, truncate(16), full is available.\rchangelog-producer\rnone\rEnum\nWhether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys. Possible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.\rchangelog-producer.row-deduplicate\rfalse\rBoolean\rWhether to generate -U, +U changelog for the same record. This configuration is only valid for the changelog-producer is lookup or full-compaction.\rchangelog-producer.row-deduplicate-ignore-fields\r(none)\rString\rFields that are ignored for comparison while generating -U, +U changelog for the same record. This configuration is only valid for the changelog-producer.row-deduplicate is true.\rchangelog.num-retained.max\r(none)\rInteger\rThe maximum number of completed changelog to retain. Should be greater than or equal to the minimum number.\rchangelog.num-retained.min\r(none)\rInteger\rThe minimum number of completed changelog to retain. Should be greater than or equal to 1.\rchangelog.time-retained\r(none)\rDuration\rThe maximum time of completed changelog to retain.\rcommit.callback.#.param\r(none)\rString\rParameter string for the constructor of class #. Callback class should parse the parameter by itself.\rcommit.callbacks\r(none)\rString\rA list of commit callback classes to be called after a successful commit. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).\rcommit.force-compact\rfalse\rBoolean\rWhether to force a compaction before commit.\rcommit.force-create-snapshot\rfalse\rBoolean\rWhether to force create snapshot on commit.\rcommit.max-retries\r10\rInteger\rMaximum number of retries when commit failed.\rcommit.timeout\r(none)\rDuration\rTimeout duration of retry when commit failed.\rcommit.user-prefix\r(none)\rString\rSpecifies the commit user prefix.\rcompaction.force-up-level-0\rfalse\rBoolean\rIf set to true, compaction strategy will always include all level 0 files in candidates.\rcompaction.max-size-amplification-percent\r200\rInteger\rThe size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.\rcompaction.min.file-num\r5\rInteger\rFor file set [f_0,...,f_N], the minimum file number to trigger a compaction for append-only table.\rcompaction.optimization-interval\r(none)\rDuration\rImplying how often to perform an optimization compaction, this configuration is used to ensure the query timeliness of the read-optimized system table.\rcompaction.size-ratio\r1\rInteger\rPercentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.\rconsumer-id\r(none)\rString\rConsumer id for recording the offset of consumption in the storage.\rconsumer.expiration-time\r(none)\rDuration\rThe expiration interval of consumer files. A consumer file will be expired if it's lifetime after last modification is over this value.\rconsumer.ignore-progress\rfalse\rBoolean\rWhether to ignore consumer progress for the newly started job.\rconsumer.mode\rexactly-once\rEnum\nSpecify the consumer consistency mode for table.\nPossible values:\"exactly-once\": Readers consume data at snapshot granularity, and strictly ensure that the snapshot-id recorded in the consumer is the snapshot-id + 1 that all readers have exactly consumed.\"at-least-once\": Each reader consumes snapshots at a different rate, and the snapshot with the slowest consumption progress among all readers will be recorded in the consumer.\rcontinuous.discovery-interval\r10 s\rDuration\rThe discovery interval of continuous reading.\rcross-partition-upsert.bootstrap-parallelism\r10\rInteger\rThe parallelism for bootstrap in a single task for cross partition upsert.\rcross-partition-upsert.index-ttl\r(none)\rDuration\rThe TTL in rocksdb index for cross partition upsert (primary keys not contain all partition fields), this can avoid maintaining too many indexes and lead to worse and worse performance, but please note that this may also cause data duplication.\rdata-file.external-paths\r(none)\rString\rThe external paths where the data of this table will be written, multiple elements separated by commas.\rdata-file.external-paths.specific-fs\r(none)\rString\rThe specific file system of the external path when data-file.external-paths.strategy is set to specific-fs, should be the prefix scheme of the external path, now supported are s3 and oss.\rdata-file.external-paths.strategy\rnone\rEnum\nThe strategy of selecting an external path when writing data.\nPossible values:\"none\": Do not choose any external storage, data will still be written to the default warehouse path.\"specific-fs\": Select a specific file system as the external path. Currently supported are S3 and OSS.\"round-robin\": When writing a new file, a path is chosen from data-file.external-paths in turn.\rdata-file.path-directory\r(none)\rString\rSpecify the path directory of data files.\rdata-file.prefix\r\"data-\"\rString\rSpecify the file name prefix of data files.\rdata-file.thin-mode\rfalse\rBoolean\rEnable data file thin mode to avoid duplicate columns storage.\rdelete-file.thread-num\r(none)\rInteger\rThe maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.\rdelete.force-produce-changelog\rfalse\rBoolean\rForce produce changelog in delete sql, or you can use 'streaming-read-overwrite' to read changelog from overwrite commit.\rdeletion-vector.index-file.target-size\r2 mb\rMemorySize\rThe target size of deletion vector index file.\rdeletion-vectors.enabled\rfalse\rBoolean\rWhether to enable deletion vectors mode. In this mode, index files containing deletion vectors are generated when data is written, which marks the data for deletion. During read operations, by applying these index files, merging can be avoided.\rdynamic-bucket.assigner-parallelism\r(none)\rInteger\rParallelism of assigner operator for dynamic bucket mode, it is related to the number of initialized bucket, too small will lead to insufficient processing speed of assigner.\rdynamic-bucket.initial-buckets\r(none)\rInteger\rInitial buckets for a partition in assigner operator for dynamic bucket mode.\rdynamic-bucket.max-buckets\r-1\rInteger\rMax buckets for a partition in dynamic bucket mode, It should either be equal to -1 (unlimited), or it must be greater than 0 (fixed upper bound).\rdynamic-bucket.target-row-num\r2000000\rLong\rIf the bucket is -1, for primary key table, is dynamic bucket mode, this option controls the target row number for one bucket.\rdynamic-partition-overwrite\rtrue\rBoolean\rWhether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.\rend-input.check-partition-expire\rfalse\rBoolean\rOptional endInput check partition expire used in case of batch mode or bounded stream.\rfields.default-aggregate-function\r(none)\rString\rDefault aggregate function of all fields for partial-update and aggregate merge function.\rfile-index.in-manifest-threshold\r500 bytes\rMemorySize\rThe threshold to store file index bytes in manifest.\rfile-index.read.enabled\rtrue\rBoolean\rWhether enabled read file index.\rfile-reader-async-threshold\r10 mb\rMemorySize\rThe threshold for read file async.\rfile.block-size\r(none)\rMemorySize\rFile block size of format, default value of orc stripe is 64 MB, and parquet row group is 128 MB.\rfile.compression\r\"zstd\"\rString\rDefault file compression. For faster read and write, it is recommended to use zstd.\rfile.compression.per.level\rMap\rDefine different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zstd'.\rfile.compression.zstd-level\r1\rInteger\rDefault file compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.\rfile.format\r\"parquet\"\rString\rSpecify the message format of data files, currently orc, parquet and avro are supported.\rfile.format.per.level\rMap\rDefine different file format for different level, you can add the conf like this: 'file.format.per.level' = '0:avro,3:parquet', if the file format for level is not provided, the default format which set by `file.format` will be used.\rfile.suffix.include.compression\rfalse\rBoolean\rWhether to add file compression type in the file name of data file and changelog file.\rforce-lookup\rfalse\rBoolean\rWhether to force the use of lookup for compaction.\rfull-compaction.delta-commits\r(none)\rInteger\rFull compaction will be constantly triggered after delta commits.\rignore-delete\rfalse\rBoolean\rWhether to ignore delete records.\rincremental-between\r(none)\rString\rRead incremental changes between start snapshot (exclusive) and end snapshot (inclusive), for example, '5,10' means changes between snapshot 5 and snapshot 10.\rincremental-between-scan-mode\rauto\rEnum\nScan kind when Read incremental changes between start snapshot (exclusive) and end snapshot (inclusive). Possible values:\"auto\": Scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files.\"delta\": Scan newly changed files between snapshots.\"changelog\": Scan changelog files between snapshots.\"diff\": Get diff by comparing data of end snapshot with data of start snapshot.\rincremental-between-timestamp\r(none)\rString\rRead incremental changes between start timestamp (exclusive) and end timestamp (inclusive), for example, 't1,t2' means changes between timestamp t1 and timestamp t2.\rincremental-to-auto-tag\r(none)\rString\rUsed to specify the end tag (inclusive), and Paimon will find an earlier tag and return changes between them. If the tag doesn't exist or the earlier tag doesn't exist, return empty. local-merge-buffer-size\r(none)\rMemorySize\rLocal merge will buffer and merge input records before they're shuffled by bucket and written into sink. The buffer will be flushed when it is full.\rMainly to resolve data skew on primary keys. We recommend starting with 64 mb when trying out this feature.\rlocal-sort.max-num-file-handles\r128\rInteger\rThe maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.\rlookup-compact\rRADICAL\rEnum\nLookup compact mode used for lookup compaction.\nPossible values:\"RADICAL\"\"GENTLE\"\rlookup-compact.max-interval\r(none)\rInteger\rThe max interval for a gentle mode lookup compaction to be triggered. For every interval, a forced lookup compaction will be performed to flush L0 files to higher level. This option is only valid when lookup-compact mode is gentle.\rlookup-wait\rtrue\rBoolean\rWhen need to lookup, commit will wait for compaction by lookup.\rlookup.cache-file-retention\r1 h\rDuration\rThe cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.\rlookup.cache-max-disk-size\rinfinite\rMemorySize\rMax disk size for lookup cache, you can use this option to limit the use of local disks.\rlookup.cache-max-memory-size\r256 mb\rMemorySize\rMax memory size for lookup cache.\rlookup.cache-spill-compression\r\"zstd\"\rString\rSpill compression for lookup cache, currently zstd, none, lz4 and lzo are supported.\rlookup.cache.bloom.filter.enabled\rtrue\rBoolean\rWhether to enable the bloom filter for lookup cache.\rlookup.cache.bloom.filter.fpp\r0.05\rDouble\rDefine the default false positive probability for lookup cache bloom filters.\rlookup.cache.high-priority-pool-ratio\r0.25\rDouble\rThe fraction of cache memory that is reserved for high-priority data like index, filter.\rlookup.hash-load-factor\r0.75\rFloat\rThe index load factor for lookup.\rlookup.local-file-type\rsort\rEnum\nThe local file type for lookup.\nPossible values:\"sort\": Construct a sorted file for lookup.\"hash\": Construct a hash file for lookup.\rmanifest.compression\r\"zstd\"\rString\rDefault file compression for manifest.\rmanifest.delete-file-drop-stats\rfalse\rBoolean\rFor DELETE manifest entry in manifest file, drop stats to reduce memory and storage. Default value is false only for compatibility of old reader.\rmanifest.format\r\"avro\"\rString\rSpecify the message format of manifest files.\rmanifest.full-compaction-threshold-size\r16 mb\rMemorySize\rThe size threshold for triggering full compaction of manifest.\rmanifest.merge-min-count\r30\rInteger\rTo avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.\rmanifest.target-file-size\r8 mb\rMemorySize\rSuggested file size of a manifest file.\rmerge-engine\rdeduplicate\rEnum\nSpecify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.\"first-row\": De-duplicate and keep the first row.\rmetadata.stats-dense-store\rtrue\rBoolean\rWhether to store statistic densely in metadata (manifest files), which will significantly reduce the storage size of metadata when the none statistic mode is set.\nNote, when this mode is enabled with 'metadata.stats-mode:none', the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.\rmetadata.stats-mode\r\"truncate(16)\"\rString\rThe mode of metadata stats collection. none, counts, truncate(16), full is available.\n\"none\": means disable the metadata stats collection.\"counts\" means only collect the null count.\"full\": means collect the null count, min/max value.\"truncate(16)\": means collect the null count, min/max value with truncated length of 16.Field level stats mode can be specified by fields.{field_name}.stats-mode\rmetadata.stats-mode.per.level\rMap\rDefine different 'metadata.stats-mode' for different level, you can add the conf like this: 'metadata.stats-mode.per.level' = '0:none', if the metadata.stats-mode for level is not provided, the default mode which set by `metadata.stats-mode` will be used.\rmetastore.partitioned-table\rfalse\rBoolean\rWhether to create this table as a partitioned table in metastore.\rFor example, if you want to list all partitions of a Paimon table in Hive, you need to create this table as a partitioned table in Hive metastore.\rThis config option does not affect the default filesystem metastore.\rmetastore.tag-to-partition\r(none)\rString\rWhether to create this table as a partitioned table for mapping non-partitioned table tags in metastore. This allows the Hive engine to view this table in a partitioned table view and use partitioning field to read specific partitions (specific tags).\rmetastore.tag-to-partition.preview\rnone\rEnum\nWhether to preview tag of generated snapshots in metastore. This allows the Hive engine to query specific tag before creation.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.\rnum-levels\r(none)\rInteger\rTotal level number, for example, there are 3 levels, including 0,1,2 levels.\rnum-sorted-run.compaction-trigger\r5\rInteger\rThe sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).\rnum-sorted-run.stop-trigger\r(none)\rInteger\rThe number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.\robject-location\r(none)\rString\rThe object location for object table.\rpage-size\r64 kb\rMemorySize\rMemory page size.\rparquet.enable.dictionary\r(none)\rInteger\rTurn off the dictionary encoding for all fields in parquet.\rpartial-update.remove-record-on-delete\rfalse\rBoolean\rWhether to remove the whole row in partial-update engine when -D records are received.\rpartial-update.remove-record-on-sequence-group\r(none)\rString\rWhen -D records of the given sequence groups are received, remove the whole row.\rpartition\r(none)\rString\rDefine partition by table options, cannot define partition on DDL and table options at the same time.\rpartition.default-name\r\"__DEFAULT_PARTITION__\"\rString\rThe default partition name in case the dynamic partition column value is null/empty string.\rpartition.end-input-to-done\rfalse\rBoolean\rWhether mark the done status to indicate that the data is ready when end input.\rpartition.expiration-check-interval\r1 h\rDuration\rThe check interval of partition expiration.\rpartition.expiration-max-num\r100\rInteger\rThe default deleted num of partition expiration.\rpartition.expiration-strategy\rvalues-time\rEnum\nThe strategy determines how to extract the partition time and compare it with the current time.\nPossible values:\"values-time\": This strategy compares the time extracted from the partition value with the current time.\"update-time\": This strategy compares the last update time of the partition with the current time.\rpartition.expiration-time\r(none)\rDuration\rThe expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.\rpartition.idle-time-to-report-statistic\r0 ms\rDuration\rSet a time duration when a partition has no new data after this time duration, start to report the partition statistics to hms.\rpartition.legacy-name\rtrue\rBoolean\rThe legacy partition name is using `toString` fpr all types. If false, using cast to string for all types.\rpartition.mark-done-action\r\"success-file\"\rString\rAction to mark a partition done is to notify the downstream application that the partition has finished writing, the partition is ready to be read.\n1. 'success-file': add '_success' file to directory.\n2. 'done-partition': add 'xxx.done' partition to metastore.\n3. 'mark-event': mark partition event to metastore.\n4. 'http-report': report partition mark done to remote http server.\n5. 'custom': use policy class to create a mark-partition policy.\nBoth can be configured at the same time: 'done-partition,success-file,mark-event,custom'.\rpartition.mark-done-action.custom.class\r(none)\rString\rThe partition mark done class for implement PartitionMarkDoneAction interface. Only work in custom mark-done-action.\rpartition.mark-done-action.http.params\r(none)\rString\rHttp client request parameters will be written to the request body, this can only be used by http-report partition mark done action.\rpartition.mark-done-action.http.timeout\r5 s\rDuration\rHttp client connection timeout, this can only be used by http-report partition mark done action.\rpartition.mark-done-action.http.url\r(none)\rString\rMark done action will reports the partition to the remote http server, this can only be used by http-report partition mark done action.\rpartition.sink-strategy\rNONE\rEnum\nThis is only for partitioned unaware-buckets append table, and the purpose is to reduce small files and improve write performance. Through this repartitioning strategy to reduce the number of partitions written by each task to as few as possible.none: Rebalanced or Forward partitioning, this is the default behavior, this strategy is suitable for the number of partitions you write in a batch is much smaller than write parallelism.hash: Hash the partitions value, this strategy is suitable for the number of partitions you write in a batch is greater equals than write parallelism.\nPossible values:\"NONE\"\"HASH\"\rpartition.timestamp-formatter\r(none)\rString\rThe formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.\rpartition.timestamp-pattern\r(none)\rString\rYou can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.\rprimary-key\r(none)\rString\rDefine primary key by table options, cannot define primary key on DDL and table options at the same time.\rread.batch-size\r1024\rInteger\rRead batch size for any file format if it supports.\rrecord-level.expire-time\r(none)\rDuration\rRecord level expire time for primary key table, expiration happens in compaction, there is no strong guarantee to expire records in time. You must specific 'record-level.time-field' too.\rrecord-level.time-field\r(none)\rString\rTime field for record level expire. It supports the following types: `timestamps in seconds with INT`,`timestamps in seconds with BIGINT`, `timestamps in milliseconds with BIGINT` or `timestamp`.\rrowkind.field\r(none)\rString\rThe field that generates the row kind for primary key table, the row kind determines which data is '+I', '-U', '+U' or '-D'.\rscan.bounded.watermark\r(none)\rLong\rEnd condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.\rscan.fallback-branch\r(none)\rString\rWhen a batch job queries from a table, if a partition does not exist in the current branch, the reader will try to get this partition from this fallback branch.\rscan.file-creation-time-millis\r(none)\rLong\rAfter configuring this time, only the data files created after this time will be read. It is independent of snapshots, but it is imprecise filtering (depending on whether or not compaction occurs).\rscan.manifest.parallelism\r(none)\rInteger\rThe parallelism of scanning manifest files, default value is the size of cpu processor. Note: Scale-up this parameter will increase memory usage while scanning manifest files. We can consider downsize it when we encounter an out of memory exception while scanning\rscan.max-splits-per-task\r10\rInteger\rMax split size should be cached for one task while scanning. If splits size cached in enumerator are greater than tasks size multiply by this value, scanner will pause scanning.\rscan.mode\rdefault\rEnum\nSpecify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" or \"scan.tag-name\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-file-creation-time\": For streaming and batch sources, produces a snapshot and filters the data files by creation time. For streaming sources, upon first startup, and continue to read the latest changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" or \"scan.tag-name\" but does not read new changes.\"from-snapshot-full\": For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.\"incremental\": Read incremental changes between start and end snapshot or timestamp.\rscan.plan-sort-partition\rfalse\rBoolean\rWhether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.\rscan.snapshot-id\r(none)\rLong\rOptional snapshot id used in case of \"from-snapshot\" or \"from-snapshot-full\" scan mode\rscan.tag-name\r(none)\rString\rOptional tag name used in case of \"from-snapshot\" scan mode.\rscan.timestamp\r(none)\rString\rOptional timestamp used in case of \"from-timestamp\" scan mode, it will be automatically converted to timestamp in unix milliseconds, use local time zone\rscan.timestamp-millis\r(none)\rLong\rOptional timestamp used in case of \"from-timestamp\" scan mode. If there is no snapshot earlier than this time, the earliest snapshot will be chosen.\rscan.watermark\r(none)\rLong\rOptional watermark used in case of \"from-snapshot\" scan mode. If there is no snapshot later than this watermark, will throw an exceptions.\rsequence.field\r(none)\rString\rThe field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.\rsequence.field.sort-order\rascending\rEnum\nSpecify the order of sequence.field.\nPossible values:\"ascending\": specifies sequence.field sort order is ascending.\"descending\": specifies sequence.field sort order is descending.\rsink.watermark-time-zone\r\"UTC\"\rString\rThe time zone to parse the long watermark value to TIMESTAMP value. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the value should be the user configured local time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.\rsnapshot.clean-empty-directories\rfalse\rBoolean\rWhether to try to clean empty directories when expiring snapshots, if enabled, please note:hdfs: may print exceptions in NameNode.oss/s3: may cause performance issue.\rsnapshot.expire.execution-mode\rsync\rEnum\nSpecifies the execution mode of expire.\nPossible values:\"sync\": Execute expire synchronously. If there are too many files, it may take a long time and block stream processing.\"async\": Execute expire asynchronously. If the generation of snapshots is greater than the deletion, there will be a backlog of files.\rsnapshot.expire.limit\r50\rInteger\rThe maximum number of snapshots allowed to expire at a time.\rsnapshot.num-retained.max\rinfinite\rInteger\rThe maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.\rsnapshot.num-retained.min\r10\rInteger\rThe minimum number of completed snapshots to retain. Should be greater than or equal to 1.\rsnapshot.time-retained\r1 h\rDuration\rThe maximum time of completed snapshots to retain.\rsnapshot.watermark-idle-timeout\r(none)\rDuration\rIn watermarking, if a source remains idle beyond the specified timeout duration, it triggers snapshot advancement and facilitates tag creation.\rsort-compaction.local-sample.magnification\r1000\rInteger\rThe magnification of local sample for sort-compaction.The size of local sample is sink parallelism * magnification.\rsort-compaction.range-strategy\rQUANTITY\rEnum\nThe range strategy of sort compaction, the default value is quantity.\rIf the data size allocated for the sorting task is uneven,which may lead to performance bottlenecks, the config can be set to size.\nPossible values:\"SIZE\"\"QUANTITY\"\rsort-engine\rloser-tree\rEnum\nSpecify the sort engine for table with primary key.\nPossible values:\"min-heap\": Use min-heap for multiway sorting.\"loser-tree\": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.\rsort-spill-buffer-size\r64 mb\rMemorySize\rAmount of data to spill records to disk in spilled sort.\rsort-spill-threshold\r(none)\rInteger\rIf the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.\rsource.split.open-file-cost\r4 mb\rMemorySize\rOpen file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.\rsource.split.target-size\r128 mb\rMemorySize\rTarget size of a source split when scanning a bucket.\rspill-compression\r\"zstd\"\rString\rCompression for spill, currently zstd, lzo and zstd are supported.\rspill-compression.zstd-level\r1\rInteger\rDefault spill compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.\rstreaming-read-mode\r(none)\rEnum\nThe mode of streaming read that specifies to read the data of table file or log.\nPossible values:\"log\": Read from the data of table log store.\"file\": Read from the data of table file store.\rstreaming-read-overwrite\rfalse\rBoolean\rWhether to read the changes from overwrite in streaming mode. Cannot be set to true when changelog producer is full-compaction or lookup because it will read duplicated changes.\rstreaming.read.snapshot.delay\r(none)\rDuration\rThe delay duration of stream read when scan incremental snapshots.\rtag.automatic-completion\rfalse\rBoolean\rWhether to automatically complete missing tags.\rtag.automatic-creation\rnone\rEnum\nWhether to create tag automatically. And how to generate tags.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.\rtag.batch.customized-name\r(none)\rString\rUse customized name when creating tags in Batch mode.\rtag.callback.#.param\r(none)\rString\rParameter string for the constructor of class #. Callback class should parse the parameter by itself.\rtag.callbacks\r(none)\rString\rA list of commit callback classes to be called after a successful tag. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).\rtag.create-success-file\rfalse\rBoolean\rWhether to create tag success file for new created tags.\rtag.creation-delay\r0 ms\rDuration\rHow long is the delay after the period ends before creating a tag. This can allow some late data to enter the Tag.\rtag.creation-period\rdaily\rEnum\nWhat frequency is used to generate tags.\nPossible values:\"daily\": Generate a tag every day.\"hourly\": Generate a tag every hour.\"two-hours\": Generate a tag every two hours.\rtag.creation-period-duration\r(none)\rDuration\rThe period duration for tag auto create periods.If user set it, tag.creation-period would be invalid.\rtag.default-time-retained\r(none)\rDuration\rThe default maximum time retained for newly created tags. It affects both auto-created tags and manually created (by procedure) tags.\rtag.num-retained-max\r(none)\rInteger\rThe maximum number of tags to retain. It only affects auto-created tags.\rtag.period-formatter\rwith_dashes\rEnum\nThe date format for tag periods.\nPossible values:\"with_dashes\": Dates and hours with dashes, e.g., 'yyyy-MM-dd HH'\"without_dashes\": Dates and hours without dashes, e.g., 'yyyyMMdd HH'\"without_dashes_and_spaces\": Dates and hours without dashes and spaces, e.g., 'yyyyMMddHH'\rtarget-file-size\r(none)\rMemorySize\rTarget size of a file.primary key table: the default value is 128 MB.append table: the default value is 256 MB.\rtype\rtable\rEnum\nType of the table.\nPossible values:\"table\": Normal Paimon table.\"format-table\": A file format table refers to a directory that contains multiple files of the same format.\"materialized-table\": A materialized table combines normal Paimon table and materialized SQL.\"object-table\": An object table combines normal Paimon table and object location.\rwrite-buffer-for-append\rfalse\rBoolean\rThis option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.\rwrite-buffer-size\r256 mb\rMemorySize\rAmount of data to build up in memory before converting to a sorted on-disk file.\rwrite-buffer-spill.max-disk-size\rinfinite\rMemorySize\rThe max disk to use for write buffer spill. This only work when the write buffer spill is enabled\rwrite-buffer-spillable\r(none)\rBoolean\rWhether the write buffer can be spillable. Enabled by default when using object storage or when 'target-file-size' is greater than 'write-buffer-size'.\rwrite-manifest-cache\r0 bytes\rMemorySize\rCache size for reading manifest files for write initialization.\rwrite-max-writers-to-spill\r10\rInteger\rWhen in batch append inserting, if the writer number is greater than this option, we open the buffer cache and spill function to avoid out-of-memory. write-only\rfalse\rBoolean\rIf set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.\rwrite.batch-size\r1024\rInteger\rWrite batch size for any file format if it supports.\rzorder.var-length-contribution\r8\rInteger\rThe bytes of types (CHAR, VARCHAR, BINARY, VARBINARY) devote to the zorder sort.\rCatalogOptions\r#\rOptions for paimon catalog.\nKey\rDefault\rType\rDescription\rcache-enabled\rtrue\rBoolean\rControls whether the catalog will cache databases, tables, manifests and partitions.\rcache.expiration-interval\r10 min\rDuration\rControls the duration for which databases and tables in the catalog are cached.\rcache.manifest.max-memory\r(none)\rMemorySize\rControls the maximum memory to cache manifest content.\rcache.manifest.small-file-memory\r128 mb\rMemorySize\rControls the cache memory to cache small manifest files.\rcache.manifest.small-file-threshold\r1 mb\rMemorySize\rControls the threshold of small manifest file.\rcache.partition.max-num\r0\rLong\rControls the max number for which partitions in the catalog are cached.\rcache.snapshot.max-num-per-table\r20\rInteger\rControls the max number for snapshots per table in the catalog are cached.\rcase-sensitive\r(none)\rBoolean\rIndicates whether this catalog is case-sensitive.\rclient-pool-size\r2\rInteger\rConfigure the size of the connection pool.\rfile-io.allow-cache\rtrue\rBoolean\rWhether to allow static cache in file io implementation. If not allowed, this means that there may be a large number of FileIO instances generated, enabling caching can lead to resource leakage.\rfile-io.populate-meta\rfalse\rBoolean\rWhether to populate file metadata while listing or getting file status.\rformat-table.enabled\rtrue\rBoolean\rWhether to support format tables, format table corresponds to a regular csv, parquet or orc table, allowing read and write operations. However, during these processes, it does not connect to the metastore; hence, newly added partitions will not be reflected in the metastore and need to be manually added as separate partition operations.\rlock-acquire-timeout\r8 min\rDuration\rThe maximum time to wait for acquiring the lock.\rlock-check-max-sleep\r8 s\rDuration\rThe maximum sleep time when retrying to check the lock.\rlock.enabled\r(none)\rBoolean\rEnable Catalog Lock.\rlock.type\r(none)\rString\rThe Lock Type for Catalog, such as 'hive', 'zookeeper'.\rmetastore\r\"filesystem\"\rString\rMetastore of paimon catalog, supports filesystem, hive and jdbc.\rresolving-file-io.enabled\rfalse\rBoolean\rWhether to enable resolving fileio, when this option is enabled, in conjunction with the table's property data-file.external-paths, Paimon can read and write to external storage paths, such as OSS or S3. In order to access these external paths correctly, you also need to configure the corresponding access key and secret key.\rsync-all-properties\rtrue\rBoolean\rSync all table properties to hive metastore\rtable.type\rmanaged\rEnum\nType of table.\nPossible values:\"managed\": Paimon owned table where the entire lifecycle of the table data is managed.\"external\": The table where Paimon has loose coupling with the data stored in external locations.\ruri\r(none)\rString\rUri of metastore server.\rwarehouse\r(none)\rString\rThe warehouse root path of catalog.\rHiveCatalogOptions\r#\rOptions for Hive catalog.\nKey\rDefault\rType\rDescription\rclient-pool-cache.eviction-interval-ms\r300000\rLong\rSetting the client's pool cache eviction interval(ms).\rclient-pool-cache.keys\r(none)\rString\rSpecify client cache key, multiple elements separated by commas.\n\"ugi\": the Hadoop UserGroupInformation instance that represents the current user using the cache.\"user_name\" similar to UGI but only includes the user's name determined by UserGroupInformation#getUserName.\"conf\": name of an arbitrary configuration. The value of the configuration will be extracted from catalog properties and added to the cache key. A conf element should start with a \"conf:\" prefix which is followed by the configuration name. E.g. specifying \"conf:a.b.c\" will add \"a.b.c\" to the key, and so that configurations with different default catalog wouldn't share the same client pool. Multiple conf elements can be specified.\rhadoop-conf-dir\r(none)\rString\rFile directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported.\rIf not configured, try to load from 'HADOOP_CONF_DIR' or 'HADOOP_HOME' system environment.\rConfigure Priority: 1.from 'hadoop-conf-dir' 2.from HADOOP_CONF_DIR 3.from HADOOP_HOME/conf 4.HADOOP_HOME/etc/hadoop.\rhive-conf-dir\r(none)\rString\rFile directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on.\rIf not configured, try to load from 'HIVE_CONF_DIR' env.\rlocation-in-properties\rfalse\rBoolean\rSetting the location in properties of hive table/database.\rIf you don't want to access the location by the filesystem of hive when using a object storage such as s3,oss\ryou can set this option to true.\rmetastore.client.class\r\"org.apache.hadoop.hive.metastore.HiveMetaStoreClient\"\rString\rClass name of Hive metastore client.\rNOTE: This class must directly implements org.apache.hadoop.hive.metastore.IMetaStoreClient.\rJdbcCatalogOptions\r#\rOptions for Jdbc catalog.\nKey\rDefault\rType\rDescription\rcatalog-key\r\"jdbc\"\rString\rCustom jdbc catalog store key.\rlock-key-max-length\r255\rInteger\rSet the maximum length of the lock key. The 'lock-key' is composed of concatenating three fields : 'catalog-key', 'database', and 'table'.\rFlinkCatalogOptions\r#\rFlink catalog options for paimon.\nKey\rDefault\rType\rDescription\rdefault-database\r\"default\"\rString\rdisable-create-table-in-default-db\rfalse\rBoolean\rIf true, creating table in default database is not allowed. Default is false.\rFlinkConnectorOptions\r#\rFlink connector options for paimon.\nKey\rDefault\rType\rDescription\rchangelog.precommit-compact.thread-num\r(none)\rInteger\rMaximum number of threads to copy bytes from small changelog files. By default is the number of processors available to the Java virtual machine.\rend-input.watermark\r(none)\rLong\rOptional endInput watermark used in case of batch mode or bounded stream.\rlookup.async\rfalse\rBoolean\rWhether to enable async lookup join.\rlookup.async-thread-number\r16\rInteger\rThe thread number for lookup async.\rlookup.bootstrap-parallelism\r4\rInteger\rThe parallelism for bootstrap in a single task for lookup join.\rlookup.cache\rAUTO\rEnum\nThe cache mode of lookup join.\nPossible values:\"AUTO\"\"FULL\"\rlookup.dynamic-partition.refresh-interval\r1 h\rDuration\rSpecific dynamic partition refresh interval for lookup, scan all partitions and obtain corresponding partition.\rlookup.refresh.async\rfalse\rBoolean\rWhether to refresh lookup table in an async thread.\rlookup.refresh.async.pending-snapshot-count\r5\rInteger\rIf the pending snapshot count exceeds the threshold, lookup operator will refresh the table in sync.\rlookup.refresh.time-periods-blacklist\r(none)\rString\rThe blacklist contains several time periods. During these time periods, the lookup table's cache refreshing is forbidden. Blacklist format is start1-\u0026gt;end1,start2-\u0026gt;end2,... , and the time format is yyyy-MM-dd HH:mm. Only used when lookup table is FULL cache mode.\rpartition.idle-time-to-done\r(none)\rDuration\rSet a time duration when a partition has no new data after this time duration, mark the done status to indicate that the data is ready.\rpartition.mark-done-action.mode\rprocess-time\rEnum\nHow to trigger partition mark done action.\nPossible values:\"process-time\": Based on the time of the machine, mark the partition done once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, mark the partition done once the watermark passes period time plus delay.\rpartition.time-interval\r(none)\rDuration\rYou can specify time interval for partition, for example, daily partition is '1 d', hourly partition is '1 h'.\rpostpone.default-bucket-num\r1\rInteger\rBucket number for the partitions compacted for the first time in postpone bucket tables.\rprecommit-compact\rfalse\rBoolean\rIf true, it will add a compact coordinator and worker operator after the writer operator,in order to compact several changelog files (for primary key tables) or newly created data files (for unaware bucket tables) from the same partition into large ones, which can decrease the number of small files.\rscan.bounded\r(none)\rBoolean\rBounded mode for Paimon consumer. By default, Paimon automatically selects bounded mode based on the mode of the Flink job.\rscan.infer-parallelism\rtrue\rBoolean\rIf it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).\rscan.infer-parallelism.max\r1024\rInteger\rIf scan.infer-parallelism is true, limit the parallelism of source through this option.\rscan.parallelism\r(none)\rInteger\rDefine a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.\rscan.partitions\r(none)\rString\rSpecify the partitions to scan. Partitions should be given in the form of key1=value1,key2=value2. Partition keys not specified will be filled with the value of partition.default-name. Multiple partitions should be separated by semicolon (;). This option can support normal source tables and lookup join tables. For lookup joins, two special values max_pt() and max_two_pt() are also supported, specifying the (two) partition(s) with the largest partition value.\rscan.remove-normalize\rfalse\rBoolean\rWhether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.\rscan.split-enumerator.batch-size\r10\rInteger\rHow many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.\rscan.split-enumerator.mode\rfair\rEnum\nThe mode used by StaticFileStoreSplitEnumerator to assign splits.\nPossible values:\"fair\": Distribute splits evenly when batch reading to prevent a few tasks from reading all.\"preemptive\": Distribute splits preemptively according to the consumption speed of the task.\rscan.watermark.alignment.group\r(none)\rString\rA group of sources to align watermarks.\rscan.watermark.alignment.max-drift\r(none)\rDuration\rMaximal drift to align watermarks, before we pause consuming from the source/task/partition.\rscan.watermark.alignment.update-interval\r1 s\rDuration\rHow often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.\rscan.watermark.emit.strategy\ron-event\rEnum\nEmit strategy for watermark generation.\nPossible values:\"on-periodic\": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.\"on-event\": Emit watermark per record.\rscan.watermark.idle-timeout\r(none)\rDuration\rIf no records flow in a partition of a stream for that amount of time, then that partition is considered \"idle\" and will not hold back the progress of watermarks in downstream operators.\rsink.clustering.by-columns\r(none)\rString\rSpecifies the column name(s) used for comparison during range partitioning, in the format 'columnName1,columnName2'. If not set or set to an empty string, it indicates that the range partitioning feature is not enabled. This option will be effective only for bucket unaware table without primary keys and batch execution mode.\rsink.clustering.sample-factor\r100\rInteger\rSpecifies the sample factor. Let S represent the total number of samples, F represent the sample factor, and P represent the sink parallelism, then S=F×P. The minimum allowed sample factor is 20.\rsink.clustering.sort-in-cluster\rtrue\rBoolean\rIndicates whether to further sort data belonged to each sink task after range partitioning.\rsink.clustering.strategy\r\"auto\"\rString\rSpecifies the comparison algorithm used for range partitioning, including 'zorder', 'hilbert', and 'order', corresponding to the z-order curve algorithm, hilbert curve algorithm, and basic type comparison algorithm, respectively. When not configured, it will automatically determine the algorithm based on the number of columns in 'sink.clustering.by-columns'. 'order' is used for 1 column, 'zorder' for less than 5 columns, and 'hilbert' for 5 or more columns.\rsink.committer-cpu\r1.0\rDouble\rSink committer cpu to control cpu cores of global committer.\rsink.committer-memory\r(none)\rMemorySize\rSink committer memory to control heap memory of global committer.\rsink.committer-operator-chaining\rtrue\rBoolean\rAllow sink committer and writer operator to be chained together\rsink.cross-partition.managed-memory\r256 mb\rMemorySize\rWeight of managed memory for RocksDB in cross-partition update, Flink will compute the memory size according to the weight, the actual memory used depends on the running environment.\rsink.managed.writer-buffer-memory\r256 mb\rMemorySize\rWeight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.\rsink.operator-uid.suffix\r(none)\rString\rSet the uid suffix for the writer, dynamic bucket assigner and committer operators. The uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.\rsink.parallelism\r(none)\rInteger\rDefines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.\rsink.savepoint.auto-tag\rfalse\rBoolean\rIf true, a tag will be automatically created for the snapshot created by flink savepoint.\rsink.use-managed-memory-allocator\rfalse\rBoolean\rIf true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.\rsource.checkpoint-align.enabled\rfalse\rBoolean\rWhether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.\rsource.checkpoint-align.timeout\r30 s\rDuration\rIf the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.\rsource.operator-uid.suffix\r(none)\rString\rSet the uid suffix for the source operators. After setting, the uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.\rstreaming-read.shuffle-bucket-with-partition\rtrue\rBoolean\rWhether shuffle by partition and bucket when streaming read.\runaware-bucket.compaction.parallelism\r(none)\rInteger\rDefines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.\rSparkCatalogOptions\r#\rSpark catalog options for paimon.\nKey\rDefault\rType\rDescription\rcatalog.create-underlying-session-catalog\rfalse\rBoolean\rIf true, create and use an underlying session catalog instead of default session catalog when use SparkGenericCatalog.\rdefaultDatabase\r\"default\"\rString\rThe default database name.\rSparkConnectorOptions\r#\rSpark connector options for paimon.\nKey\rDefault\rType\rDescription\rread.changelog\rfalse\rBoolean\rWhether to read row in the form of changelog (add rowkind column in row to represent its change type).\rread.stream.maxBytesPerTrigger\r(none)\rLong\rThe maximum number of bytes returned in a single batch.\rread.stream.maxFilesPerTrigger\r(none)\rInteger\rThe maximum number of files returned in a single batch.\rread.stream.maxRowsPerTrigger\r(none)\rLong\rThe maximum number of rows returned in a single batch.\rread.stream.maxTriggerDelayMs\r(none)\rLong\rThe maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.\rread.stream.minRowsPerTrigger\r(none)\rLong\rThe minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.\rrequiredSparkConfsCheck.enabled\rtrue\rBoolean\rWhether to verify SparkSession is initialized with required configurations.\rwrite.merge-schema\rfalse\rBoolean\rIf true, merge the data schema and the table schema automatically before write data.\rwrite.merge-schema.explicit-cast\rfalse\rBoolean\rIf true, allow to merge data types if the two types meet the rules for explicit casting.\rORC Options\r#\rKey\rDefault\rType\rDescription\rorc.column.encoding.direct\r(none)\rInteger\rComma-separated list of fields for which dictionary encoding is to be skipped in orc.\rorc.dictionary.key.threshold\r0.8\rDouble\rIf the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 0 to always disable dictionary encoding. Use 1 to always use dictionary encoding.\rorc.timestamp-ltz.legacy.type\rtrue\rBoolean\rThis option is used to be compatible with the paimon-orc‘s old behavior for the `timestamp_ltz` data type.\rRocksDB Options\r#\rThe following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\nKey\rDefault\rType\rDescription\rlookup.cache-rows\r10000\rLong\rThe maximum number of rows to store in the cache.\rlookup.continuous.discovery-interval\r(none)\rDuration\rThe discovery interval of lookup continuous reading. This is used as an SQL hint. If it's not configured, the lookup function will fallback to 'continuous.discovery-interval'.\rrocksdb.block.blocksize\r4 kb\rMemorySize\rThe approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.\rrocksdb.block.cache-size\r128 mb\rMemorySize\rThe amount of the cache for data blocks in RocksDB.\rrocksdb.block.metadata-blocksize\r4 kb\rMemorySize\rApproximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.\rrocksdb.bloom-filter.bits-per-key\r10.0\rDouble\rBits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.\rrocksdb.bloom-filter.block-based-mode\rfalse\rBoolean\rIf true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.\rrocksdb.compaction.level.max-size-level-base\r256 mb\rMemorySize\rThe upper-bound of the total size of level base files in bytes. The default value is '256MB'.\rrocksdb.compaction.level.target-file-size-base\r64 mb\rMemorySize\rThe target file size for compaction, which determines a level-1 file size. The default value is '64MB'.\rrocksdb.compaction.level.use-dynamic-size\rfalse\rBoolean\rIf true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.\rrocksdb.compaction.style\rLEVEL\rEnum\nThe specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"\rrocksdb.compression.type\rLZ4_COMPRESSION\rEnum\nThe compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"\rrocksdb.files.open\r-1\rInteger\rThe maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.\rrocksdb.thread.num\r2\rInteger\rThe maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.\rrocksdb.use-bloom-filter\rfalse\rBoolean\rIf true, every newly created SST file will contain a Bloom filter. It is disabled by default.\rrocksdb.writebuffer.count\r2\rInteger\rThe maximum number of write buffers that are built up in memory. The default value is '2'.\rrocksdb.writebuffer.number-to-merge\r1\rInteger\rThe minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.\rrocksdb.writebuffer.size\r64 mb\rMemorySize\rThe amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.\r"},{"id":104,"href":"/concepts/rest/rest-api/","title":"REST API","section":"RESTCatalog","content":"\r"},{"id":105,"href":"/versions/","title":"Versions","section":"Apache Paimon","content":"\rVersions\r#\rAn appendix of hosted documentation for all versions of Apache Paimon.\nmaster\rstable\r1.1\r1.0\r0.9\r"}]